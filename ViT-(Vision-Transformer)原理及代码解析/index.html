<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<link rel="icon" href="/assets/images/logo.png">

<title>ViT (Vision Transformer)原理及代码解析 | Chaos万有引力</title>

<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>ViT (Vision Transformer)原理及代码解析 | Chaos万有引力</title>
<meta name="generator" content="Jekyll v3.8.7" />
<meta property="og:title" content="ViT (Vision Transformer)原理及代码解析" />
<meta name="author" content="Luna" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="    今天我们来详细了解一下Vision Transformer。基于timm的代码。 1. Patch Embedding Transformer原本是用来做NLP的工作的，所以ViT的首要任务是将图转换成词的结构，这里采取的方法是如上图左下角所示，将图片分割成小块，每个小块就相当于句子里的一个词。这里把每个小块称作Patch，而Patch Embedding就是把每个Patch再经过一个全连接网络压缩成一定维度的向量。 这里是VisionTransformer源代码中关于Patch Embedding的部分： # 默认img_size=224, patch_size=16，in_chans=3，embed_dim=768， self.patch_embed = embed_layer(     img_size=img_size, patch_size=patch_size,  in_chans=in_chans, embed_dim=embed_dim) 而embed_layer其实是PatchEmbed： class PatchEmbed(nn.Module):     &quot;&quot;&quot; 2D Image to Patch Embedding     &quot;&quot;&quot; def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, norm_layer=None, flatten=True): super().__init__()         # img_size = (img_size, img_size)         img_size = to_2tuple(img_size) patch_size = to_2tuple(patch_size) self.img_size = img_size self.patch_size = patch_size self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1]) self.num_patches = self.grid_size[0] * self.grid_size[1] self.flatten = flatten         # 输入通道，输出通道，卷积核大小，步长         # C*H*W-&gt;embed_dim*grid_size*grid_size self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size) self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity() def forward(self, x): B, C, H, W = x.shape assert H == self.img_size[0] and W == self.img_size[1], \ f&quot;Input image size ({H}*{W}) doesn&#39;t match model ({self.img_size[0]}*{self.img_size[1]}).&quot; x = self.proj(x) if self.flatten: x = x.flatten(2).transpose(1, 2) # BCHW -&gt; BNC x = self.norm(x) return x proj虽然用的是卷积的写法，但其实是将每个patch接入了同样的全连接网络，将每个patch转换成了一个向量。x的维度是（B，C，H，W）其中B是batch size，C通常是三通道，H和W分别是图片的高和宽，而输出则是（B，N，E），B依然是batch size，N则是每张图被切割成了patch之后，patch的数量，E是embed_size，每个patch会通过一个全连接网络转换成一个向量，E是这个向量的长度，根据卷积的原理，也可以理解为每个patch的特征数量。 2. Positional Encoding     把图片分割成了patch，然后把每个patch转换成了embedding，接下来就是在embedding中加入位置信息。产生位置信息的方式主要分两大类，一类是直接通过固定算法产生，一种是训练获得。但加位置信息的方式还是比较统一且粗暴的。 产生一个位置向量，长度和patch embedding一致，然后直接相加。那么这个位置向量大概长什么样呢？ 比如patch embedding长度为4，那么位置向量长度也为4，每个位置有一个在[-1,1]的值。 假设你现在一张图切成了20个patch，embedding的长度是512，那么位置向量可以是上面这样的（tensor2tensor中的get_timing_signal_1d函数），每一行代表一个位置向量，第一行是位置0的位置向量，第二行是位置1的位置向量。 位置向量也可以是下面这样的（参考[1], [4]）： 公式如下： pos是单词在句子中的位置，或者patch在图中的位置，而i对应的是embedding的位置，dmodel对应的是patch embedding的长度，。这里说一下为什么要加这个位置编码，以及加上以后会有什么效果，我们观察上两幅图，可以发现，位置编码是随位置而改变的，位置差别越大的，那么向量差别也越大。在NLP课程里说过，把一个词转换成向量，就好像把一个词映射到了一个高维空间的位置，意思相近的词会在高维空间内比较靠近，而加上位置向量，会让位置相近的词更靠近，位置远的词离得更远。再来，为什么用cos，sin这种方式，作者的解释是，使用sin和cos编码可以得到词语之间的相对位置。 这儿我是这么理解的，根据这两个公式，当我们知道了sin(pos+k)，cos(pos+k)，再知道了sin(pos)和cos(pos)，那么k的值我们是可以算出来的，而且用加减乘除就可以算出来。因此这样的编码方式不但能表达单词的位置，还能表达单词与单词之间的相对位置。 再看timm中对positional encoding的实现： 可以发现timm中的positional encoding是随机数，也就是说没有做positional encoding，可能只是给你留了个位置，而默认的值服从某种正太分布，且限于很小的数值区间，这里就不上代码和详细解释了。至于这里为什么是随机数。一个是保留位置，便你扩展，二是本来positional encoding就有两类方式可以实现，一种是用一定的算法生成，另外一种就是通过训练调整获得。timm应该是默认是通过训练来调整获得。    3. Self-Attention     接下来看ViT中的Attention，这和Transformer中的self-attention应该是一致的，我们先看看参考[1]是如何介绍self-attention的。参考[1]举了一个语义处理的例子，有一句话是这样的     “The animal didn’t cross the street because it was too tired.” 我们人很容易理解，后面的it是指animal，但是要怎么让机器能够把it和animal关联起来呢？ Self-attention就是在这种需求下产生的，如上图所示，我们应当有一个结构能够表达每个单词和其他每个单词的关系。那这里我们处理的是图像问题，Self-attention的存在就可以理解成，我们应当有一个结构能够表达每个patch和其他patch的关系。之前说过，图像中的patch和语义处理中的词可以同等来看。     我们再来看具体怎么实现的：     1. 基于输入向量创建三个向量：query向量，key向量和value向量。     2. 由query向量和key向量产生自注意力。     Thinking和Machine可以理解为图片被切分的两个patch，现在计算Thinking的自注意力，通过q乘k，除以一定系数（scaled dot-product attention，点积得到的结果值通常很大，使得softmax结果不能很好地表达attention值。这时候除以一个缩放因子，可以一定程度上减缓这种情况。），通过softmax之后会得到一个关于Thinking的注意力向量，比如这个例子是[0.88, 0.12]，这个向量的意思是，要解释Thinking这个词在这个句子中的意思，应当取0.88份Thinking原本的意思，再取0.12份Machine原本的意思，就是Thinking在这个句子中的意思。最后图中Sum之后的结果所表达的就是每个单词在这个句子当中的意思。整个过程可以用下面这张图表达： 4. Multi-Head Attention timm中attention是在self-attention基础上改进的multi-head attention，也就是在产生q，k，v的时候，对q，k，v进行了切分，分别分成了num_heads份，对每一份分别进行self-attention的操作，最后再拼接起来，这样在一定程度上进行了参数隔离，至于这样为什么效果会更好，我觉得应该是这样操作会让关联的特征集中在一起，更容易训练。     class Attention(nn.Module): def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.): super().__init__() self.num_heads = num_heads         # q,k,v向量长度 head_dim = dim // num_heads         self.scale = head_dim ** -0.5 self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias) self.attn_drop = nn.Dropout(attn_drop) self.proj = nn.Linear(dim, dim) self.proj_drop = nn.Dropout(proj_drop) def forward(self, x):         # 这里C对应上面的E，向量的长度 B, N, C = x.shape         # (B, N, C) -&gt; (3，B，num_heads, N, C//num_heads), //是向下取整的意思。 qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)         # 将qkv在0维度上切成三个数据块，q,k,v:(B，num_heads, N, C//num_heads)         # 这里的效果是从每个向量产生三个向量，分别是query，key和value q, k, v = qkv.unbind(0) # make torchscript happy (cannot use tensor as tuple)         # @矩阵相乘获得score (B,num_heads,N,N) attn = (q @ k.transpose(-2, -1)) * self.scale attn = attn.softmax(dim=-1) attn = self.attn_drop(attn)         # (B,num_heads,N,N)@(B,num_heads,N,C//num_heads)-&gt;(B,num_heads,N,C//num_heads)         # (B,num_heads,N,C//num_heads) --&gt;(B,N,num_heads,C//num_heads)         # (B,N,num_heads,C//num_heads) -&gt; (B, N, C) x = (attn @ v).transpose(1, 2).reshape(B, N, C)         # (B, N, C) -&gt; (B, N, C) x = self.proj(x) x = self.proj_drop(x) return x multi-head attention的总示意图如下： 5. Layer Normalization     Layer normalization对应的一个概念是我们熟悉的Batch Normalization，这两个根本的不同在于，Layer normalization是对每个样本的所有特征进行归一化，而Batch Normalization是对每个通道的所有样本进行归一化。     为了便于理解，这里贴一下官网给LN的示例代码： # NLP Example batch, sentence_length, embedding_dim = 20, 5, 10 embedding = torch.randn(batch, sentence_length, embedding_dim) # 指定归一化的维度 layer_norm = nn.LayerNorm(embedding_dim) # 进行归一化 layer_norm(embedding) # Image Example N, C, H, W = 20, 5, 10, 10 input = torch.randn(N, C, H, W) # Normalize over the last three dimensions (i.e. the channel and spatial dimensions) # as shown in the image below layer_norm = nn.LayerNorm([C, H, W]) output = layer_norm(input) 在ViT中，虽然LN处理的是图片数据，但在进行LN之前，图片已经被切割成了Patch，而每个Patch表示的是一个词，因此是在用语义的逻辑在解决视觉问题，因此在ViT中，LN也是按语义的逻辑在用的。关于这个概念的详细细节，可以参考[3]和[2]。 6. Drop Path     Dropout是最早用于解决网络过拟合的方法，是所有drop类方法的始祖。方法示意图如下： 在向前传播的时候，让神经元以一定概率停止工作。这样可以使模型泛化能力变强，因为神经元会以一定概率失效，这样的机制会使结果不会过分依赖于个别神经元。训练阶段，以keep_prob概率使神经元失效，而推理的时候，会保留所有神经元的有效性，因此，训练时候加了dropout的神经元推理出来的结果要乘以keep_prob。     接下来以dropout的思路来理解drop path，drop path没找到示意图，那直接看timm上的代码： def drop_path(x, drop_prob: float = 0., training: bool = False): if drop_prob == 0. or not training: return x     # drop_prob是进行droppath的概率 keep_prob = 1 - drop_prob # work with diff dim tensors, not just 2D ConvNets     # 在ViT中，shape是(B,1,1),B是batch size shape = (x.shape[0],) + (1,) * (x.ndim - 1)     # 按shape,产生0-1之间的随机向量,并加上keep_prob   random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)     # 向下取整，二值化，这样random_tensor里1出现的概率的期望就是keep_prob random_tensor.floor_() # binarize     # 将一定图层变为0 output = x.div(keep_prob) * random_tensor return output 由代码可以看出，drop path是在batch那个维度，随机将一些图层直接变成0，以加快运算速度。 7. Encoder Transformer的架构图：     Transformer是由一堆encoder和decoder形成的，那encoder一般的架构图如下： Encoder在ViT中的实现细节如下面代码所示（layer normalization -&gt; multi-head attention -&gt; drop path -&gt; layer normalization -&gt; mlp -&gt; drop path），换了个名字，叫block了： class Block(nn.Module): def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):         super().__init__()         # 将每个样本的每个通道的特征向量做归一化         # 也就是说每个特征向量是独立做归一化的         # 我们这里虽然是图片数据，但图片被切割成了patch，用的是语义的逻辑 self.norm1 = norm_layer(dim) self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)         # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here self.drop_path = DropPath(drop_path) if drop_path &gt; 0. else nn.Identity() self.norm2 = norm_layer(dim) mlp_hidden_dim = int(dim * mlp_ratio)         # 全连接，激励，drop，全连接，drop,若out_features没填，那么输出维度不变。 self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)     def forward(self, x):         # 最后一维归一化，multi-head attention, drop_path         # (B, N, C) -&gt; (B, N, C) x = x + self.drop_path(self.attn(self.norm1(x)))         # (B, N, C) -&gt; (B, N, C) x = x + self.drop_path(self.mlp(self.norm2(x))) return x 在ViT中这样的block会有好几层，形成blocks： # stochastic depth decay rule dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)] self.blocks = nn.Sequential(*[     Block(         dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, drop=drop_rate,         attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, act_layer=act_layer)     for i in range(depth)]) 如果drop_path_rate大于0，每一层block的drop_path的会线性增加。depth是一个blocks里block的数量。也可以理解为blocks这个网络块的深度。 8. Forward Features Patch embedding -&gt; 加cls -&gt; 加pos embedding -&gt; 用blocks进行encoding -&gt; layer normalization -&gt; 输出图的embedding def forward_features(self, x):     # x由（B，C，H，W）-&gt;（B，N，E）     x = self.patch_embed(x)     # stole cls_tokens impl from Phil Wang, thanks     # cls_token由(1, 1, 768)-&gt;(B, 1, 768), B是batch_size     cls_token = self.cls_token.expand(x.shape[0], -1, -1)     # dist_token是None,DeiT models才会用到dist_token。     if self.dist_token is None:         # x由(B, N, E)-&gt;(B, 1+N, E)         x = torch.cat((cls_token, x), dim=1)     else:         # x由(B, N, E)-&gt;(B, 2+N, E)         x = torch.cat((cls_token, self.dist_token.expand(x.shape[0], -1, -1), x), dim=1)     # +pos_embed:(1, 1+N, E)，再加一个dropout层     x = self.pos_drop(x + self.pos_embed)     x = self.blocks(x)     # nn.LayerNorm     x = self.norm(x)     if self.dist_token is None:         # 不是DeiT，输出就是x[:,0]，(B, 1, 768)，即cls_token         return self.pre_logits(x[:, 0])     else:         # 是DeiT，输出就是cls_token和dist_token         return x[:, 0], x[:, 1] 这里在patch 那个维度加入了一个cls_token，可以这样理解这个存在，其他的embedding表达的都是不同的patch的特征，而cls_token是要综合所有patch的信息，产生一个新的embedding，来表达整个图的信息。而dist_token则是属于DeiT网络的结构。 9. Forward 这就是这个模型的总流程了：forward features -&gt; 最终输出 def forward(self, x):         #（B，C，H，W）-&gt; (B, 1, 768)         # (B,C,H,W) -&gt; (B, 1, 768), (B, 1, 768) x = self.forward_features(x)          if self.head_dist is not None:             # 如果num_classes&gt;0, (B, 1, 768)-&gt;(B, 1, num_classes) # 否则不变             x, x_dist = self.head(x[0]), self.head_dist(x[1])             if self.training and not torch.jit.is_scripting(): return x, x_dist else:                 # during inference,  # return the average of both classifier predictions return (x + x_dist) / 2 else:             # 如果num_classes&gt;0, (B, 1, 768)-&gt;(B, 1, num_classes)             # 否则不变 x = self.head(x) return x 这样ViT算是给我说完了，DeiT又涉及到很多新的概念，之后也会参考代码，进行详细解说。     觉得有用，关注，在看，点赞，转发分享来一波哦！ 参考： [1] Jay Alammar, The Illustrated Transformer, jalammar.github.io, 2018 [2] 简枫，聊聊 Transformer，知乎，2019 [3] 大师兄，模型优化之Layer Normalization，知乎，2020 [4] TensorFlow Core，理解语言的 Transformer 模型，TensorFlow，https://www.tensorflow.org/tutorials/text/transformer" />
<meta property="og:description" content="    今天我们来详细了解一下Vision Transformer。基于timm的代码。 1. Patch Embedding Transformer原本是用来做NLP的工作的，所以ViT的首要任务是将图转换成词的结构，这里采取的方法是如上图左下角所示，将图片分割成小块，每个小块就相当于句子里的一个词。这里把每个小块称作Patch，而Patch Embedding就是把每个Patch再经过一个全连接网络压缩成一定维度的向量。 这里是VisionTransformer源代码中关于Patch Embedding的部分： # 默认img_size=224, patch_size=16，in_chans=3，embed_dim=768， self.patch_embed = embed_layer(     img_size=img_size, patch_size=patch_size,  in_chans=in_chans, embed_dim=embed_dim) 而embed_layer其实是PatchEmbed： class PatchEmbed(nn.Module):     &quot;&quot;&quot; 2D Image to Patch Embedding     &quot;&quot;&quot; def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, norm_layer=None, flatten=True): super().__init__()         # img_size = (img_size, img_size)         img_size = to_2tuple(img_size) patch_size = to_2tuple(patch_size) self.img_size = img_size self.patch_size = patch_size self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1]) self.num_patches = self.grid_size[0] * self.grid_size[1] self.flatten = flatten         # 输入通道，输出通道，卷积核大小，步长         # C*H*W-&gt;embed_dim*grid_size*grid_size self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size) self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity() def forward(self, x): B, C, H, W = x.shape assert H == self.img_size[0] and W == self.img_size[1], \ f&quot;Input image size ({H}*{W}) doesn&#39;t match model ({self.img_size[0]}*{self.img_size[1]}).&quot; x = self.proj(x) if self.flatten: x = x.flatten(2).transpose(1, 2) # BCHW -&gt; BNC x = self.norm(x) return x proj虽然用的是卷积的写法，但其实是将每个patch接入了同样的全连接网络，将每个patch转换成了一个向量。x的维度是（B，C，H，W）其中B是batch size，C通常是三通道，H和W分别是图片的高和宽，而输出则是（B，N，E），B依然是batch size，N则是每张图被切割成了patch之后，patch的数量，E是embed_size，每个patch会通过一个全连接网络转换成一个向量，E是这个向量的长度，根据卷积的原理，也可以理解为每个patch的特征数量。 2. Positional Encoding     把图片分割成了patch，然后把每个patch转换成了embedding，接下来就是在embedding中加入位置信息。产生位置信息的方式主要分两大类，一类是直接通过固定算法产生，一种是训练获得。但加位置信息的方式还是比较统一且粗暴的。 产生一个位置向量，长度和patch embedding一致，然后直接相加。那么这个位置向量大概长什么样呢？ 比如patch embedding长度为4，那么位置向量长度也为4，每个位置有一个在[-1,1]的值。 假设你现在一张图切成了20个patch，embedding的长度是512，那么位置向量可以是上面这样的（tensor2tensor中的get_timing_signal_1d函数），每一行代表一个位置向量，第一行是位置0的位置向量，第二行是位置1的位置向量。 位置向量也可以是下面这样的（参考[1], [4]）： 公式如下： pos是单词在句子中的位置，或者patch在图中的位置，而i对应的是embedding的位置，dmodel对应的是patch embedding的长度，。这里说一下为什么要加这个位置编码，以及加上以后会有什么效果，我们观察上两幅图，可以发现，位置编码是随位置而改变的，位置差别越大的，那么向量差别也越大。在NLP课程里说过，把一个词转换成向量，就好像把一个词映射到了一个高维空间的位置，意思相近的词会在高维空间内比较靠近，而加上位置向量，会让位置相近的词更靠近，位置远的词离得更远。再来，为什么用cos，sin这种方式，作者的解释是，使用sin和cos编码可以得到词语之间的相对位置。 这儿我是这么理解的，根据这两个公式，当我们知道了sin(pos+k)，cos(pos+k)，再知道了sin(pos)和cos(pos)，那么k的值我们是可以算出来的，而且用加减乘除就可以算出来。因此这样的编码方式不但能表达单词的位置，还能表达单词与单词之间的相对位置。 再看timm中对positional encoding的实现： 可以发现timm中的positional encoding是随机数，也就是说没有做positional encoding，可能只是给你留了个位置，而默认的值服从某种正太分布，且限于很小的数值区间，这里就不上代码和详细解释了。至于这里为什么是随机数。一个是保留位置，便你扩展，二是本来positional encoding就有两类方式可以实现，一种是用一定的算法生成，另外一种就是通过训练调整获得。timm应该是默认是通过训练来调整获得。    3. Self-Attention     接下来看ViT中的Attention，这和Transformer中的self-attention应该是一致的，我们先看看参考[1]是如何介绍self-attention的。参考[1]举了一个语义处理的例子，有一句话是这样的     “The animal didn’t cross the street because it was too tired.” 我们人很容易理解，后面的it是指animal，但是要怎么让机器能够把it和animal关联起来呢？ Self-attention就是在这种需求下产生的，如上图所示，我们应当有一个结构能够表达每个单词和其他每个单词的关系。那这里我们处理的是图像问题，Self-attention的存在就可以理解成，我们应当有一个结构能够表达每个patch和其他patch的关系。之前说过，图像中的patch和语义处理中的词可以同等来看。     我们再来看具体怎么实现的：     1. 基于输入向量创建三个向量：query向量，key向量和value向量。     2. 由query向量和key向量产生自注意力。     Thinking和Machine可以理解为图片被切分的两个patch，现在计算Thinking的自注意力，通过q乘k，除以一定系数（scaled dot-product attention，点积得到的结果值通常很大，使得softmax结果不能很好地表达attention值。这时候除以一个缩放因子，可以一定程度上减缓这种情况。），通过softmax之后会得到一个关于Thinking的注意力向量，比如这个例子是[0.88, 0.12]，这个向量的意思是，要解释Thinking这个词在这个句子中的意思，应当取0.88份Thinking原本的意思，再取0.12份Machine原本的意思，就是Thinking在这个句子中的意思。最后图中Sum之后的结果所表达的就是每个单词在这个句子当中的意思。整个过程可以用下面这张图表达： 4. Multi-Head Attention timm中attention是在self-attention基础上改进的multi-head attention，也就是在产生q，k，v的时候，对q，k，v进行了切分，分别分成了num_heads份，对每一份分别进行self-attention的操作，最后再拼接起来，这样在一定程度上进行了参数隔离，至于这样为什么效果会更好，我觉得应该是这样操作会让关联的特征集中在一起，更容易训练。     class Attention(nn.Module): def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.): super().__init__() self.num_heads = num_heads         # q,k,v向量长度 head_dim = dim // num_heads         self.scale = head_dim ** -0.5 self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias) self.attn_drop = nn.Dropout(attn_drop) self.proj = nn.Linear(dim, dim) self.proj_drop = nn.Dropout(proj_drop) def forward(self, x):         # 这里C对应上面的E，向量的长度 B, N, C = x.shape         # (B, N, C) -&gt; (3，B，num_heads, N, C//num_heads), //是向下取整的意思。 qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)         # 将qkv在0维度上切成三个数据块，q,k,v:(B，num_heads, N, C//num_heads)         # 这里的效果是从每个向量产生三个向量，分别是query，key和value q, k, v = qkv.unbind(0) # make torchscript happy (cannot use tensor as tuple)         # @矩阵相乘获得score (B,num_heads,N,N) attn = (q @ k.transpose(-2, -1)) * self.scale attn = attn.softmax(dim=-1) attn = self.attn_drop(attn)         # (B,num_heads,N,N)@(B,num_heads,N,C//num_heads)-&gt;(B,num_heads,N,C//num_heads)         # (B,num_heads,N,C//num_heads) --&gt;(B,N,num_heads,C//num_heads)         # (B,N,num_heads,C//num_heads) -&gt; (B, N, C) x = (attn @ v).transpose(1, 2).reshape(B, N, C)         # (B, N, C) -&gt; (B, N, C) x = self.proj(x) x = self.proj_drop(x) return x multi-head attention的总示意图如下： 5. Layer Normalization     Layer normalization对应的一个概念是我们熟悉的Batch Normalization，这两个根本的不同在于，Layer normalization是对每个样本的所有特征进行归一化，而Batch Normalization是对每个通道的所有样本进行归一化。     为了便于理解，这里贴一下官网给LN的示例代码： # NLP Example batch, sentence_length, embedding_dim = 20, 5, 10 embedding = torch.randn(batch, sentence_length, embedding_dim) # 指定归一化的维度 layer_norm = nn.LayerNorm(embedding_dim) # 进行归一化 layer_norm(embedding) # Image Example N, C, H, W = 20, 5, 10, 10 input = torch.randn(N, C, H, W) # Normalize over the last three dimensions (i.e. the channel and spatial dimensions) # as shown in the image below layer_norm = nn.LayerNorm([C, H, W]) output = layer_norm(input) 在ViT中，虽然LN处理的是图片数据，但在进行LN之前，图片已经被切割成了Patch，而每个Patch表示的是一个词，因此是在用语义的逻辑在解决视觉问题，因此在ViT中，LN也是按语义的逻辑在用的。关于这个概念的详细细节，可以参考[3]和[2]。 6. Drop Path     Dropout是最早用于解决网络过拟合的方法，是所有drop类方法的始祖。方法示意图如下： 在向前传播的时候，让神经元以一定概率停止工作。这样可以使模型泛化能力变强，因为神经元会以一定概率失效，这样的机制会使结果不会过分依赖于个别神经元。训练阶段，以keep_prob概率使神经元失效，而推理的时候，会保留所有神经元的有效性，因此，训练时候加了dropout的神经元推理出来的结果要乘以keep_prob。     接下来以dropout的思路来理解drop path，drop path没找到示意图，那直接看timm上的代码： def drop_path(x, drop_prob: float = 0., training: bool = False): if drop_prob == 0. or not training: return x     # drop_prob是进行droppath的概率 keep_prob = 1 - drop_prob # work with diff dim tensors, not just 2D ConvNets     # 在ViT中，shape是(B,1,1),B是batch size shape = (x.shape[0],) + (1,) * (x.ndim - 1)     # 按shape,产生0-1之间的随机向量,并加上keep_prob   random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)     # 向下取整，二值化，这样random_tensor里1出现的概率的期望就是keep_prob random_tensor.floor_() # binarize     # 将一定图层变为0 output = x.div(keep_prob) * random_tensor return output 由代码可以看出，drop path是在batch那个维度，随机将一些图层直接变成0，以加快运算速度。 7. Encoder Transformer的架构图：     Transformer是由一堆encoder和decoder形成的，那encoder一般的架构图如下： Encoder在ViT中的实现细节如下面代码所示（layer normalization -&gt; multi-head attention -&gt; drop path -&gt; layer normalization -&gt; mlp -&gt; drop path），换了个名字，叫block了： class Block(nn.Module): def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):         super().__init__()         # 将每个样本的每个通道的特征向量做归一化         # 也就是说每个特征向量是独立做归一化的         # 我们这里虽然是图片数据，但图片被切割成了patch，用的是语义的逻辑 self.norm1 = norm_layer(dim) self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)         # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here self.drop_path = DropPath(drop_path) if drop_path &gt; 0. else nn.Identity() self.norm2 = norm_layer(dim) mlp_hidden_dim = int(dim * mlp_ratio)         # 全连接，激励，drop，全连接，drop,若out_features没填，那么输出维度不变。 self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)     def forward(self, x):         # 最后一维归一化，multi-head attention, drop_path         # (B, N, C) -&gt; (B, N, C) x = x + self.drop_path(self.attn(self.norm1(x)))         # (B, N, C) -&gt; (B, N, C) x = x + self.drop_path(self.mlp(self.norm2(x))) return x 在ViT中这样的block会有好几层，形成blocks： # stochastic depth decay rule dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)] self.blocks = nn.Sequential(*[     Block(         dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, drop=drop_rate,         attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, act_layer=act_layer)     for i in range(depth)]) 如果drop_path_rate大于0，每一层block的drop_path的会线性增加。depth是一个blocks里block的数量。也可以理解为blocks这个网络块的深度。 8. Forward Features Patch embedding -&gt; 加cls -&gt; 加pos embedding -&gt; 用blocks进行encoding -&gt; layer normalization -&gt; 输出图的embedding def forward_features(self, x):     # x由（B，C，H，W）-&gt;（B，N，E）     x = self.patch_embed(x)     # stole cls_tokens impl from Phil Wang, thanks     # cls_token由(1, 1, 768)-&gt;(B, 1, 768), B是batch_size     cls_token = self.cls_token.expand(x.shape[0], -1, -1)     # dist_token是None,DeiT models才会用到dist_token。     if self.dist_token is None:         # x由(B, N, E)-&gt;(B, 1+N, E)         x = torch.cat((cls_token, x), dim=1)     else:         # x由(B, N, E)-&gt;(B, 2+N, E)         x = torch.cat((cls_token, self.dist_token.expand(x.shape[0], -1, -1), x), dim=1)     # +pos_embed:(1, 1+N, E)，再加一个dropout层     x = self.pos_drop(x + self.pos_embed)     x = self.blocks(x)     # nn.LayerNorm     x = self.norm(x)     if self.dist_token is None:         # 不是DeiT，输出就是x[:,0]，(B, 1, 768)，即cls_token         return self.pre_logits(x[:, 0])     else:         # 是DeiT，输出就是cls_token和dist_token         return x[:, 0], x[:, 1] 这里在patch 那个维度加入了一个cls_token，可以这样理解这个存在，其他的embedding表达的都是不同的patch的特征，而cls_token是要综合所有patch的信息，产生一个新的embedding，来表达整个图的信息。而dist_token则是属于DeiT网络的结构。 9. Forward 这就是这个模型的总流程了：forward features -&gt; 最终输出 def forward(self, x):         #（B，C，H，W）-&gt; (B, 1, 768)         # (B,C,H,W) -&gt; (B, 1, 768), (B, 1, 768) x = self.forward_features(x)          if self.head_dist is not None:             # 如果num_classes&gt;0, (B, 1, 768)-&gt;(B, 1, num_classes) # 否则不变             x, x_dist = self.head(x[0]), self.head_dist(x[1])             if self.training and not torch.jit.is_scripting(): return x, x_dist else:                 # during inference,  # return the average of both classifier predictions return (x + x_dist) / 2 else:             # 如果num_classes&gt;0, (B, 1, 768)-&gt;(B, 1, num_classes)             # 否则不变 x = self.head(x) return x 这样ViT算是给我说完了，DeiT又涉及到很多新的概念，之后也会参考代码，进行详细解说。     觉得有用，关注，在看，点赞，转发分享来一波哦！ 参考： [1] Jay Alammar, The Illustrated Transformer, jalammar.github.io, 2018 [2] 简枫，聊聊 Transformer，知乎，2019 [3] 大师兄，模型优化之Layer Normalization，知乎，2020 [4] TensorFlow Core，理解语言的 Transformer 模型，TensorFlow，https://www.tensorflow.org/tutorials/text/transformer" />
<link rel="canonical" href="http://localhost:4000/ViT-(Vision-Transformer)%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/" />
<meta property="og:url" content="http://localhost:4000/ViT-(Vision-Transformer)%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/" />
<meta property="og:site_name" content="Chaos万有引力" />
<meta property="og:image" content="http://localhost:4000/assets/images/ViT%20(Vision%20Transformer)%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/1.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-10-31T00:00:00+08:00" />
<script type="application/ld+json">
{"description":"    今天我们来详细了解一下Vision Transformer。基于timm的代码。 1. Patch Embedding Transformer原本是用来做NLP的工作的，所以ViT的首要任务是将图转换成词的结构，这里采取的方法是如上图左下角所示，将图片分割成小块，每个小块就相当于句子里的一个词。这里把每个小块称作Patch，而Patch Embedding就是把每个Patch再经过一个全连接网络压缩成一定维度的向量。 这里是VisionTransformer源代码中关于Patch Embedding的部分： # 默认img_size=224, patch_size=16，in_chans=3，embed_dim=768， self.patch_embed = embed_layer(     img_size=img_size, patch_size=patch_size,  in_chans=in_chans, embed_dim=embed_dim) 而embed_layer其实是PatchEmbed： class PatchEmbed(nn.Module):     &quot;&quot;&quot; 2D Image to Patch Embedding     &quot;&quot;&quot; def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, norm_layer=None, flatten=True): super().__init__()         # img_size = (img_size, img_size)         img_size = to_2tuple(img_size) patch_size = to_2tuple(patch_size) self.img_size = img_size self.patch_size = patch_size self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1]) self.num_patches = self.grid_size[0] * self.grid_size[1] self.flatten = flatten         # 输入通道，输出通道，卷积核大小，步长         # C*H*W-&gt;embed_dim*grid_size*grid_size self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size) self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity() def forward(self, x): B, C, H, W = x.shape assert H == self.img_size[0] and W == self.img_size[1], \\ f&quot;Input image size ({H}*{W}) doesn&#39;t match model ({self.img_size[0]}*{self.img_size[1]}).&quot; x = self.proj(x) if self.flatten: x = x.flatten(2).transpose(1, 2) # BCHW -&gt; BNC x = self.norm(x) return x proj虽然用的是卷积的写法，但其实是将每个patch接入了同样的全连接网络，将每个patch转换成了一个向量。x的维度是（B，C，H，W）其中B是batch size，C通常是三通道，H和W分别是图片的高和宽，而输出则是（B，N，E），B依然是batch size，N则是每张图被切割成了patch之后，patch的数量，E是embed_size，每个patch会通过一个全连接网络转换成一个向量，E是这个向量的长度，根据卷积的原理，也可以理解为每个patch的特征数量。 2. Positional Encoding     把图片分割成了patch，然后把每个patch转换成了embedding，接下来就是在embedding中加入位置信息。产生位置信息的方式主要分两大类，一类是直接通过固定算法产生，一种是训练获得。但加位置信息的方式还是比较统一且粗暴的。 产生一个位置向量，长度和patch embedding一致，然后直接相加。那么这个位置向量大概长什么样呢？ 比如patch embedding长度为4，那么位置向量长度也为4，每个位置有一个在[-1,1]的值。 假设你现在一张图切成了20个patch，embedding的长度是512，那么位置向量可以是上面这样的（tensor2tensor中的get_timing_signal_1d函数），每一行代表一个位置向量，第一行是位置0的位置向量，第二行是位置1的位置向量。 位置向量也可以是下面这样的（参考[1], [4]）： 公式如下： pos是单词在句子中的位置，或者patch在图中的位置，而i对应的是embedding的位置，dmodel对应的是patch embedding的长度，。这里说一下为什么要加这个位置编码，以及加上以后会有什么效果，我们观察上两幅图，可以发现，位置编码是随位置而改变的，位置差别越大的，那么向量差别也越大。在NLP课程里说过，把一个词转换成向量，就好像把一个词映射到了一个高维空间的位置，意思相近的词会在高维空间内比较靠近，而加上位置向量，会让位置相近的词更靠近，位置远的词离得更远。再来，为什么用cos，sin这种方式，作者的解释是，使用sin和cos编码可以得到词语之间的相对位置。 这儿我是这么理解的，根据这两个公式，当我们知道了sin(pos+k)，cos(pos+k)，再知道了sin(pos)和cos(pos)，那么k的值我们是可以算出来的，而且用加减乘除就可以算出来。因此这样的编码方式不但能表达单词的位置，还能表达单词与单词之间的相对位置。 再看timm中对positional encoding的实现： 可以发现timm中的positional encoding是随机数，也就是说没有做positional encoding，可能只是给你留了个位置，而默认的值服从某种正太分布，且限于很小的数值区间，这里就不上代码和详细解释了。至于这里为什么是随机数。一个是保留位置，便你扩展，二是本来positional encoding就有两类方式可以实现，一种是用一定的算法生成，另外一种就是通过训练调整获得。timm应该是默认是通过训练来调整获得。    3. Self-Attention     接下来看ViT中的Attention，这和Transformer中的self-attention应该是一致的，我们先看看参考[1]是如何介绍self-attention的。参考[1]举了一个语义处理的例子，有一句话是这样的     “The animal didn’t cross the street because it was too tired.” 我们人很容易理解，后面的it是指animal，但是要怎么让机器能够把it和animal关联起来呢？ Self-attention就是在这种需求下产生的，如上图所示，我们应当有一个结构能够表达每个单词和其他每个单词的关系。那这里我们处理的是图像问题，Self-attention的存在就可以理解成，我们应当有一个结构能够表达每个patch和其他patch的关系。之前说过，图像中的patch和语义处理中的词可以同等来看。     我们再来看具体怎么实现的：     1. 基于输入向量创建三个向量：query向量，key向量和value向量。     2. 由query向量和key向量产生自注意力。     Thinking和Machine可以理解为图片被切分的两个patch，现在计算Thinking的自注意力，通过q乘k，除以一定系数（scaled dot-product attention，点积得到的结果值通常很大，使得softmax结果不能很好地表达attention值。这时候除以一个缩放因子，可以一定程度上减缓这种情况。），通过softmax之后会得到一个关于Thinking的注意力向量，比如这个例子是[0.88, 0.12]，这个向量的意思是，要解释Thinking这个词在这个句子中的意思，应当取0.88份Thinking原本的意思，再取0.12份Machine原本的意思，就是Thinking在这个句子中的意思。最后图中Sum之后的结果所表达的就是每个单词在这个句子当中的意思。整个过程可以用下面这张图表达： 4. Multi-Head Attention timm中attention是在self-attention基础上改进的multi-head attention，也就是在产生q，k，v的时候，对q，k，v进行了切分，分别分成了num_heads份，对每一份分别进行self-attention的操作，最后再拼接起来，这样在一定程度上进行了参数隔离，至于这样为什么效果会更好，我觉得应该是这样操作会让关联的特征集中在一起，更容易训练。     class Attention(nn.Module): def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.): super().__init__() self.num_heads = num_heads         # q,k,v向量长度 head_dim = dim // num_heads         self.scale = head_dim ** -0.5 self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias) self.attn_drop = nn.Dropout(attn_drop) self.proj = nn.Linear(dim, dim) self.proj_drop = nn.Dropout(proj_drop) def forward(self, x):         # 这里C对应上面的E，向量的长度 B, N, C = x.shape         # (B, N, C) -&gt; (3，B，num_heads, N, C//num_heads), //是向下取整的意思。 qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)         # 将qkv在0维度上切成三个数据块，q,k,v:(B，num_heads, N, C//num_heads)         # 这里的效果是从每个向量产生三个向量，分别是query，key和value q, k, v = qkv.unbind(0) # make torchscript happy (cannot use tensor as tuple)         # @矩阵相乘获得score (B,num_heads,N,N) attn = (q @ k.transpose(-2, -1)) * self.scale attn = attn.softmax(dim=-1) attn = self.attn_drop(attn)         # (B,num_heads,N,N)@(B,num_heads,N,C//num_heads)-&gt;(B,num_heads,N,C//num_heads)         # (B,num_heads,N,C//num_heads) --&gt;(B,N,num_heads,C//num_heads)         # (B,N,num_heads,C//num_heads) -&gt; (B, N, C) x = (attn @ v).transpose(1, 2).reshape(B, N, C)         # (B, N, C) -&gt; (B, N, C) x = self.proj(x) x = self.proj_drop(x) return x multi-head attention的总示意图如下： 5. Layer Normalization     Layer normalization对应的一个概念是我们熟悉的Batch Normalization，这两个根本的不同在于，Layer normalization是对每个样本的所有特征进行归一化，而Batch Normalization是对每个通道的所有样本进行归一化。     为了便于理解，这里贴一下官网给LN的示例代码： # NLP Example batch, sentence_length, embedding_dim = 20, 5, 10 embedding = torch.randn(batch, sentence_length, embedding_dim) # 指定归一化的维度 layer_norm = nn.LayerNorm(embedding_dim) # 进行归一化 layer_norm(embedding) # Image Example N, C, H, W = 20, 5, 10, 10 input = torch.randn(N, C, H, W) # Normalize over the last three dimensions (i.e. the channel and spatial dimensions) # as shown in the image below layer_norm = nn.LayerNorm([C, H, W]) output = layer_norm(input) 在ViT中，虽然LN处理的是图片数据，但在进行LN之前，图片已经被切割成了Patch，而每个Patch表示的是一个词，因此是在用语义的逻辑在解决视觉问题，因此在ViT中，LN也是按语义的逻辑在用的。关于这个概念的详细细节，可以参考[3]和[2]。 6. Drop Path     Dropout是最早用于解决网络过拟合的方法，是所有drop类方法的始祖。方法示意图如下： 在向前传播的时候，让神经元以一定概率停止工作。这样可以使模型泛化能力变强，因为神经元会以一定概率失效，这样的机制会使结果不会过分依赖于个别神经元。训练阶段，以keep_prob概率使神经元失效，而推理的时候，会保留所有神经元的有效性，因此，训练时候加了dropout的神经元推理出来的结果要乘以keep_prob。     接下来以dropout的思路来理解drop path，drop path没找到示意图，那直接看timm上的代码： def drop_path(x, drop_prob: float = 0., training: bool = False): if drop_prob == 0. or not training: return x     # drop_prob是进行droppath的概率 keep_prob = 1 - drop_prob # work with diff dim tensors, not just 2D ConvNets     # 在ViT中，shape是(B,1,1),B是batch size shape = (x.shape[0],) + (1,) * (x.ndim - 1)     # 按shape,产生0-1之间的随机向量,并加上keep_prob   random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)     # 向下取整，二值化，这样random_tensor里1出现的概率的期望就是keep_prob random_tensor.floor_() # binarize     # 将一定图层变为0 output = x.div(keep_prob) * random_tensor return output 由代码可以看出，drop path是在batch那个维度，随机将一些图层直接变成0，以加快运算速度。 7. Encoder Transformer的架构图：     Transformer是由一堆encoder和decoder形成的，那encoder一般的架构图如下： Encoder在ViT中的实现细节如下面代码所示（layer normalization -&gt; multi-head attention -&gt; drop path -&gt; layer normalization -&gt; mlp -&gt; drop path），换了个名字，叫block了： class Block(nn.Module): def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):         super().__init__()         # 将每个样本的每个通道的特征向量做归一化         # 也就是说每个特征向量是独立做归一化的         # 我们这里虽然是图片数据，但图片被切割成了patch，用的是语义的逻辑 self.norm1 = norm_layer(dim) self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)         # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here self.drop_path = DropPath(drop_path) if drop_path &gt; 0. else nn.Identity() self.norm2 = norm_layer(dim) mlp_hidden_dim = int(dim * mlp_ratio)         # 全连接，激励，drop，全连接，drop,若out_features没填，那么输出维度不变。 self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)     def forward(self, x):         # 最后一维归一化，multi-head attention, drop_path         # (B, N, C) -&gt; (B, N, C) x = x + self.drop_path(self.attn(self.norm1(x)))         # (B, N, C) -&gt; (B, N, C) x = x + self.drop_path(self.mlp(self.norm2(x))) return x 在ViT中这样的block会有好几层，形成blocks： # stochastic depth decay rule dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)] self.blocks = nn.Sequential(*[     Block(         dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, drop=drop_rate,         attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, act_layer=act_layer)     for i in range(depth)]) 如果drop_path_rate大于0，每一层block的drop_path的会线性增加。depth是一个blocks里block的数量。也可以理解为blocks这个网络块的深度。 8. Forward Features Patch embedding -&gt; 加cls -&gt; 加pos embedding -&gt; 用blocks进行encoding -&gt; layer normalization -&gt; 输出图的embedding def forward_features(self, x):     # x由（B，C，H，W）-&gt;（B，N，E）     x = self.patch_embed(x)     # stole cls_tokens impl from Phil Wang, thanks     # cls_token由(1, 1, 768)-&gt;(B, 1, 768), B是batch_size     cls_token = self.cls_token.expand(x.shape[0], -1, -1)     # dist_token是None,DeiT models才会用到dist_token。     if self.dist_token is None:         # x由(B, N, E)-&gt;(B, 1+N, E)         x = torch.cat((cls_token, x), dim=1)     else:         # x由(B, N, E)-&gt;(B, 2+N, E)         x = torch.cat((cls_token, self.dist_token.expand(x.shape[0], -1, -1), x), dim=1)     # +pos_embed:(1, 1+N, E)，再加一个dropout层     x = self.pos_drop(x + self.pos_embed)     x = self.blocks(x)     # nn.LayerNorm     x = self.norm(x)     if self.dist_token is None:         # 不是DeiT，输出就是x[:,0]，(B, 1, 768)，即cls_token         return self.pre_logits(x[:, 0])     else:         # 是DeiT，输出就是cls_token和dist_token         return x[:, 0], x[:, 1] 这里在patch 那个维度加入了一个cls_token，可以这样理解这个存在，其他的embedding表达的都是不同的patch的特征，而cls_token是要综合所有patch的信息，产生一个新的embedding，来表达整个图的信息。而dist_token则是属于DeiT网络的结构。 9. Forward 这就是这个模型的总流程了：forward features -&gt; 最终输出 def forward(self, x):         #（B，C，H，W）-&gt; (B, 1, 768)         # (B,C,H,W) -&gt; (B, 1, 768), (B, 1, 768) x = self.forward_features(x)          if self.head_dist is not None:             # 如果num_classes&gt;0, (B, 1, 768)-&gt;(B, 1, num_classes) # 否则不变             x, x_dist = self.head(x[0]), self.head_dist(x[1])             if self.training and not torch.jit.is_scripting(): return x, x_dist else:                 # during inference,  # return the average of both classifier predictions return (x + x_dist) / 2 else:             # 如果num_classes&gt;0, (B, 1, 768)-&gt;(B, 1, num_classes)             # 否则不变 x = self.head(x) return x 这样ViT算是给我说完了，DeiT又涉及到很多新的概念，之后也会参考代码，进行详细解说。     觉得有用，关注，在看，点赞，转发分享来一波哦！ 参考： [1] Jay Alammar, The Illustrated Transformer, jalammar.github.io, 2018 [2] 简枫，聊聊 Transformer，知乎，2019 [3] 大师兄，模型优化之Layer Normalization，知乎，2020 [4] TensorFlow Core，理解语言的 Transformer 模型，TensorFlow，https://www.tensorflow.org/tutorials/text/transformer","url":"http://localhost:4000/ViT-(Vision-Transformer)%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/","image":"http://localhost:4000/assets/images/ViT%20(Vision%20Transformer)%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/1.png","@type":"BlogPosting","headline":"ViT (Vision Transformer)原理及代码解析","dateModified":"2021-10-31T00:00:00+08:00","datePublished":"2021-10-31T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/ViT-(Vision-Transformer)%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/images/logo.png"},"name":"Luna"},"author":{"@type":"Person","name":"Luna"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    
<link href='/assets/css/syntax.css' rel='stylesheet' type='text/css'/>
<link href="/assets/css/prism.css" rel="stylesheet">

<link href="/assets/css/theme.css" rel="stylesheet">
<script src="/assets/js/jquery.min.js"></script>

</head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-172775777-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-172775777-1');
</script>








<body>
	<!-- defer loading of font and font awesome -->
	<noscript id="deferred-styles">
		<link href="https://fonts.googleapis.com/css?family=Sen:400,700&display=swap" rel="stylesheet">
                <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC&display=swap" rel="stylesheet"> 
        <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" crossorigin="anonymous">
	</noscript>

<!-- Begin Sidebar Navigation
================================================== -->

<div class="sidebar">    
</div>   
<div class="nav-icon">
    <div class="hamburger-bar"></div>
</div>
<div id="blackover-nav" class="blackover"></div>
<nav id="menu">
    <ul>
        <h3>Navigation</h3>
        <li><a href="/">Home</a></li>
        <li><a href="/about">About </a></li>
        <li><a href="/categories">Categories</a></li>
        <li><a href="/tags">Tags</a></li>
        <li><a href="/wechat">WeChat Public Account</a></li>
        <li><a href="/authors">Authors</a></li>
        <li><a href="/contact">Contact</a></li>       
    </ul>   
</nav>

<script src="/assets/js/lunr.js"></script>

<style>
    
</style>

<div class="wrap-search">
    <div class="d-flex align-items-center ml-auto">
        <i class="fas fa-search show-search"></i>
        <form class="bd-search ml-3" onSubmit="return lunr_search(document.getElementById('lunrsearch').value);">
            <input type="text" class="form-control bigradius text-small launch-modal-search" id="lunrsearch" name="q" maxlength="255" value="" placeholder="Type and enter..."/>
        </form>
    </div>
</div>

<div id="lunrsearchresults">
    <ul></ul>
</div>

<script src="/assets/js/lunrsearchengine.js"></script>


<!-- End Sidebar Navigation
================================================== -->

<div class="site-content ">

<div class="container">

    <!-- Site Logo/Name
    ================================================== -->
  
    <!-- div style = "display: inline-flex"--> 
    <a class="navbar-brand" href="/">
        <img src="/assets/images/logo.png" alt="Chaos万有引力">
    </a>  
   

    <!-- Site Tag
    ================================================== -->
    
    <!--/div-->

    <!-- Content
    ================================================== -->
    <div class="main-content">
        <div class="entry-header">
    <!-- Post Title -->
    <h1 class="posttitle">ViT (Vision Transformer)原理及代码解析</h1>
    <!-- Author & Date  Box -->
    
    
    <div class="d-flex align-items-center mt-4">
        <div>
            
            <img class="author-thumb" src="/assets/images/Luna.jpg" alt="Luna">
            
        </div>            
        <div>
        Written by <a target="_blank" class="text-dark" href="https://chaos-gravity.github.io/">Luna</a> on 
        <span class="post-date"><time class="post-date" datetime="2021-10-31">31 Oct 2021</time></span>           
        
        </div>            
    </div>
    
    
</div>

<!-- Adsense under title if enabled from _config.yml (change your pub id and slot) -->

    <script data-ad-client="ca-pub-6110174571048791" async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- Under Header -->
<!--
<ins class="adsbygoogle"
    style="display:block"
    data-ad-client="ca-pub-6110174571048791"
    data-ad-slot=""
    data-ad-format="auto"
    data-full-width-responsive="true"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<br/>
-->




<!-- Featured Image -->
<!--

<div class="entry-featured-image">
    
    <img class="featured-image " src="/assets/images/ViT (Vision Transformer)原理及代码解析/1.png" alt="ViT (Vision Transformer)原理及代码解析">
    
</div>

-->

<!-- Content -->
<!-- Post, Page Content
================================================== -->
<div class="article-post">
    <!-- Toc if any -->
    
    <!-- End Toc -->
    <div class="article-post-content">
    <p>    今天我们来详细了解一下Vision Transformer。基于timm的代码。</p>

<p><img src="../assets/images/ViT (Vision Transformer)原理及代码解析/1.png" style="zoom:80%;" /></p>

<p><strong>1. Patch Embedding</strong></p>

<p>Transformer原本是用来做NLP的工作的，所以ViT的首要任务是将图转换成词的结构，这里采取的方法是如上图左下角所示，将图片分割成小块，每个小块就相当于句子里的一个词。这里把每个小块称作Patch，而<strong>Patch Embedding</strong>就是把每个Patch再经过一个全连接网络压缩成一定维度的向量。</p>

<p>这里是<strong>VisionTransformer</strong>源代码中关于Patch Embedding的部分：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 默认img_size=224, patch_size=16，in_chans=3，embed_dim=768，
</span><span class="bp">self</span><span class="p">.</span><span class="n">patch_embed</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">embed_layer</span><span class="p">(</span>
<span class="err">    </span><span class="n">img_size</span><span class="o">=</span><span class="n">img_size</span><span class="p">,</span><span class="err"> </span><span class="n">patch_size</span><span class="o">=</span><span class="n">patch_size</span><span class="p">,</span><span class="err"> </span>
    <span class="n">in_chans</span><span class="o">=</span><span class="n">in_chans</span><span class="p">,</span><span class="err"> </span><span class="n">embed_dim</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">)</span>
</code></pre></div></div>

<p>而embed_layer其实是PatchEmbed：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PatchEmbed</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
<span class="err">    </span><span class="s">""" 2D Image to Patch Embedding
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">img_size</span><span class="o">=</span><span class="mi">224</span><span class="p">,</span> <span class="n">patch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">in_chans</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">norm_layer</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">flatten</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
<span class="err">        </span><span class="c1"># img_size = (img_size, img_size)
</span><span class="err">        </span><span class="n">img_size</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">to_2tuple</span><span class="p">(</span><span class="n">img_size</span><span class="p">)</span>
        <span class="n">patch_size</span> <span class="o">=</span> <span class="n">to_2tuple</span><span class="p">(</span><span class="n">patch_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">img_size</span> <span class="o">=</span> <span class="n">img_size</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">patch_size</span> <span class="o">=</span> <span class="n">patch_size</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">grid_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">img_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">patch_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">img_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="n">patch_size</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_patches</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">grid_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">grid_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">flatten</span> <span class="o">=</span> <span class="n">flatten</span>

<span class="err">        </span><span class="c1"># 输入通道，输出通道，卷积核大小，步长
</span><span class="err">        </span><span class="c1"># C*H*W-&gt;embed_dim*grid_size*grid_size
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_chans</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">patch_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">patch_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">norm_layer</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span> <span class="k">if</span> <span class="n">norm_layer</span> <span class="k">else</span> <span class="n">nn</span><span class="p">.</span><span class="n">Identity</span><span class="p">()</span>


    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
        <span class="k">assert</span> <span class="n">H</span> <span class="o">==</span> <span class="bp">self</span><span class="p">.</span><span class="n">img_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">W</span> <span class="o">==</span> <span class="bp">self</span><span class="p">.</span><span class="n">img_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> \
            <span class="s">f"Input image size (</span><span class="si">{</span><span class="n">H</span><span class="si">}</span><span class="s">*</span><span class="si">{</span><span class="n">W</span><span class="si">}</span><span class="s">) doesn't match model (</span><span class="si">{</span><span class="bp">self</span><span class="p">.</span><span class="n">img_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s">*</span><span class="si">{</span><span class="bp">self</span><span class="p">.</span><span class="n">img_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s">)."</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">flatten</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># BCHW -&gt; BNC
</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<p>proj虽然用的是卷积的写法，但其实是将每个patch接入了同样的全连接网络，将每个patch转换成了一个向量。x的维度是（B，C，H，W）其中B是batch size，C通常是三通道，H和W分别是图片的高和宽，而输出则是（B，N，E），B依然是batch size，N则是每张图被切割成了patch之后，patch的数量，E是embed_size，每个patch会通过一个全连接网络转换成一个向量，E是这个向量的长度，根据卷积的原理，也可以理解为每个patch的特征数量。</p>

<p><strong>2. Positional Encoding</strong></p>

<p>    把图片分割成了patch，然后把每个patch转换成了embedding，接下来就是在embedding中加入位置信息。产生位置信息的方式主要分两大类，一类是直接通过固定算法产生，一种是训练获得。但加位置信息的方式还是比较统一且粗暴的。</p>

<p><img src="../assets/images/ViT (Vision Transformer)原理及代码解析/2.jpeg" style="zoom:80%;" /></p>

<p>产生一个位置向量，长度和patch embedding一致，然后直接相加。那么这个位置向量大概长什么样呢？</p>

<p><img src="../assets/images/ViT (Vision Transformer)原理及代码解析/3.png" style="zoom:80%;" /></p>

<p>比如patch embedding长度为4，那么位置向量长度也为4，每个位置有一个在[-1,1]的值。</p>

<p><img src="../assets/images/ViT (Vision Transformer)原理及代码解析/4.png" style="zoom:80%;" /></p>

<p>假设你现在一张图切成了20个patch，embedding的长度是512，那么位置向量可以是上面这样的（tensor2tensor中的get_timing_signal_1d函数），每一行代表一个位置向量，第一行是位置0的位置向量，第二行是位置1的位置向量。</p>

<p>位置向量也可以是下面这样的（参考[1], [4]）：</p>

<p><img src="../assets/images/ViT (Vision Transformer)原理及代码解析/5.png" alt="" /></p>

<p>公式如下：</p>

<p><img src="../assets/images/ViT (Vision Transformer)原理及代码解析/6.png" style="zoom: 67%;" /></p>

<p>pos是单词在句子中的位置，或者patch在图中的位置，而i对应的是embedding的位置，dmodel对应的是patch embedding的长度，。这里说一下为什么要加这个位置编码，以及加上以后会有什么效果，我们观察上两幅图，可以发现，位置编码是随位置而改变的，位置差别越大的，那么向量差别也越大。在NLP课程里说过，把一个词转换成向量，就好像把一个词映射到了一个高维空间的位置，意思相近的词会在高维空间内比较靠近，而加上位置向量，会让位置相近的词更靠近，位置远的词离得更远。再来，为什么用cos，sin这种方式，作者的解释是，使用sin和cos编码可以得到词语之间的相对位置。</p>

<p><img src="../assets/images/ViT (Vision Transformer)原理及代码解析/7.jpeg" style="zoom: 80%;" /></p>

<p><img src="../assets/images/ViT (Vision Transformer)原理及代码解析/8.jpeg" style="zoom:80%;" /></p>

<p>这儿我是这么理解的，根据这两个公式，当我们知道了sin(pos+k)，cos(pos+k)，再知道了sin(pos)和cos(pos)，那么k的值我们是可以算出来的，而且用加减乘除就可以算出来。因此这样的编码方式不但能表达单词的位置，还能表达单词与单词之间的相对位置。</p>

<p>再看timm中对positional encoding的实现：</p>

<p><img src="../assets/images/ViT (Vision Transformer)原理及代码解析/9.jpeg" alt="" /></p>

<p>可以发现timm中的positional encoding是随机数，也就是说没有做positional encoding，可能只是给你留了个位置，而默认的值服从某种正太分布，且限于很小的数值区间，这里就不上代码和详细解释了。至于这里为什么是随机数。一个是保留位置，便你扩展，二是本来positional encoding就有两类方式可以实现，一种是用一定的算法生成，另外一种就是通过训练调整获得。timm应该是默认是通过训练来调整获得。   </p>

<p><strong>3. Self-Attention</strong></p>

<p>    接下来看ViT中的Attention，这和Transformer中的<strong>self-attention</strong>应该是一致的，我们先看看参考[1]是如何介绍self-attention的。参考[1]举了一个语义处理的例子，有一句话是这样的</p>

<p>    “The animal didn’t cross the street because it was too tired.”</p>

<p>我们人很容易理解，后面的it是指animal，但是要怎么让机器能够把it和animal关联起来呢？</p>

<p><img src="../assets/images/ViT (Vision Transformer)原理及代码解析/10.png" alt="" /></p>

<p>Self-attention就是在这种需求下产生的，如上图所示，我们应当有一个结构能够表达每个单词和其他每个单词的关系。那这里我们处理的是图像问题，Self-attention的存在就可以理解成，我们应当有一个结构能够表达每个patch和其他patch的关系。之前说过，图像中的patch和语义处理中的词可以同等来看。</p>

<p>    我们再来看具体怎么实现的：</p>

<p>    1. 基于输入向量创建三个向量：query向量，key向量和value向量。</p>

<p><img src="../assets/images/ViT (Vision Transformer)原理及代码解析/11.png" style="zoom:80%;" /></p>

<p>    2. 由query向量和key向量产生自注意力。</p>

<p><img src="../assets/images/ViT (Vision Transformer)原理及代码解析/12.png" style="zoom:80%;" /></p>

<p>    Thinking和Machine可以理解为图片被切分的两个patch，现在计算Thinking的自注意力，通过q乘k，除以一定系数（<strong>scaled dot-product attention</strong>，点积得到的结果值通常很大，使得softmax结果不能很好地表达attention值。这时候除以一个缩放因子，可以一定程度上减缓这种情况。），通过softmax之后会得到一个关于Thinking的注意力向量，比如这个例子是[0.88, 0.12]，这个向量的意思是，要解释Thinking这个词在这个句子中的意思，应当取0.88份Thinking原本的意思，再取0.12份Machine原本的意思，就是Thinking在这个句子中的意思。最后图中Sum之后的结果所表达的就是每个单词在这个句子当中的意思。整个过程可以用下面这张图表达：</p>

<p><img src="../assets/images/ViT (Vision Transformer)原理及代码解析/13.png" style="zoom:80%;" /></p>

<p><strong>4. Multi-Head Attention</strong></p>

<p>timm中attention是在self-attention基础上改进的<strong>multi-head attention</strong>，也就是在产生q，k，v的时候，对q，k，v进行了切分，分别分成了num_heads份，对每一份分别进行self-attention的操作，最后再拼接起来，这样在一定程度上进行了参数隔离，至于这样为什么效果会更好，我觉得应该是这样操作会让关联的特征集中在一起，更容易训练。</p>

<p><img src="../assets/images/ViT (Vision Transformer)原理及代码解析/14.png" style="zoom:80%;" /></p>

<p>   </p>

<p><img src="../assets/images/ViT (Vision Transformer)原理及代码解析/15.png" style="zoom: 80%;" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Attention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">qkv_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">attn_drop</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">proj_drop</span><span class="o">=</span><span class="mf">0.</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
<span class="err">        </span><span class="c1"># q,k,v向量长度
</span>        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">dim</span> <span class="o">//</span> <span class="n">num_heads</span>
<span class="err">        </span><span class="bp">self</span><span class="p">.</span><span class="n">scale</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">head_dim</span><span class="err"> </span><span class="o">**</span><span class="err"> </span><span class="o">-</span><span class="mf">0.5</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">qkv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">qkv_bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">attn_drop</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">attn_drop</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">proj_drop</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">proj_drop</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="err">        </span><span class="c1"># 这里C对应上面的E，向量的长度
</span>        <span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
<span class="err">        </span><span class="c1"># (B, N, C) -&gt; (3，B，num_heads, N, C//num_heads), //是向下取整的意思。
</span>        <span class="n">qkv</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">qkv</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">C</span> <span class="o">//</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">).</span><span class="n">permute</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="err">        </span><span class="c1"># 将qkv在0维度上切成三个数据块，q,k,v:(B，num_heads, N, C//num_heads)
</span><span class="err">        </span><span class="c1"># 这里的效果是从每个向量产生三个向量，分别是query，key和value
</span>        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">qkv</span><span class="p">.</span><span class="n">unbind</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>   <span class="c1"># make torchscript happy (cannot use tensor as tuple)
</span><span class="err">        </span><span class="c1"># @矩阵相乘获得score (B,num_heads,N,N)
</span>        <span class="n">attn</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">scale</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">attn_drop</span><span class="p">(</span><span class="n">attn</span><span class="p">)</span>
<span class="err">        </span><span class="c1"># (B,num_heads,N,N)@(B,num_heads,N,C//num_heads)-&gt;(B,num_heads,N,C//num_heads)
</span><span class="err">        </span><span class="c1"># (B,num_heads,N,C//num_heads) --&gt;(B,N,num_heads,C//num_heads)
</span><span class="err">        </span><span class="c1"># (B,N,num_heads,C//num_heads) -&gt; (B, N, C)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">attn</span> <span class="o">@</span> <span class="n">v</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
<span class="err">        </span><span class="c1"># (B, N, C) -&gt; (B, N, C)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">proj_drop</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<p>multi-head attention的总示意图如下：</p>

<p><img src="../assets/images/ViT (Vision Transformer)原理及代码解析/16.png" style="zoom:80%;" /></p>

<p><strong>5. Layer Normalization</strong></p>

<p>    Layer normalization对应的一个概念是我们熟悉的Batch Normalization，这两个根本的不同在于，Layer normalization是对每个样本的所有特征进行归一化，而Batch Normalization是对每个通道的所有样本进行归一化。</p>

<p>    为了便于理解，这里贴一下官网给LN的示例代码：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># NLP Example
</span><span class="n">batch</span><span class="p">,</span><span class="err"> </span><span class="n">sentence_length</span><span class="p">,</span><span class="err"> </span><span class="n">embedding_dim</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="mi">20</span><span class="p">,</span><span class="err"> </span><span class="mi">5</span><span class="p">,</span><span class="err"> </span><span class="mi">10</span>
<span class="n">embedding</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span><span class="err"> </span><span class="n">sentence_length</span><span class="p">,</span><span class="err"> </span><span class="n">embedding_dim</span><span class="p">)</span>
<span class="c1"># 指定归一化的维度
</span><span class="n">layer_norm</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embedding_dim</span><span class="p">)</span>
<span class="c1"># 进行归一化
</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span>


<span class="c1"># Image Example
</span><span class="n">N</span><span class="p">,</span><span class="err"> </span><span class="n">C</span><span class="p">,</span><span class="err"> </span><span class="n">H</span><span class="p">,</span><span class="err"> </span><span class="n">W</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="mi">20</span><span class="p">,</span><span class="err"> </span><span class="mi">5</span><span class="p">,</span><span class="err"> </span><span class="mi">10</span><span class="p">,</span><span class="err"> </span><span class="mi">10</span>
<span class="nb">input</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="err"> </span><span class="n">C</span><span class="p">,</span><span class="err"> </span><span class="n">H</span><span class="p">,</span><span class="err"> </span><span class="n">W</span><span class="p">)</span>
<span class="c1"># Normalize over the last three dimensions (i.e. the channel and spatial dimensions)
# as shown in the image below
</span><span class="n">layer_norm</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">([</span><span class="n">C</span><span class="p">,</span><span class="err"> </span><span class="n">H</span><span class="p">,</span><span class="err"> </span><span class="n">W</span><span class="p">])</span>
<span class="n">output</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">layer_norm</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="../assets/images/ViT (Vision Transformer)原理及代码解析/17.jpeg" style="zoom:67%;" /></p>

<p>在ViT中，虽然LN处理的是图片数据，但在进行LN之前，图片已经被切割成了Patch，而每个Patch表示的是一个词，因此是在用语义的逻辑在解决视觉问题，因此在ViT中，LN也是按语义的逻辑在用的。关于这个概念的详细细节，可以参考[3]和[2]。</p>

<p><strong>6. Drop Path</strong></p>

<p>    Dropout是最早用于解决网络过拟合的方法，是所有drop类方法的始祖。方法示意图如下：</p>

<p><img src="../assets/images/ViT (Vision Transformer)原理及代码解析/18.png" style="zoom:80%;" /></p>

<p>在向前传播的时候，让神经元以一定概率停止工作。这样可以使模型泛化能力变强，因为神经元会以一定概率失效，这样的机制会使结果不会过分依赖于个别神经元。训练阶段，以keep_prob概率使神经元失效，而推理的时候，会保留所有神经元的有效性，因此，训练时候加了dropout的神经元推理出来的结果要乘以keep_prob。</p>

<p>    接下来以dropout的思路来理解drop path，drop path没找到示意图，那直接看timm上的代码：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">def</span><span class="err"> </span><span class="n">drop_path</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="err"> </span><span class="n">drop_prob</span><span class="p">:</span><span class="err"> </span><span class="nb">float</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="mf">0.</span><span class="p">,</span><span class="err"> </span><span class="n">training</span><span class="p">:</span><span class="err"> </span><span class="nb">bool</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="bp">False</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">drop_prob</span> <span class="o">==</span> <span class="mf">0.</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">training</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span>
<span class="err">    </span><span class="c1"># drop_prob是进行droppath的概率
</span>    <span class="n">keep_prob</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">drop_prob</span>
    <span class="c1"># work with diff dim tensors, not just 2D ConvNets
</span><span class="err">    </span><span class="c1"># 在ViT中，shape是(B,1,1),B是batch size
</span>    <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
<span class="err">    </span><span class="c1"># 按shape,产生0-1之间的随机向量,并加上keep_prob  
</span>    <span class="n">random_tensor</span> <span class="o">=</span> <span class="n">keep_prob</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
<span class="err">    </span><span class="c1"># 向下取整，二值化，这样random_tensor里1出现的概率的期望就是keep_prob
</span>    <span class="n">random_tensor</span><span class="p">.</span><span class="n">floor_</span><span class="p">()</span>  <span class="c1"># binarize
</span><span class="err">    </span><span class="c1"># 将一定图层变为0
</span>    <span class="n">output</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">div</span><span class="p">(</span><span class="n">keep_prob</span><span class="p">)</span> <span class="o">*</span> <span class="n">random_tensor</span>
    <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div>

<p>由代码可以看出，drop path是在batch那个维度，随机将一些图层直接变成0，以加快运算速度。</p>

<p><strong>7. Encoder</strong></p>

<p>Transformer的架构图：</p>

<p><img src="../assets/images/ViT (Vision Transformer)原理及代码解析/19.png" style="zoom:67%;" /></p>

<p>    Transformer是由一堆encoder和decoder形成的，那encoder一般的架构图如下：</p>

<p><img src="../assets/images/ViT (Vision Transformer)原理及代码解析/20.png" style="zoom:80%;" /></p>

<p>Encoder在ViT中的实现细节如下面代码所示（layer normalization -&gt; multi-head attention -&gt; drop path -&gt; layer normalization -&gt; mlp -&gt; drop path），换了个名字，叫block了：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Block</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>


    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="o">=</span><span class="mf">4.</span><span class="p">,</span> <span class="n">qkv_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">drop</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">attn_drop</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
                 <span class="n">drop_path</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">act_layer</span><span class="o">=</span><span class="n">nn</span><span class="p">.</span><span class="n">GELU</span><span class="p">,</span> <span class="n">norm_layer</span><span class="o">=</span><span class="n">nn</span><span class="p">.</span><span class="n">LayerNorm</span><span class="p">):</span>
<span class="err">        </span><span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
<span class="err">        </span><span class="c1"># 将每个样本的每个通道的特征向量做归一化
</span><span class="err">        </span><span class="c1"># 也就是说每个特征向量是独立做归一化的
</span><span class="err">        </span><span class="c1"># 我们这里虽然是图片数据，但图片被切割成了patch，用的是语义的逻辑
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">norm_layer</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">qkv_bias</span><span class="o">=</span><span class="n">qkv_bias</span><span class="p">,</span> <span class="n">attn_drop</span><span class="o">=</span><span class="n">attn_drop</span><span class="p">,</span> <span class="n">proj_drop</span><span class="o">=</span><span class="n">drop</span><span class="p">)</span>
<span class="err">        </span><span class="c1"># NOTE: drop path for stochastic depth, we shall see if this is better than dropout here
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">drop_path</span> <span class="o">=</span> <span class="n">DropPath</span><span class="p">(</span><span class="n">drop_path</span><span class="p">)</span> <span class="k">if</span> <span class="n">drop_path</span> <span class="o">&gt;</span> <span class="mf">0.</span> <span class="k">else</span> <span class="n">nn</span><span class="p">.</span><span class="n">Identity</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">norm_layer</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
        <span class="n">mlp_hidden_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">dim</span> <span class="o">*</span> <span class="n">mlp_ratio</span><span class="p">)</span>
<span class="err">        </span><span class="c1"># 全连接，激励，drop，全连接，drop,若out_features没填，那么输出维度不变。
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">Mlp</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">hidden_features</span><span class="o">=</span><span class="n">mlp_hidden_dim</span><span class="p">,</span> <span class="n">act_layer</span><span class="o">=</span><span class="n">act_layer</span><span class="p">,</span> <span class="n">drop</span><span class="o">=</span><span class="n">drop</span><span class="p">)</span>


<span class="err">    </span><span class="n">def</span><span class="err"> </span><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="err"> </span><span class="n">x</span><span class="p">):</span>
<span class="err">        </span><span class="c1"># 最后一维归一化，multi-head attention, drop_path
</span><span class="err">        </span><span class="c1"># (B, N, C) -&gt; (B, N, C)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">drop_path</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">attn</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
<span class="err">        </span><span class="c1"># (B, N, C) -&gt; (B, N, C)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">drop_path</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">mlp</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<p>在ViT中这样的block会有好几层，形成blocks：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># stochastic depth decay rule
</span><span class="n">dpr</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="p">[</span><span class="n">x</span><span class="p">.</span><span class="n">item</span><span class="p">()</span><span class="err"> </span><span class="k">for</span><span class="err"> </span><span class="n">x</span><span class="err"> </span><span class="ow">in</span><span class="err"> </span><span class="n">torch</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="err"> </span><span class="n">drop_path_rate</span><span class="p">,</span><span class="err"> </span><span class="n">depth</span><span class="p">)]</span>
<span class="bp">self</span><span class="p">.</span><span class="n">blocks</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="p">[</span>
<span class="err">    </span><span class="n">Block</span><span class="p">(</span>
<span class="err">        </span><span class="n">dim</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">,</span><span class="err"> </span><span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span><span class="err"> </span><span class="n">mlp_ratio</span><span class="o">=</span><span class="n">mlp_ratio</span><span class="p">,</span><span class="err"> </span><span class="n">qkv_bias</span><span class="o">=</span><span class="n">qkv_bias</span><span class="p">,</span><span class="err"> </span><span class="n">drop</span><span class="o">=</span><span class="n">drop_rate</span><span class="p">,</span>
<span class="err">        </span><span class="n">attn_drop</span><span class="o">=</span><span class="n">attn_drop_rate</span><span class="p">,</span><span class="err"> </span><span class="n">drop_path</span><span class="o">=</span><span class="n">dpr</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="err"> </span><span class="n">norm_layer</span><span class="o">=</span><span class="n">norm_layer</span><span class="p">,</span><span class="err"> </span><span class="n">act_layer</span><span class="o">=</span><span class="n">act_layer</span><span class="p">)</span>
<span class="err">    </span><span class="k">for</span><span class="err"> </span><span class="n">i</span><span class="err"> </span><span class="ow">in</span><span class="err"> </span><span class="nb">range</span><span class="p">(</span><span class="n">depth</span><span class="p">)])</span>
</code></pre></div></div>

<p>如果drop_path_rate大于0，每一层block的drop_path的会线性增加。depth是一个blocks里block的数量。也可以理解为blocks这个网络块的深度。</p>

<p><strong>8. Forward Features</strong></p>

<p>Patch embedding -&gt; 加cls -&gt; 加pos embedding -&gt; 用blocks进行encoding -&gt; layer normalization -&gt; 输出图的embedding</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">def</span><span class="err"> </span><span class="n">forward_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="err"> </span><span class="n">x</span><span class="p">):</span>
<span class="err">    </span><span class="c1"># x由（B，C，H，W）-&gt;（B，N，E）
</span><span class="err">    </span><span class="n">x</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="bp">self</span><span class="p">.</span><span class="n">patch_embed</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="err">    </span><span class="c1"># stole cls_tokens impl from Phil Wang, thanks
</span><span class="err">    </span><span class="c1"># cls_token由(1, 1, 768)-&gt;(B, 1, 768), B是batch_size
</span><span class="err">    </span><span class="n">cls_token</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="bp">self</span><span class="p">.</span><span class="n">cls_token</span><span class="p">.</span><span class="n">expand</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="err"> </span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="err"> </span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="err">    </span><span class="c1"># dist_token是None,DeiT models才会用到dist_token。
</span><span class="err">    </span><span class="k">if</span><span class="err"> </span><span class="bp">self</span><span class="p">.</span><span class="n">dist_token</span><span class="err"> </span><span class="ow">is</span><span class="err"> </span><span class="bp">None</span><span class="p">:</span>
<span class="err">        </span><span class="c1"># x由(B, N, E)-&gt;(B, 1+N, E)
</span><span class="err">        </span><span class="n">x</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">((</span><span class="n">cls_token</span><span class="p">,</span><span class="err"> </span><span class="n">x</span><span class="p">),</span><span class="err"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="err">    </span><span class="k">else</span><span class="p">:</span>
<span class="err">        </span><span class="c1"># x由(B, N, E)-&gt;(B, 2+N, E)
</span><span class="err">        </span><span class="n">x</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">((</span><span class="n">cls_token</span><span class="p">,</span><span class="err"> </span><span class="bp">self</span><span class="p">.</span><span class="n">dist_token</span><span class="p">.</span><span class="n">expand</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="err"> </span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="err"> </span><span class="o">-</span><span class="mi">1</span><span class="p">),</span><span class="err"> </span><span class="n">x</span><span class="p">),</span><span class="err"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="err">    </span><span class="c1"># +pos_embed:(1, 1+N, E)，再加一个dropout层
</span><span class="err">    </span><span class="n">x</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="bp">self</span><span class="p">.</span><span class="n">pos_drop</span><span class="p">(</span><span class="n">x</span><span class="err"> </span><span class="o">+</span><span class="err"> </span><span class="bp">self</span><span class="p">.</span><span class="n">pos_embed</span><span class="p">)</span>
<span class="err">    </span><span class="n">x</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="bp">self</span><span class="p">.</span><span class="n">blocks</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="err">    </span><span class="c1"># nn.LayerNorm
</span><span class="err">    </span><span class="n">x</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="bp">self</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="err">    </span><span class="k">if</span><span class="err"> </span><span class="bp">self</span><span class="p">.</span><span class="n">dist_token</span><span class="err"> </span><span class="ow">is</span><span class="err"> </span><span class="bp">None</span><span class="p">:</span>
<span class="err">        </span><span class="c1"># 不是DeiT，输出就是x[:,0]，(B, 1, 768)，即cls_token
</span><span class="err">        </span><span class="k">return</span><span class="err"> </span><span class="bp">self</span><span class="p">.</span><span class="n">pre_logits</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="err"> </span><span class="mi">0</span><span class="p">])</span>
<span class="err">    </span><span class="k">else</span><span class="p">:</span>
<span class="err">        </span><span class="c1"># 是DeiT，输出就是cls_token和dist_token
</span><span class="err">        </span><span class="k">return</span><span class="err"> </span><span class="n">x</span><span class="p">[:,</span><span class="err"> </span><span class="mi">0</span><span class="p">],</span><span class="err"> </span><span class="n">x</span><span class="p">[:,</span><span class="err"> </span><span class="mi">1</span><span class="p">]</span>
</code></pre></div></div>

<p>这里在patch 那个维度加入了一个cls_token，可以这样理解这个存在，其他的embedding表达的都是不同的patch的特征，而cls_token是要综合所有patch的信息，产生一个新的embedding，来表达整个图的信息。而dist_token则是属于DeiT网络的结构。</p>

<p><strong>9. Forward</strong></p>

<p>这就是这个模型的总流程了：forward features -&gt; 最终输出</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="err">        </span><span class="c1">#（B，C，H，W）-&gt; (B, 1, 768)
</span><span class="err">        </span><span class="c1"># (B,C,H,W) -&gt; (B, 1, 768), (B, 1, 768)
</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">forward_features</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="err">        </span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">head_dist</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
<span class="err">            </span><span class="c1"># 如果num_classes&gt;0, (B, 1, 768)-&gt;(B, 1, num_classes)
</span>            <span class="c1"># 否则不变
</span><span class="err">            </span><span class="n">x</span><span class="p">,</span><span class="err"> </span><span class="n">x_dist</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="bp">self</span><span class="p">.</span><span class="n">head</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span><span class="err"> </span><span class="bp">self</span><span class="p">.</span><span class="n">head_dist</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="err">            </span><span class="k">if</span><span class="err"> </span><span class="bp">self</span><span class="p">.</span><span class="n">training</span><span class="err"> </span><span class="ow">and</span><span class="err"> </span><span class="ow">not</span><span class="err"> </span><span class="n">torch</span><span class="p">.</span><span class="n">jit</span><span class="p">.</span><span class="n">is_scripting</span><span class="p">():</span>
                <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">x_dist</span>
            <span class="k">else</span><span class="p">:</span>
<span class="err">                </span><span class="c1"># during inference, 
</span>                <span class="c1"># return the average of both classifier predictions
</span>                <span class="k">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">x_dist</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>
<span class="err">            </span><span class="c1"># 如果num_classes&gt;0, (B, 1, 768)-&gt;(B, 1, num_classes)
</span><span class="err">            </span><span class="c1"># 否则不变
</span>            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<p>这样ViT算是给我说完了，DeiT又涉及到很多新的概念，之后也会参考代码，进行详细解说。</p>

<p>    觉得有用，关注，在看，点赞，转发分享来一波哦！<img src="../assets/images/ViT (Vision Transformer)原理及代码解析/21.png" alt="" /></p>

<p>参考：</p>

<p>[1] Jay Alammar, The Illustrated Transformer, jalammar.github.io, 2018</p>

<p>[2] 简枫，聊聊 Transformer，知乎，2019</p>

<p>[3] 大师兄，模型优化之Layer Normalization，知乎，2020</p>

<p>[4] TensorFlow Core，理解语言的 Transformer 模型，TensorFlow，https://www.tensorflow.org/tutorials/text/transformer</p>

    </div>
</div>


<!-- Rating -->


<!-- Author Box if enabled from _config.yml -->
<!-- Author Box -->




<!-- Comments if not disabled with comments: false -->
<!-- Comments
================================================== -->
 
<div class="comments">
    <button class="btn btn-dark show-comments">Load Comments</button>         
    <div id="comments">  
        <h4 class="mb-4">Comments</h4>                 
            <section class="disqus">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        var disqus_shortname = 'chaos-gravity'; 
        var disqus_developer = 0;
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = window.location.protocol + '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>
     
    <div class="clearfix"></div>              
    </div>    
</div>       


<!-- Share -->
<div class="share">
    <p>
        Share
    </p>
    <ul>
        <li class="ml-1 mr-1">
            <a target="_blank" href="https://twitter.com/intent/tweet?text=ViT (Vision Transformer)原理及代码解析&url=http://localhost:4000/ViT-(Vision-Transformer)%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                <i class="fab fa-twitter"></i>
            </a>
        </li>

        <li class="ml-1 mr-1">
            <a target="_blank" href="https://facebook.com/sharer.php?u=http://localhost:4000/ViT-(Vision-Transformer)%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/" onclick="window.open(this.href, 'facebook-share', 'width=550,height=435');return false;">
                <i class="fab fa-facebook-f"></i>
            </a>
        </li>

        <li class="ml-1 mr-1">
            <a target="_blank" href="https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/ViT-(Vision-Transformer)%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/" onclick="window.open(this.href, 'width=550,height=435');return false;">
                <i class="fab fa-linkedin-in"></i>
            </a>
        </li>

    </ul>
</div>


<!-- Related Post -->
<!-- Related Posts
================================================== -->
<div class=" related-posts ">  

    
    <h2 class="text-center mb-4">Explore more like this</h2>
    
    
    <div class="d-flex justify-content-center align-items-center">
    
    <!-- Categories -->
    
    
    <a class="smoothscroll badge badge-primary text-capitalize" href="/categories#CV">CV</a>                
    
    <a class="smoothscroll badge badge-primary text-capitalize" href="/categories#Representation-Learning">Representation Learning</a>                
    

    <!-- Tags -->  
    
                    
    <a class="smoothscroll badge badge-primary text-capitalize" href="/tags#ViT">ViT</a>               
    

    </div>

    
    
    
    <div class="blog-grid-container">
        
        
            
            
        
            
        
            
        
        
        
            
            
        
            
        
            
        
        
        
            
            
        
            
        
            
            <!-- begin post -->


<div class="blog-grid-item">
    <div class="card h-100">
        <div class="maxthumb">
            <a href="/%E9%92%A2%E7%90%B4%E6%9B%B2%E8%BD%AC%E8%B0%B1-Solo-Piano-Transcription/">
                

                    
                        <img class="img-thumb" src="/assets/images/piano_trans_paper_2020/0.png" alt="钢琴曲转谱（Solo Piano Transcription）"> 
                    

                
            </a>
        </div>
        <div class="card-body">
            <h2 class="card-title">
                <a class="text-dark" href="/%E9%92%A2%E7%90%B4%E6%9B%B2%E8%BD%AC%E8%B0%B1-Solo-Piano-Transcription/">钢琴曲转谱（Solo Piano Transcription）</a>
                
            </h2>
            <h4 class="card-text">​    今天来看一篇钢琴琴谱翻译的文章，出自ByteDance字节跳动，Giant-Piano（GiantMIDI-Piano：字节跳动发布的大型古典钢琴乐MIDI数据集）采用的转谱模型就是这个：
</h4>
        </div>
        <div class="card-footer bg-white">
            <div class="wrapfooter">
                
                <span class="meta-footer-thumb">
                
                <img class="author-thumb" src="/assets/images/Luna.jpg" alt="Luna">
                
                </span>
                <span class="author-meta">
                <span class="post-name"><a target="_blank" href="https://chaos-gravity.github.io/">Luna</a></span> 
                
                <span class="post-date">28 Jun 2022</span>
                </span>
                <div class="clearfix"></div>
            </div>
        </div>
    </div>
</div>
<!-- end post -->

            
            
            
        
        
        
            
            
        
            
        
            
            <!-- begin post -->


<div class="blog-grid-item">
    <div class="card h-100">
        <div class="maxthumb">
            <a href="/UC-Berkeley%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-Implicit-Models-GANs-(%E4%B8%8A)/">
                

                    
                        <img class="img-thumb" src="/assets/images/UC Berkeley非监督学习--Implicit Models -- GANs (上)/1.png" alt="UC Berkeley非监督学习--Implicit Models -- GANs (上)"> 
                    

                
            </a>
        </div>
        <div class="card-body">
            <h2 class="card-title">
                <a class="text-dark" href="/UC-Berkeley%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-Implicit-Models-GANs-(%E4%B8%8A)/">UC Berkeley非监督学习--Implicit Models -- GANs (上)</a>
                
            </h2>
            <h4 class="card-text">    翻译整理一下UC Berkeley非监督学习的课程。这篇翻译第五六讲Implicit Models – GANs。分三篇：上，中，下。



    这个课程总共十二讲，官方链接：

http</h4>
        </div>
        <div class="card-footer bg-white">
            <div class="wrapfooter">
                
                <span class="meta-footer-thumb">
                
                <img class="author-thumb" src="/assets/images/Luna.jpg" alt="Luna">
                
                </span>
                <span class="author-meta">
                <span class="post-name"><a target="_blank" href="https://chaos-gravity.github.io/">Luna</a></span> 
                
                <span class="post-date">10 May 2022</span>
                </span>
                <div class="clearfix"></div>
            </div>
        </div>
    </div>
</div>
<!-- end post -->

            
            
            
        
        
        
            
            
        
            
        
            
            <!-- begin post -->


<div class="blog-grid-item">
    <div class="card h-100">
        <div class="maxthumb">
            <a href="/UC-Berkeley%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-Latent-Variable-Models-VAE-%E6%BD%9C%E5%8F%98%E9%87%8F%E6%A8%A1%E5%9E%8B-VAE/">
                

                    
                        <img class="img-thumb" src="/assets/images/UC Berkeley非监督学习--Latent Variable Models -- VAE（潜变量模型--VAE）/1.png" alt="UC Berkeley非监督学习--Latent Variable Models -- VAE（潜变量模型--VAE）"> 
                    

                
            </a>
        </div>
        <div class="card-body">
            <h2 class="card-title">
                <a class="text-dark" href="/UC-Berkeley%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-Latent-Variable-Models-VAE-%E6%BD%9C%E5%8F%98%E9%87%8F%E6%A8%A1%E5%9E%8B-VAE/">UC Berkeley非监督学习--Latent Variable Models -- VAE（潜变量模型--VAE）</a>
                
            </h2>
            <h4 class="card-text">    翻译整理一下UC Berkeley非监督学习的课程。这篇翻译第四讲Latent Variable Models – VAE。



    这个课程总共十二讲，官方链接：

https://s</h4>
        </div>
        <div class="card-footer bg-white">
            <div class="wrapfooter">
                
                <span class="meta-footer-thumb">
                
                <img class="author-thumb" src="/assets/images/Luna.jpg" alt="Luna">
                
                </span>
                <span class="author-meta">
                <span class="post-name"><a target="_blank" href="https://chaos-gravity.github.io/">Luna</a></span> 
                
                <span class="post-date">06 May 2022</span>
                </span>
                <div class="clearfix"></div>
            </div>
        </div>
    </div>
</div>
<!-- end post -->

            
            
                
        </div>        
</div>

<!-- Review with LD-JSON, adapt it for your needs if you like, but make sure you test the generated HTML source code first: 
https://search.google.com/structured-data/testing-tool/u/0/
================================================== -->


    </div>

    
    <!-- Newsletter
    ================================================== -->
    <div class="newsletter text-center">
        <span class="h4"><img src="/assets/images/logo.png" class="newsletter-logo" alt="Chaos万有引力"> &nbsp; Never miss a piece of <b>information</b> from us, subscribe to our newsletter</span>
        <form action="https://github.us10.list-manage.com/subscribe/post?u=af33f9baa54232f085e579b0f&amp;id=1548279ad6" method="post" name="mc-embedded-subscribe-form" class="wj-contact-form validate" target="_blank" novalidate>
            <div class="mc-field-group d-inline-flex">
            <input type="email" placeholder="Your e-mail" name="EMAIL" class="required email" id="mce-EMAIL" autocomplete="on" required>
            <input type="submit" value="Subscribe" name="subscribe" class="heart">
            </div>
        </form>
    </div>
    
    
</div>

<!-- Begin Footer
================================================== -->
<footer class="footer">
    <div class="container">
        <div class="row">
            <div class="col-md-6 col-sm-12 text-center text-lg-left">
                Copyright © 2022 Chaos万有引力 
            </div>
            <div class="col-md-6 col-sm-12 text-center text-lg-right">    
                <a target="_blank" href="https://chaos-gravity.github.io/">Chaos-Gravity</a> by Beer
            </div>
        </div>
    </div>
</footer>
<!-- End Footer
================================================== -->

</div> <!-- /.site-content -->

<!-- Scripts (if you need bootstrap.js, please add it yourself. I didn't use it for performance reasons, it was not needed in this theme)
================================================== -->

<script src="/assets/js/prism.js"></script>

<script src="/assets/js/theme.js"></script>




<script id="dsq-count-scr" src="//chaos-gravity.disqus.com/count.js"></script>


</body>
</html>
