{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "def get_headers(config):\n",
    "    headers = {\n",
    "        \"Cookie\": config['cookie'],\n",
    "        \"User-Agent\": config['user-agent']\n",
    "    }\n",
    "    return headers\n",
    "def get_params(config):\n",
    "    url = \"https://mp.weixin.qq.com/cgi-bin/appmsgpublish\"\n",
    "    begin = \"0\"\n",
    "    params = {\n",
    "        \"sub\": \"list\",\n",
    "        \"search_field\":\"null\",\n",
    "        \"sub_action\": \"list_ex\", \n",
    "        \"begin\": begin,\n",
    "        \"free_publish_type\": \"1\",\n",
    "        \"count\": \"5\",\n",
    "        \"fakeid\": config['fakeid'],\n",
    "        \"type\": \"101_1\",\n",
    "        \"token\": config['token'],\n",
    "        \"lang\": \"zh_CN\",\n",
    "        \"f\": \"json\",\n",
    "        \"ajax\": \"1\"\n",
    "    }\n",
    "    return params\n",
    "def get_article_list(headers, params):\n",
    "    i = 0\n",
    "    url = \"https://mp.weixin.qq.com/cgi-bin/appmsgpublish\"\n",
    "    column_name = \"aid,appmsgid,author_name,title,cover_img,digest,link,create_time\"\n",
    "    article_list_path = \"article_list2.csv\"\n",
    "    with open(article_list_path, \"a\") as f:\n",
    "        f.write(column_name + '\\n')\n",
    "    while True:\n",
    "        begin = i * 5\n",
    "        params[\"begin\"] = str(begin)\n",
    "        # 随机暂停几秒，避免过快的请求导致过快的被查到\n",
    "        time.sleep(random.randint(1,10))\n",
    "        resp = requests.get(url, headers=headers, params = params, verify=False)\n",
    "        # 微信流量控制, 退出\n",
    "        if resp.json()['base_resp']['ret'] == 200013:\n",
    "            print(\"frequencey control, stop at {}\".format(str(begin)))\n",
    "            time.sleep(3600)\n",
    "            continue\n",
    "        \n",
    "        if i == \"0\":\n",
    "            total_count = eval(resp.json()['publish_page'])['total_count']\n",
    "            print(\"We have \"+str(tatal_count) + \" articles.\")\n",
    "        publish_list = eval(resp.json()['publish_page'])['publish_list']\n",
    "        # 如果返回的内容中为空则结束\n",
    "        if len(publish_list) == 0:\n",
    "            print(\"all ariticle parsed\")\n",
    "            break\n",
    "    \n",
    "        for publish in publish_list:\n",
    "            publish = eval(publish['publish_info'].replace(\"true\",\"True\").replace(\"false\",\"False\"))['appmsgex'][0]\n",
    "            info = '\"{}\",\"{}\",\"{}\",\"{}\",\"{}\",\"{}\",\"{}\",\"{}\"'.format(str(publish[\"aid\"]), \\\n",
    "                str(publish['appmsgid']), str(publish['author_name']), \\\n",
    "                str(publish['title'].replace(\"\\n\",\"\").replace(\",\",\";\")), \\\n",
    "                str(publish['cover']), str(publish['digest'].replace(\"\\n\",\"\").replace(\",\",\";\")), \\\n",
    "                str(publish['link']), str(publish['create_time']))\n",
    "            with open(article_list_path, \"a\") as f:\n",
    "                f.write(info+'\\n')\n",
    "            print(\"\\n\".join(info.split(\",\")))\n",
    "            print(\"\\n\\n---------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "        # 翻页\n",
    "        i += 1\n",
    "def main():\n",
    "    with open(\"chaos-gravity.yaml\", \"r\") as file:\n",
    "        file_data = file.read()\n",
    "    config = yaml.safe_load(file_data) \n",
    "    headers = get_headers(config)\n",
    "    params = get_params(config)\n",
    "    get_article_list(headers, params)\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "import html2text as ht\n",
    "import yaml\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import tomd\n",
    "import re\n",
    "\n",
    "with open(\"chaos-gravity.yaml\", \"r\") as file:\n",
    "    file_data = file.read()\n",
    "config = yaml.safe_load(file_data)\n",
    "def get_headers(config):\n",
    "    headers = {\n",
    "        \"Cookie\": config['cookie'],\n",
    "        \"User-Agent\": config['user-agent']\n",
    "    }\n",
    "    return headers\n",
    "\n",
    "pd.set_option('max_colwidth', 1000)\n",
    "article_list = pd.read_csv('article_list2.csv', encoding='GB18030')\n",
    "headers = get_headers(config)\n",
    "\n",
    "    \n",
    "def download_images(soup, content):\n",
    "    dir_name = \"assets/images/\" + str(filename[:-3]) + \"/\"\n",
    "    if not os.path.isdir(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "    cnt = 1\n",
    "    images = content.find_all(\"img\")\n",
    "    for image in images:\n",
    "        img_src = image.get('data-src')\n",
    "        img_type = image.get('data-type')\n",
    "        img_name = \"{0:d}.{1:s}\".format(cnt, img_type if img_type else 'png')\n",
    "        cnt += 1\n",
    "        file_path = \"assets/images/{0:s}/{1:s}\".format(filename[:-3], img_name)\n",
    "        if not os.path.exists(file_path):\n",
    "            with open (file_path, 'wb') as file:\n",
    "                response = requests.get(url = img_src)\n",
    "                for block in response.iter_content(1024):\n",
    "                    if block:\n",
    "                        file.write(block)\n",
    "                    else:\n",
    "                        break\n",
    "        tag = soup.new_tag('span')\n",
    "        tag.string = \"![](assets/images/{0:s}/{1:s})\".format(filename[:-3], img_name)\n",
    "        image.replace_with(tag)\n",
    "\n",
    "        \n",
    "def download_videos(soup, content):\n",
    "    dir_name = \"assets/videos/\" + str(title) + \"/\"\n",
    "    if not os.path.isdir(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "    if videos:\n",
    "        for video in videos:\n",
    "            video_src = video.get('data-src')\n",
    "\n",
    "            tag = soup.new_tag('span')\n",
    "            tag.string = \"[此处是视频]({0:s})\".format(video_src)\n",
    "            video.replace_with(tag)\n",
    "\n",
    "def for_list(content):\n",
    "    for a in content.find_all(\"ol\"):\n",
    "        i = 1\n",
    "        for b in a.find_all(\"li\"):\n",
    "            tag = soup.new_tag('span')\n",
    "            tag.string = \"{0:d}. {1:s}\\n\".format(i, b.text)\n",
    "            b.replace_with(tag)\n",
    "            i = i+1\n",
    "def for_code2(content):\n",
    "    for a in content.find_all(class_=\"code-snippet__fix code-snippet__js\"):\n",
    "        for b in a.find_all(\"code\"):\n",
    "            tag = soup.new_tag('code')\n",
    "            tag.string = \"\\n\" + b.text\n",
    "            b.replace_with(tag)            \n",
    "def for_code(content):\n",
    "    for a in content.find_all(class_=\"code-snippet__fix code-snippet__js\"):\n",
    "        tag = soup.new_tag('code')\n",
    "        string = \"\"\n",
    "        length = len(a.find_all(\"code\"))\n",
    "        i = 1\n",
    "        for b in a.find_all(\"code\"):\n",
    "            string = string + \"\\n\" + b.text\n",
    "            if i<length:\n",
    "                b.extract()\n",
    "            i = i+1\n",
    "        tag.string = string\n",
    "        b.replace_with(tag)\n",
    "for index, row in article_list.iterrows():\n",
    "    print(index)\n",
    "    print(row)\n",
    "    create_time = time.strftime(\"%Y-%m-%d\",time.localtime(row[\"create_time\"]))\n",
    "    url = row['link'].replace(\"\\/\",\"/\")\n",
    "    article = requests.get(url,headers=headers)\n",
    "    if article.status_code == 200:\n",
    "        html = article.text\n",
    "    # for_code \n",
    "    html = html.replace(\"<br  />\",\"\\n\")\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    content = soup.find(id='img-content')\n",
    "    #有些文章已经被删除了，就提取不到content了\n",
    "    if content:\n",
    "        title = content.find(class_='rich_media_title').text.strip()\n",
    "        print(title)\n",
    "        # \\/:*?\"<>|这些字符不能作为文件名或者文件夹名\n",
    "        filename = title.replace('\\/','-').replace('\\\\','-').replace('\\\"','-').replace('|','-').replace(\":\",\"_\").replace(\"?\",\"_\").replace(\"*\",\"_\").replace(\"<\",\"_\").replace(\">\",\"_\") + '.md'\n",
    "        try:\n",
    "            copyright = content.find(class_='wx_tap_link js_wx_tap_highlight rich_media_meta icon_appmsg_tag appmsg_title_tag weui-wa-hotarea').text\n",
    "        except:\n",
    "            copyright = None\n",
    "        try:\n",
    "            author = content.find(class_='wx_tap_link js_wx_tap_highlight rich_media_meta_link weui-wa-hotarea').text\n",
    "        except:\n",
    "            author = content.find(class_ = \"rich_media_meta rich_media_meta_text\").text.split(\"\\n\")[-2].split(\" \")[-1]\n",
    "        blog_name = content.find(class_='profile_nickname').text\n",
    "        blog_sign = content.div.div.div.find_all('p')[1].span.text\n",
    "        for i in soup.html.body.find_all('script'):\n",
    "            if 'publish_time' in str(i):\n",
    "                publish_time = int(i.text.split(\"document.getElementById\")[0].split(\"{e(\")[-1].split(\",\\\"\")[-1].split(\"\\\",\")[0])\n",
    "        content.find(id='meta_content').decompose()\n",
    "        download_images(soup, content)\n",
    "        brs = content.find_all('br')\n",
    "        if brs:\n",
    "            for br in brs:\n",
    "                br.decompose()\n",
    "        for_list(content)\n",
    "        for_code2(content)\n",
    "        mdText = str(content).replace('<br/>','').replace('</code><code>', \"\").replace('\\n</code>','</code>').replace('<code>\\n','<code>')\n",
    "        # 代码部分自动换行\n",
    "        mdText = tomd.Tomd(mdText).markdown\n",
    "        # 改格式转错的地方\n",
    "        mdText = mdText.replace('# \\n            \\n'+title, '## '+title).replace(\"# \\n\", \"\\n\")\n",
    "        mdText = re.sub(r'\\n[\\- ]+[ ]+\\n```', \"\\n \\n```\", mdText)\n",
    "        with open(filename, 'w', encoding='utf8') as file:\n",
    "            file.write(mdText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去掉<code> </code>  <em> </em> 将'-&gt;'  替换成‘->’ \n",
    "import os \n",
    "for item in os.listdir():\n",
    "    if item.split(\".\")[-1] == \"md\":\n",
    "        with open(item, 'r', encoding='utf8') as file:\n",
    "            print(\"-------------------------------------------------\")\n",
    "            str = ''\n",
    "            a = str.join(file.readlines())\n",
    "            if ('</em>' in a):\n",
    "                print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看是否有丢失图片，或者排序错误\n",
    "import os \n",
    "for item in os.listdir():\n",
    "    if item.split(\".\")[-1] == \"md\":\n",
    "        with open(item, 'r', encoding='utf8') as file:\n",
    "            print(\"-------------------------------------------------\")\n",
    "            print(item)\n",
    "            str = ''\n",
    "            a = str.join(file.readlines())\n",
    "            a = a.split(\"assets/images/\")\n",
    "            if len(a)>1:\n",
    "                a = a[1:]\n",
    "                i = 1\n",
    "                for b in a:\n",
    "                    b = b.split(\"\\n\")[0]\n",
    "                    b = b.split(\"/\")[1].split(\".\")[0]\n",
    "            \n",
    "                    if i !=int(b):\n",
    "                        print(\"No\")\n",
    "                    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#改成Jekyll格式\n",
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "import html2text as ht\n",
    "import yaml\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import tomd\n",
    "import re\n",
    "\n",
    "with open(\"../wechat/chaos-gravity.yaml\", \"r\") as file:\n",
    "    file_data = file.read()\n",
    "config = yaml.safe_load(file_data)\n",
    "def get_headers(config):\n",
    "    headers = {\n",
    "        \"Cookie\": config['cookie'],\n",
    "        \"User-Agent\": config['user-agent']\n",
    "    }\n",
    "    return headers\n",
    "\n",
    "pd.set_option('max_colwidth', 1000)\n",
    "article_list = pd.read_csv('../wechat/article_list2.csv', encoding='GB18030')\n",
    "headers = get_headers(config)\n",
    "\n",
    "for index, row in article_list.iterrows():\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    print(index)\n",
    "    create_time = time.strftime(\"%Y-%m-%d\",time.localtime(row[\"create_time\"]))\n",
    "    url = row['link'].replace(\"\\/\",\"/\")\n",
    "    article = requests.get(url,headers=headers)\n",
    "    if article.status_code == 200:\n",
    "        html = article.text\n",
    "    # for_code \n",
    "    html = html.replace(\"<br  />\",\"\\n\")\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    content = soup.find(id='img-content')\n",
    "    #有些文章已经被删除了，就提取不到content了\n",
    "    if content:\n",
    "        title = content.find(class_='rich_media_title').text.strip()\n",
    "        \n",
    "        # \\/:*?\"<>|这些字符不能作为文件名或者文件夹名\n",
    "        filename = title.replace('\\/','-').replace('\\\\','-').replace('\\\"','-').replace('|','-').replace(\":\",\"_\").replace(\"?\",\"_\").replace(\"*\",\"_\").replace(\"<\",\"_\").replace(\">\",\"_\") + '.md'\n",
    "        try:\n",
    "            copyright = content.find(class_='wx_tap_link js_wx_tap_highlight rich_media_meta icon_appmsg_tag appmsg_title_tag weui-wa-hotarea').text\n",
    "        except:\n",
    "            copyright = None\n",
    "        try:\n",
    "            author = content.find(class_='wx_tap_link js_wx_tap_highlight rich_media_meta_link weui-wa-hotarea').text\n",
    "        except:\n",
    "            author = content.find(class_ = \"rich_media_meta rich_media_meta_text\").text.split(\"\\n\")[-2].split(\" \")[-1]\n",
    "        blog_name = content.find(class_='profile_nickname').text\n",
    "        blog_sign = content.div.div.div.find_all('p')[1].span.text\n",
    "        for i in soup.html.body.find_all('script'):\n",
    "            if 'publish_time' in str(i):\n",
    "                publish_time = int(i.text.split(\"document.getElementById\")[0].split(\"{e(\")[-1].split(\",\\\"\")[-1].split(\"\\\",\")[0])\n",
    "        with open(\"../wechat2/\"+filename, 'r', encoding='utf8') as file:\n",
    "            mdText = ''\n",
    "            mdText = mdText.join(file.readlines())\n",
    "        \n",
    "        mdText = mdText.replace(\"assets/images/\", \"../assets/images/\")\n",
    "        head = \"---\\nlayout: post\\ntitle: \\\"\"\n",
    "        head = head + mdText.split(\"\\n\")[1][3:]+\"\\\"\\nauthor: Luna\"\n",
    "        head = head + \"\\ncategories: []\\ntags: []\\nimage: \"\n",
    "        \n",
    "        \n",
    "        if len(mdText.split(\"assets/images/\"))>1:\n",
    "            image = mdText.split(\"assets/images/\")[1]\n",
    "            if \"style\" in image:\n",
    "                image_link = \"assets/images/\" + image.split(\"\\\" style=\\\"\")[0]\n",
    "            else:\n",
    "                image_link = \"assets/images/\" + image.split(\")\\n\")[0]\n",
    "        else:\n",
    "            image_link = \"assets/images/\" + filename + \"/0.png\"\n",
    "        \n",
    "        head = head + image_link+\"\\n---\\n\"\n",
    "        mdText = mdText.replace(\"\\n\"+mdText.split(\"\\n\")[1]+\"\\n\", head)\n",
    "\n",
    "        with open(\"../wechat_to_github/_posts/\"+filename, 'w', encoding='utf8') as file:\n",
    "            file.write(mdText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#给文件名加上时间\n",
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "import html2text as ht\n",
    "import yaml\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import tomd\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "with open(\"../wechat/chaos-gravity.yaml\", \"r\") as file:\n",
    "    file_data = file.read()\n",
    "config = yaml.safe_load(file_data)\n",
    "def get_headers(config):\n",
    "    headers = {\n",
    "        \"Cookie\": config['cookie'],\n",
    "        \"User-Agent\": config['user-agent']\n",
    "    }\n",
    "    return headers\n",
    "\n",
    "pd.set_option('max_colwidth', 1000)\n",
    "article_list = pd.read_csv('../wechat/article_list2.csv', encoding='GB18030')\n",
    "headers = get_headers(config)\n",
    "\n",
    "for index, row in article_list.iterrows():\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "    print(index)\n",
    "    create_time = time.strftime(\"%Y-%m-%d\",time.localtime(row[\"create_time\"]))\n",
    "    url = row['link'].replace(\"\\/\",\"/\")\n",
    "    article = requests.get(url,headers=headers)\n",
    "    if article.status_code == 200:\n",
    "        html = article.text\n",
    "    # for_code \n",
    "    html = html.replace(\"<br  />\",\"\\n\")\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    content = soup.find(id='img-content')\n",
    "    #有些文章已经被删除了，就提取不到content了\n",
    "    if content:\n",
    "        title = content.find(class_='rich_media_title').text.strip()\n",
    "        print(title)\n",
    "        # \\/:*?\"<>|这些字符不能作为文件名或者文件夹名\n",
    "        filename = title.replace('\\/','-').replace('\\\\','-').replace('\\\"','-').replace('|','-').replace(\":\",\"_\").replace(\"?\",\"_\").replace(\"*\",\"_\").replace(\"<\",\"_\").replace(\">\",\"_\") + '.md'\n",
    "        try:\n",
    "            copyright = content.find(class_='wx_tap_link js_wx_tap_highlight rich_media_meta icon_appmsg_tag appmsg_title_tag weui-wa-hotarea').text\n",
    "        except:\n",
    "            copyright = None\n",
    "        try:\n",
    "            author = content.find(class_='wx_tap_link js_wx_tap_highlight rich_media_meta_link weui-wa-hotarea').text\n",
    "        except:\n",
    "            author = content.find(class_ = \"rich_media_meta rich_media_meta_text\").text.split(\"\\n\")[-2].split(\" \")[-1]\n",
    "        blog_name = content.find(class_='profile_nickname').text\n",
    "        blog_sign = content.div.div.div.find_all('p')[1].span.text\n",
    "        for i in soup.html.body.find_all('script'):\n",
    "            if 'publish_time' in str(i):\n",
    "                publish_time = int(i.text.split(\"document.getElementById\")[0].split(\"{e(\")[-1].split(\",\\\"\")[-1].split(\"\\\",\")[0])\n",
    "\n",
    "        newfilename = str(datetime.fromtimestamp(publish_time)).split(\" \")[0]+\"-\"+filename\n",
    "        try:\n",
    "            os.rename(\"_posts/\"+filename, \"_posts/\"+newfilename)\n",
    "        except:\n",
    "            print(newfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-18-Facebook自监督学习Visual Transformers(ViT)的训练经验(Moco v3) -- 训练代码解析.md\n",
      "-------------------------------------------------\n",
      "[]\n",
      "2020-06-28-换个思路实现人工智能_ 在视觉环境中用因果归纳完成目标导向的任务——生成数据.md\n",
      "-------------------------------------------------\n",
      "[]\n",
      "2020-03-29-换个思路实现人工智能_ 在视觉环境中用因果归纳完成目标导向的任务(上).md\n",
      "-------------------------------------------------\n",
      "[]\n",
      "2021-09-06-Stanford CS224N笔记：词向量和词义（Word Vectors and Word Senses）.md\n",
      "-------------------------------------------------\n",
      "['o\\\\|c)', 'c\\\\|o)', 'o\\\\|c)', 'c\\\\|o)', 'o\\\\|c)', 'c\\\\|o)']\n",
      "2020-11-23-关系归纳偏置，深度学习，和图网络.md\n",
      "-------------------------------------------------\n",
      "[]\n",
      "2021-11-23-Facebook DeiT(Data-efficient Image Transformers) 解析.md\n",
      "-------------------------------------------------\n",
      "[]\n",
      "2022-05-06-UC Berkeley非监督学习--Latent Variable Models -- VAE（潜变量模型--VAE）.md\n",
      "-------------------------------------------------\n",
      "['x\\\\|z)', 'x\\\\|z)', '2\\\\|x1', 'x\\\\|z)', 'x\\\\|z)', 'z\\\\|x)', 'x\\\\|z)', '(z|x)', '(z|x)']\n",
      "2021-03-17-MIT因果迷你课笔记 — 因果归纳和机器学习之半监督学习.md\n",
      "-------------------------------------------------\n",
      "['Y\\\\|X)', 'Y\\\\|X)', 'Y\\\\|X)', 'Y\\\\|X)', 'Y\\\\|X)', 'Y\\\\|X)']\n",
      "2021-02-07-MIT因果迷你课笔记 —— 因果归纳模型的评估方式（SHD和SID）.md\n",
      "-------------------------------------------------\n",
      "['*\\\\|**']\n",
      "2020-07-09-Softmax.md\n",
      "-------------------------------------------------\n",
      "[]\n",
      "2022-10-11-如何抓取一个微信公众号的所有文章（Python）下篇.md\n",
      "-------------------------------------------------\n",
      "['<>|这些', \"('|',\"]\n",
      "2021-02-23-用于细分类的API-Net（Attentive Pairwise Interaction Network）.md\n",
      "-------------------------------------------------\n",
      "[]\n",
      "2020-08-03-Cross Entropy Loss.md\n",
      "-------------------------------------------------\n",
      "[]\n",
      "2021-08-29-论文写作之Reference整理规范.md\n",
      "-------------------------------------------------\n",
      "[]\n",
      "2020-10-20-Kaggle比赛系列：[Two Sigma] Using News to Predict Stock Movements.md\n",
      "-------------------------------------------------\n",
      "[]\n",
      "2020-11-10-Kaggle比赛系列：Conway's Reverse Game of Life 2020.md\n",
      "-------------------------------------------------\n",
      "[]\n",
      "2020-09-05-MIT因果迷你课笔记 —— 发现因果关系2(restricted structural causal model).md\n",
      "-------------------------------------------------\n",
      "['*\\\\|**']\n",
      "2021-11-07-Facebook自监督学习Visual Transformers(ViT)的训练经验(Moco v3) -- Moco中的ViT.md\n",
      "-------------------------------------------------\n",
      "[]\n",
      "2021-03-18-MIT因果迷你课笔记 — 因果归纳和机器学习之half-sibling regression.md\n",
      "-------------------------------------------------\n",
      "[]\n",
      "2021-03-19-MIT因果迷你课笔记 — 因果归纳和机器学习之强化学习.md\n",
      "-------------------------------------------------\n",
      "['r\\\\|t,', 't\\\\|s)', 'y\\\\|x)', 'y\\\\|x)', 'y\\\\|x)', 'y\\\\|x)']\n",
      "2021-05-15-UC Berkeley非监督学习--自监督学习（Self Supervised Learning）.md\n",
      "-------------------------------------------------\n",
      "[]\n",
      "2021-09-01-自监督学习Visual Transformers(ViT)的训练经验(Moco v3) -- 论文解析.md\n",
      "-------------------------------------------------\n",
      "[]\n",
      "2020-02-22-HelloWorld.md\n",
      "-------------------------------------------------\n",
      "[]\n",
      "2020-08-19-换个思路实现人工智能_ 在视觉环境中用因果归纳完成目标导向的任务——策略学习模型.md\n",
      "-------------------------------------------------\n",
      "[]\n",
      "2020-03-05-马可夫决策过程( Markov Decision Processes)_不确定世界的决策方法.md\n",
      "-------------------------------------------------\n",
      "[' \\\\| S']\n",
      "2020-06-05-MIT因果迷你课笔记 —— 相关和因果.md\n",
      "-------------------------------------------------\n",
      "[]\n",
      "2021-03-20-MIT因果迷你课笔记 — 因果归纳和机器学习之Domain Adaptation.md\n",
      "-------------------------------------------------\n",
      "[]\n",
      "2022-02-08-以图片为目标的视觉强化学习 -- 代码解析上篇(rlkit安装及部分代码及函数的解析和使用).md\n",
      "-------------------------------------------------\n",
      "[]\n",
      "2021-10-31-ViT (Vision Transformer)原理及代码解析.md\n",
      "-------------------------------------------------\n",
      "[]\n",
      "2020-05-05-D-separation，d-connection.md\n",
      "-------------------------------------------------\n",
      "[]\n",
      "2021-03-14-MIT因果迷你课笔记 —— 基于不变性的因果预测（invariant causal prediction）.md\n",
      "-------------------------------------------------\n",
      "[';\\\\|t\\\\', 'Y\\\\|PA', 'Y\\\\|PA', 'Y\\\\|PA', 'Y\\\\|S)', 'Y\\\\|S)']\n",
      "2022-03-09-UC Berkeley非监督学习--自回归模型.md\n",
      "-------------------------------------------------\n",
      "['B\\\\|A)', 'A\\\\|B)', 'B\\\\|A)', '3\\\\|x2', '3\\\\|x2', '1\\\\|x2', '1\\\\|x&', '1\\\\|x（', '1\\\\|x&']\n",
      "2022-06-28-钢琴曲转谱（Solo Piano Transcription）.md\n",
      "-------------------------------------------------\n",
      "[]\n",
      "2022-07-28-如何抓取一个微信公众号的所有文章（Python）上篇.md\n",
      "-------------------------------------------------\n",
      "[]\n",
      "2020-07-14-MIT因果迷你课笔记 —— 发现因果关系1.md\n",
      "-------------------------------------------------\n",
      "[' \\\\|**', ' \\\\|**', '*\\\\|**', '*\\\\|**', '*\\\\|**', '*\\\\|**', '*\\\\|**', ' \\\\| p']\n",
      "2021-03-11-以图片为目标的视觉强化学习.md\n",
      "-------------------------------------------------\n",
      "[]\n",
      "2022-05-10-UC Berkeley非监督学习--Implicit Models -- GANs (上).md\n",
      "-------------------------------------------------\n",
      "['y\\\\|x)']\n",
      "2021-08-02-美股资料收集(Python).md\n",
      "-------------------------------------------------\n",
      "['im|ut', 'l)|ut', 'al|ut']\n",
      "2021-10-11-GiantMIDI-Piano：字节跳动发布的大型古典钢琴乐MIDI数据集.md\n",
      "-------------------------------------------------\n",
      "[]\n",
      "2022-03-21-UC Berkeley非监督学习--流模型（Flow Models）.md\n",
      "-------------------------------------------------\n",
      "['x\\\\|z)']\n",
      "2022-03-01-UC Berkeley非监督学习--介绍.md\n",
      "-------------------------------------------------\n",
      "[]\n",
      "2020-06-21-MIT因果迷你课笔记 —— 因果语言和因果推理.md\n",
      "-------------------------------------------------\n",
      "['*\\\\|**', '*\\\\|**', ' \\\\| *', ' \\\\| *', '*\\\\|**', '*\\\\|**', '1\\\\|**']\n",
      "2020-11-09-ArcFace，CosFace，SphereFace，三种人脸识别算法的损失函数(Loss Function)的设计.md\n",
      "-------------------------------------------------\n",
      "['令\\\\|\\\\|', '*\\\\|\\\\|']\n",
      "2020-07-03-换个思路实现人工智能_ 在视觉环境中用因果归纳完成目标导向的任务——因果归纳模型.md\n",
      "-------------------------------------------------\n",
      "[]\n",
      "2021-02-04-MIT因果迷你课笔记 —— 发现因果关系3(多变量).md\n",
      "-------------------------------------------------\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#改成Jekyll格式\n",
    "import os \n",
    "import re\n",
    "for item in os.listdir(\"../_posts/\"):\n",
    "    if item.split(\".\")[-1] == \"md\":\n",
    "        print(item)\n",
    "        with open(\"../_posts/\"+ item, 'r', encoding='utf8') as file:\n",
    "            print(\"-------------------------------------------------\")\n",
    "            mdText = ''\n",
    "            mdText = mdText.join(file.readlines())\n",
    "            pattern = re.compile(r'..\\|..')\n",
    "            pattern2= re.compile(r'\\*\\*.\\*\\*')\n",
    "            a = re.findall(pattern, mdText)\n",
    "            b = re.findall(pattern2, mdText)\n",
    "            print(a)\n",
    "            #print(b)\n",
    "            #mdText.replace(\"|\",\"\\|\")\n",
    "            #with open(\"../_posts/\"+ item, 'w', encoding='utf8') as file:\n",
    "            #    file.write(mdText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
