
var documents = [{
    "id": 0,
    "url": "http://localhost:4000/404.html",
    "title": "404",
    "body": "404 Page not found!Please use the search bar from the bottom left or visit our homepage! "
    }, {
    "id": 1,
    "url": "http://localhost:4000/about",
    "title": "A website focus in AI",
    "body": "If you are interested in our team, and you want to get information about us at the first time. You can scan the picture below by WeChat. [It’s our WeChat Public Account named chaos-gravity. ] Have Fun! "
    }, {
    "id": 2,
    "url": "http://localhost:4000/authors",
    "title": "A website focus in AI",
    "body": "If you are interested in our team, and you want to get information about us at the first time. You can scan the picture below by WeChat. [It’s our WeChat Public Account named chaos-gravity. ] Have Fun! "
    }, {
    "id": 3,
    "url": "http://localhost:4000/categories",
    "title": "Categories",
    "body": ""
    }, {
    "id": 4,
    "url": "http://localhost:4000/contact",
    "title": "Contact",
    "body": "  Please send your message to Chaos万有引力. We will reply as soon as possible!   "
    }, {
    "id": 5,
    "url": "http://localhost:4000/",
    "title": "Home",
    "body": "                                                                                               如何抓取一个微信公众号的所有文章（Python）下篇              :       声明：基本步骤和核心方法均参考[1]，[2]，细节有大不同，这篇着重于如何将抓取到的文章用markdown的格式保存下来。1. 上篇中已经抓到了公众号的所有文章的题目和链接，下篇直接读入：im:                                                                               Luna                 11 Oct 2022                                                                                                                           如何抓取一个微信公众号的所有文章（Python）上篇              :       声明：基本步骤和核心方法均参考[1]，未做诸多更改，但是细节上可能因为微信自己做了更新，爬取细节很不同，另外就是加入了一些文本处理的操作。1. 注册微信公众号 -&gt; 新建图文消息 -&gt;:                                                                               Luna                 28 Jul 2022                                                                                                                           钢琴曲转谱（Solo Piano Transcription）              :       ​  今天来看一篇钢琴琴谱翻译的文章，出自ByteDance字节跳动，Giant-Piano（GiantMIDI-Piano：字节跳动发布的大型古典钢琴乐MIDI数据集）采用的转谱模型就是这个：:                                                                               Luna                 28 Jun 2022                                                                                                                           UC Berkeley非监督学习--Implicit Models -- GANs (上)              :           翻译整理一下UC Berkeley非监督学习的课程。这篇翻译第五六讲Implicit Models – GANs。分三篇：上，中，下。    这个课程总共十二讲，官方链接：http:                                                                               Luna                 10 May 2022                                                                                                                           UC Berkeley非监督学习--Latent Variable Models -- VAE（潜变量模型--VAE）              :           翻译整理一下UC Berkeley非监督学习的课程。这篇翻译第四讲Latent Variable Models – VAE。    这个课程总共十二讲，官方链接：https://s:                                                                               Luna                 06 May 2022                                                                                                                           UC Berkeley非监督学习--流模型（Flow Models）              :           翻译整理一下UC Berkeley非监督学习的课程。这篇翻译第三讲Flow Models，流模型。    这个课程总共十二讲，官方链接：https://sites. google. c:                                                                               Luna                 21 Mar 2022                                   &laquo;        1        2        3        4        5        6        7        8       &raquo; "
    }, {
    "id": 6,
    "url": "http://localhost:4000/wechat",
    "title": "A website focus in AI",
    "body": "If you are interested in our team, and you want to get information about us at the first time. You can scan the picture below by WeChat. [It’s our WeChat Public Account named chaos-gravity. ] Have Fun! "
    }, {
    "id": 7,
    "url": "http://localhost:4000/src/tips/",
    "title": "How to build",
    "body": "How to build: bundle exec jekyll buildbuild and open server: bundle exec jekyll serve"
    }, {
    "id": 8,
    "url": "http://localhost:4000/page2/",
    "title": "Home",
    "body": "  {% for post in paginator. posts %}    {% include postbox. html %}  {% endfor %}  {% include pagination. html %}"
    }, {
    "id": 9,
    "url": "http://localhost:4000/page3/",
    "title": "Home",
    "body": "  {% for post in paginator. posts %}    {% include postbox. html %}  {% endfor %}  {% include pagination. html %}"
    }, {
    "id": 10,
    "url": "http://localhost:4000/page4/",
    "title": "Home",
    "body": "  {% for post in paginator. posts %}    {% include postbox. html %}  {% endfor %}  {% include pagination. html %}"
    }, {
    "id": 11,
    "url": "http://localhost:4000/page5/",
    "title": "Home",
    "body": "  {% for post in paginator. posts %}    {% include postbox. html %}  {% endfor %}  {% include pagination. html %}"
    }, {
    "id": 12,
    "url": "http://localhost:4000/page6/",
    "title": "Home",
    "body": "  {% for post in paginator. posts %}    {% include postbox. html %}  {% endfor %}  {% include pagination. html %}"
    }, {
    "id": 13,
    "url": "http://localhost:4000/page7/",
    "title": "Home",
    "body": "  {% for post in paginator. posts %}    {% include postbox. html %}  {% endfor %}  {% include pagination. html %}"
    }, {
    "id": 14,
    "url": "http://localhost:4000/page8/",
    "title": "Home",
    "body": "  {% for post in paginator. posts %}    {% include postbox. html %}  {% endfor %}  {% include pagination. html %}"
    }, {
    "id": 15,
    "url": "http://localhost:4000/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ “sitemap. xml”   absolute_url }}   "
    }, {
    "id": 16,
    "url": "http://localhost:4000/%E5%A6%82%E4%BD%95%E6%8A%93%E5%8F%96%E4%B8%80%E4%B8%AA%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7%E7%9A%84%E6%89%80%E6%9C%89%E6%96%87%E7%AB%A0-Python-%E4%B8%8B%E7%AF%87/",
    "title": "如何抓取一个微信公众号的所有文章（Python）下篇",
    "body": "2022/10/11 - 声明：基本步骤和核心方法均参考[1]，[2]，细节有大不同，这篇着重于如何将抓取到的文章用markdown的格式保存下来。 1. 上篇中已经抓到了公众号的所有文章的题目和链接，下篇直接读入：: import pandas as pdfrom newspaper import Articleimport html2text as htimport yamlimport timeimport requestsfrom bs4 import BeautifulSoupimport osimport tomdimport rewith open( chaos-gravity. yaml ,  r ) as file:  file_data = file. read()config = yaml. safe_load(file_data)def get_headers(config):  headers = {     Cookie : config['cookie'],     User-Agent : config['user-agent']  }  return headerspd. set_option('max_colwidth', 1000)article_list = pd. read_csv('article_list2. csv', encoding='GB18030')headers = get_headers(config)  def download_images(soup, content):  dir_name =  assets/images/  + str(filename[:-3]) +  /   if not os. path. isdir(dir_name):    os. mkdir(dir_name)  cnt = 1  images = content. find_all( img )  for image in images:    img_src = image. get('data-src')    img_type = image. get('data-type')    img_name =  {0:d}. {1:s} . format(cnt, img_type if img_type else 'png')    cnt += 1    file_path =  assets/images/{0:s}/{1:s} . format(filename[:-3], img_name)    if not os. path. exists(file_path):      with open (file_path, 'wb') as file:        response = requests. get(url = img_src)        for block in response. iter_content(1024):          if block:            file. write(block)          else:            break    tag = soup. new_tag('span')    tag. string =  ![](assets/images/{0:s}/{1:s}) . format(filename[:-3], img_name)    image. replace_with(tag)    def download_videos(soup, content):  dir_name =  assets/videos/  + str(title) +  /   if not os. path. isdir(dir_name):    os. mkdir(dir_name)  if videos:    for video in videos:      video_src = video. get('data-src')      tag = soup. new_tag('span')      tag. string =  [此处是视频]({0:s}) . format(video_src)      video. replace_with(tag)def for_list(content):  for a in content. find_all( ol ):    i = 1    for b in a. find_all( li ):      tag = soup. new_tag('span')      tag. string =  {0:d}. {1:s}\n . format(i, b. text)      b. replace_with(tag)      i = i+1def for_code2(content):  for a in content. find_all(class_= code-snippet__fix code-snippet__js ):    for b in a. find_all( code ):      tag = soup. new_tag('code')      tag. string =  \n  + b. text      b. replace_with(tag)      def for_code(content):  for a in content. find_all(class_= code-snippet__fix code-snippet__js ):    tag = soup. new_tag('code')    string =       length = len(a. find_all( code ))    i = 1    for b in a. find_all( code ):      string = string +  \n  + b. text      if i&lt;length:        b. extract()      i = i+1    tag. string = string    b. replace_with(tag)for index, row in article_list. iterrows():  print(index)  print(row)  create_time = time. strftime( %Y-%m-%d ,time. localtime(row[ create_time ]))  url = row['link']. replace( \/ , / )  article = requests. get(url,headers=headers)  if article. status_code == 200:    html = article. text  # for_code   html = html. replace( &lt;br /&gt; , \n )  soup = BeautifulSoup(html, 'html. parser')  content = soup. find(id='img-content')  #有些文章已经被删除了，就提取不到content了  if content:    title = content. find(class_='rich_media_title'). text. strip()    print(title)    # \/:*? &lt;&gt;|这些字符不能作为文件名或者文件夹名    filename = title. replace('\/','-'). replace('\\','-'). replace('\ ','-'). replace('|','-'). replace( : , _ ). replace( ? , _ ). replace( * , _ ). replace( &lt; , _ ). replace( &gt; , _ ) + '. md'    try:      copyright = content. find(class_='wx_tap_link js_wx_tap_highlight rich_media_meta icon_appmsg_tag appmsg_title_tag weui-wa-hotarea'). text    except:      copyright = None    try:      author = content. find(class_='wx_tap_link js_wx_tap_highlight rich_media_meta_link weui-wa-hotarea'). text    except:      author = content. find(class_ =  rich_media_meta rich_media_meta_text ). text. split( \n )[-2]. split(   )[-1]    blog_name = content. find(class_='profile_nickname'). text    blog_sign = content. div. div. div. find_all('p')[1]. span. text    for i in soup. html. body. find_all('script'):      if 'publish_time' in str(i):        publish_time = int(i. text. split( document. getElementById )[0]. split( {e( )[-1]. split( ,\  )[-1]. split( \ , )[0])    content. find(id='meta_content'). decompose()    download_images(soup, content)    brs = content. find_all('br')    if brs:      for br in brs:        br. decompose()    for_list(content)    for_code2(content)    mdText = str(content). replace('&lt;br/&gt;',''). replace('&lt;/code&gt;&lt;code&gt;',   ). replace('\n&lt;/code&gt;','&lt;/code&gt;'). replace('&lt;code&gt;\n','&lt;code&gt;')    # 代码部分自动换行    mdText = tomd. Tomd(mdText). markdown    # 改格式转错的地方    mdText = mdText. replace('# \n      \n'+title, '## '+title). replace( # \n ,  \n )    mdText = re. sub(r'\n[\- ]+[ ]+\n```',  \n \n``` , mdText)    with open(filename, 'w', encoding='utf8') as file:      file. write(mdText)代码有点长，调起来费了九牛二虎之力，这里试的是微信公众号的文章转markdown，可能和其他网页不是那么匹配，主要还是要去熟悉soup的使用方法，并坚持将匹配错误的地方各个击破。 没有实现的功能：  视频没办法处理 不能百分百准确，文字和图谱还是偶有缺失，缺失率小于1% 有些文字格式会跑掉 列表不能准确转换，太懒了，懒得写了 基础版，特殊格式字符什么的，无法正确转换，需要自己开发 转换完扔需要校验，无法自动调校图片大小，需要手动上面代码主要实现的功能：    代码模块可以成功换行了     图片可以自动下载保存到本地并赋予链接     一些匹配错误的地方做了校正  "
    }, {
    "id": 17,
    "url": "http://localhost:4000/%E5%A6%82%E4%BD%95%E6%8A%93%E5%8F%96%E4%B8%80%E4%B8%AA%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7%E7%9A%84%E6%89%80%E6%9C%89%E6%96%87%E7%AB%A0-Python-%E4%B8%8A%E7%AF%87/",
    "title": "如何抓取一个微信公众号的所有文章（Python）上篇",
    "body": "2022/07/28 - 声明：基本步骤和核心方法均参考[1]，未做诸多更改，但是细节上可能因为微信自己做了更新，爬取细节很不同，另外就是加入了一些文本处理的操作。 1. 注册微信公众号 -&gt; 新建图文消息 -&gt; 超链接  -&gt; 右键下拉菜单点击“检查” -&gt; 检查页面中最上面选“Network” -&gt; 在左边的搜索公众号文章搜索自己想要的公众号，并选中 -&gt; 观察右边的检查页面，会发现下方列表中新增了一个以“appmsg”开头的项目，点击这个“appmsg”开头的项目，然后在检查页面的右方选择“Headers”。”Headers” 下面有个”General”，”General”里面有个”Request URL“ ：: https://mp. weixin. qq. com/cgi-bin/appmsgpublish?sub=list&amp;amp;search_field=null&amp;amp;begin=0&amp;amp;count=5&amp;amp;query=&amp;amp;type=101_1&amp;amp;free_publish_type=1&amp;amp;sub_action=list_ex&amp;amp;token=221080036&amp;amp;lang=zh_CN&amp;amp;f=json&amp;amp;ajax=12. 解析”Request URL“:: 该链接分三部分：  https://mp. weixin. qq. com/cgi-bin/appmsg 请求的基础部分 ?action=list 常用于动态网站，实现不同的参数值而生成不同的页面或者返回不同的结果 &amp;search_field=null&amp;begin=0&amp;count=5&amp;query=&amp;type=101_1&amp;free_publish_type=1&amp;sub_action=list_ex&amp;token=221080036&amp;lang=zh_CN&amp;f=json&amp;ajax=1 设置各种参数3. 获取Cookie和User-Agent: 直接用Python的Requests库访问该url，并不能正常获得结果。原因在于利用网页版微信公众号后台插入超链接时，是登录状态，而用python直接访问时是未登录状态。因此，需要手动获取访问时的Cookie和User-Agent,在用Python的Requests库进行访问时将其传入headers参数。这里将公众号标识符fakeid以及token参数保存在了一个yaml文件中，方便爬取时加载。这些都可以在右边的检查页面中搜到： fakeid: Mzg5. . . token: 22. . . user-agent: Mozilla/5. 0 . . . cookie: ua_id=9gk3jF5RLFPEru. . . 用yaml包读取参数 import yamlwith open( chaos-gravity. yaml ,  r ) as file:    file_data = file. read()config = yaml. safe_load(file_data) headers = {     Cookie : config['cookie'],     User-Agent : config['user-agent'] }4. 设置请求参数，生成url: 对照自己找到的Request URL改下面的参数： # 请求参数url =  https://mp. weixin. qq. com/cgi-bin/appmsgpublish begin =  0 params = {     sub :  list ,     search_field : null ,     sub_action :  list_ex ,      begin : begin,     free_publish_type :  1 ,     count :  5 ,     fakeid : config['fakeid'],     type :  101_1 ,     token : config['token'],     lang :  zh_CN ,     f :  json ,     ajax :  1 }5. 抓取文章题目，作者，链接及其他有用信息并保存成csv文件: 这部分需要一行一行代码校验，防止微信公众号改了规则，下面代码和参考[1]就很不同了，而不同的主要原因来自于细节上的更改。 i = 0column_name =  aid,appmsgid,author_name,title,cover_img,digest,link,create_time article_list_path =  article_list. csv with open(article_list_path,  a ) as f:    f. write(column_name + '\n')while True:    begin = i * 5    params[ begin ] = str(begin)    # 随机暂停几秒，避免过快的请求导致过快的被查到    time. sleep(random. randint(1,10))    resp = requests. get(url, headers=headers, params = params, verify=False)    # 微信流量控制, 退出    if resp. json()['base_resp']['ret'] == 200013:        print( frequencey control, stop at {} . format(str(begin)))        time. sleep(3600)        continue            if i ==  0 :        total_count = eval(resp. json()['publish_page'])['total_count']        print( We have  +str(tatal_count) +   articles.  )        publish_list = eval(resp. json()['publish_page'])['publish_list']    # 如果返回的内容中为空则结束    if len(publish_list) == 0:        print( all ariticle parsed )        break        for publish in publish_list:        publish = eval(publish['publish_info']. replace( true , True ). replace( false , False ))['appmsgex'][0]        info = ' {} , {} , {} , {} , {} , {} , {} , {} '. format(str(publish[ aid ]), \                str(publish['appmsgid']), str(publish['author_name']), \                str(publish['title']. replace( \n ,  ). replace( , , ; )), \                str(publish['cover']), str(publish['digest']. replace( \n ,  ). replace( , , ; )), \                str(publish['link']), str(publish['create_time']))        with open(article_list_path,  a ) as f:            f. write(info+'\n')        print( \n . join(info. split( , )))        print( \n\n---------------------------------------------------------------------------------\n )    # 翻页    i += 16. 完整代码get_wechart_article_list. py：: 注意：因为内容是保存成csv档，所以如果标题和内容简介中有逗好，都替换成了分号。 import yamlimport timeimport randomimport requestsdef get_headers(config):    headers = {         Cookie : config['cookie'],         User-Agent : config['user-agent']    }    return headersdef get_params(config):    url =  https://mp. weixin. qq. com/cgi-bin/appmsgpublish     begin =  0     params = {         sub :  list ,         search_field : null ,         sub_action :  list_ex ,          begin : begin,         free_publish_type :  1 ,         count :  5 ,         fakeid : config['fakeid'],         type :  101_1 ,         token : config['token'],         lang :  zh_CN ,         f :  json ,         ajax :  1     }    return paramsdef get_article_list(headers, params):    i = 0    column_name =  aid,appmsgid,author_name,title,cover_img,digest,link,create_time     article_list_path =  article_list. csv     with open(article_list_path,  a ) as f:        f. write(column_name + '\n')    while True:        begin = i * 5        params[ begin ] = str(begin)        # 随机暂停几秒，避免过快的请求导致过快的被查到        time. sleep(random. randint(1,10))        resp = requests. get(url, headers=headers, params = params, verify=False)        # 微信流量控制, 退出        if resp. json()['base_resp']['ret'] == 200013:            print( frequencey control, stop at {} . format(str(begin)))            time. sleep(3600)            continue                if i ==  0 :            total_count = eval(resp. json()['publish_page'])['total_count']            print( We have  +str(tatal_count) +   articles.  )            publish_list = eval(resp. json()['publish_page'])['publish_list']        # 如果返回的内容中为空则结束        if len(publish_list) == 0:            print( all ariticle parsed )            break            for publish in publish_list:            publish = eval(publish['publish_info']. replace( true , True ). replace( false , False ))['appmsgex'][0]            info = ' {} , {} , {} , {} , {} , {} , {} , {} '. format(str(publish[ aid ]), \                str(publish['appmsgid']), str(publish['author_name']), \                str(publish['title']. replace( \n ,  ). replace( , , ; )), \                str(publish['cover']), str(publish['digest']. replace( \n ,  ). replace( , , ; )), \                str(publish['link']), str(publish['create_time']))            with open(article_list_path,  a ) as f:                f. write(info+'\n')            print( \n . join(info. split( , )))            print( \n\n---------------------------------------------------------------------------------\n )        # 翻页        i += 1def main():    with open( chaos-gravity. yaml ,  r ) as file:        file_data = file. read()    config = yaml. safe_load(file_data)     headers = get_headers(config)    params = get_params(config)    get_article_list(headers, params)if __name__ == '__main__':    main()这篇抓链接，下篇抓文章。 参考链接：  https://zhuanlan. zhihu. com/p/379062852"
    }, {
    "id": 18,
    "url": "http://localhost:4000/%E9%92%A2%E7%90%B4%E6%9B%B2%E8%BD%AC%E8%B0%B1-Solo-Piano-Transcription/",
    "title": "钢琴曲转谱（Solo Piano Transcription）",
    "body": "2022/06/28 - ​  今天来看一篇钢琴琴谱翻译的文章，出自ByteDance字节跳动，Giant-Piano（GiantMIDI-Piano：字节跳动发布的大型古典钢琴乐MIDI数据集）采用的转谱模型就是这个： ​  两个相关的源代码地址： 琴转谱： https://github. com/bytedance/piano_transcription数据集相关： https://github. com/bytedance/GiantMIDI-Piano​  曲转谱（Automatic Music Transcription（AMT））就是把钢琴曲（音频）转换成电子乐谱（Musical Instrument Digital Interface（MIDI））。这个课题的相关工作是比较少的，毕竟乍一看乍一想没啥商业价值。咱读这篇，就当好玩吧，毕竟，谁不热爱音乐呢？这次除了会解读论文，还会解析代码。 ​  AMT是音乐信息提取（Music Information Retrieval（MIR））中关联基于音频的音乐理解和基于符号的音乐理解的桥梁。AMT有几个关键的应用，包括乐谱跟踪（Score Following），音频与乐谱对位（audio to score alignment），以及根据曲谱分离出曲子（score-informed source separation ）。而工业界，AMT可以用来做音乐类的教育软件，可以为创作者将弹的曲子翻译成谱子，可以将音频信息转换成符号信息等等。 ​  这篇只做钢琴曲的转谱。包括识别音调（pitch），按下琴键（onset），松开琴键（offset），速度（velocity）等信息。钢琴曲转谱并不简单，之前也有各种各样的尝试，这里就不仔细展开说了，感兴趣的看论文原文。我们直接说一说之前的工作都有哪些不足（以前的工作的不足，就是这个工作的动机）：  之前的曲转谱呢，是把曲子切割成一个一个小段（frames），然后再来标记每个frame的特征，比如是onset还是offset。这样的话翻译的精度就被frame的长度限制住了。 之前的工作多用一个frame来标记onset或者offset，但是其实类似onset的行为造成的影响，可能会持续不止一个frames，另外如果只用一个frame来标记onset或者offset，模型训练会对曲谱错位非常敏感。接下来就是看针对这两个不足，新方法都做了哪些改进。 先简略介绍些概念： 1. 梅尔频谱（以下内容来自参考[1]）: ​  音频信号是时域信号。人对声音的分辨力根据频率的不同而不同，较容易分辨低频信号，比如人很容易可以分辨500Hz和1000Hz的信号，但是却不那么容易分辨10000Hz和10500Hz，尽管两组信号的频域距离是一样的。梅尔频谱是说对频率做一个映射，相对拉大低频信号的距离，缩小高频信号的距离，使得人对他们的分辨能力和他们的距离能匹配上。 ​  具体公式和具体工程实现请参考[2]： 2. Frame-wise 翻译系统: ​  在进行曲转谱的最开始，会把曲子先切割成frames，然后再将每个frame转成频域信号，再转换成梅尔频谱，因此最后输入是： T是frames的数量，F是梅尔频带的数量。通常来说，这个任务的label，target是下面这样的： T依然还是frames的数量，而K则是琴键的数量，比如88。而网络的输出可以表示为： 那么损失函数则可以用如下方式计算： 其中： 在frame-wise翻译系统里，要判断的是每一帧里，每个琴键的状态，是有发声还是没有发声。然后说一说这种方法的缺点：  这种翻译方式，如果要转成普通琴谱的样子还需要有精确的后期工作 另外这种方式不能准确的预测onsets，而onsets又恰恰包含了大量信息3. Onsets and Frames 翻译系统: ​	为了解决上面的问题，那么自然就提出了预测onsets和offsets的双目标系统。也有人onsets和frames双目标系统，也就是说既输出onsets也输出上面frame-wise输出的结果。所以损失函数是： 其中： 这个方法也依然又缺陷：  精度受frame长短限制 而onsets和frames仅用二进制0，1来表达状态，其实也不够精确方法: ​	这篇文章的方法自然要解决之前方法的缺陷，首先看下面这张图： ​	红线表示真正onset的时间，我们上节提到的onsets &amp; frames方法仅仅标记了onset发生的那个frame。但这样是不准确的，因为这样无法表达一次onset的行为会对每个frame造成什么样的影响。ADSR则会识别onsets和offsets，onset造成影响的frames都会被标注成1。但ADSR这种方法会使音频波形变得模糊，会使offset的识别变得相对困难，而第一种方法会对onset和offset没有严格对其这种错误非常敏感。而为了解决上面两个问题，Attack&amp;decay则用连续值来标记onset。那最后一个缺点就是，以上方法都没有标记onset的具体时间。用以上的方法，onset可以是一个frame种的任何时间，而如果onset如果恰好在两个frame的边界线上，那么target怎么标记都好像不太对，而如果为了解决这个问题去缩小frame的尺寸，一方面会增加计算量，另外一个方面是之前的问题依然存在。接下来就讲这篇文章提出的方法： Regress Onset and Offset Times: ​	这篇文章的译谱方法会输出具体的onset和offset的时间，这个想法受YOLO（You Only Look Once）启发，在YOLO里，图片会被切割为很多个grids，需要预测grid和object的距离。而这个任务的目标则是输出每个frame离自己最近的onset和offset的距离，如上图种最后一行所示。这边会对目标做一个变换： i是一个相对位置，J是控制锋度的变量，变换后，target大概长下面这样： regression的意思也就在这里了。损失函数： G是targets，R是预测结果。 Velocity Estimation: ​	声音的三个属性指音量，音调和音色，音色不用判断，因为这个任务专识别钢琴，音调不同notes音调不同，所以还剩下音量，但是velocity这个单词我反复查看都是速度的意思，而文章的意思又明显指音量，即声音的大小。所以我猜可能是作者用错了词？或者这里面有我什么没看懂的关窍。总之先按音量来理解： vel下标指的是velocity，on下标指的是onset，I指targets，P指ground truth。因为onsets的过程包含了更多关于音量的信息，因此音量的判断只用onsets的部分来判断，就不用offsets的部分来判断了。 Entire System: ​	整个系统的网络框架如下图所示： 首先音频信号会切割成frames，每个frame再分别做梅尔频谱变化，输入则是做过频谱变换的数据，模型分四个部分，最左边做velocity regression，结果会接着用来加入做onset regression，原因是声音的高低有助于onset的判断，如果声音低，那么系统会消耗更多的注意力去侦测onset，这和人会耗费更多注意力在音量低的部分是一样的（这里就有点不懂这个逻辑了，比如怎么就能提高对系统对音量低的部分的注意力呢？），另外加入做onset regression的还有一个frequency dimension。onset regression和offset regression的结果会加入做frame-wise classification。更细节的模型框架见下图： 总的损失函数： 推理: ​	推理的时候首先把曲子切割成frames，转换成梅尔频谱，再输入模型，得到velocity，onset，offset，frame-wised的结果，再转换成正规的曲谱。 ​	上图展示了如果计算精确的onset和offset时间的算法，A，B，C是三个相邻的frames的中心时间，B是那个被预测最有可能是onset所在的frame的中心时间。首先对所有frame单个分析，如果一个frame它的预测为onset的值超过了一定数，则去判断周边的两个都比他低，如果是的，那么则去找一个G点，使得AG与CG是以GI为轴对称的。那么上图中BH就是要求的答案，如果A所在的frame是onset的概率小于C所在的frame，则： 否则： 文中给了推导过程，这里就不细说了，感兴趣的可以验证一下。推理过程的算法如下： 首先，onset和offset是成对出现的，算法中的10和11即计算BH的两个公式，P是frame-wise的输出。这个方法无法识别长度小于4个frames的onsets，但现实中这种情况也几乎不会发生。此外音量的值由[0,1]区间转至[0,128]区间。 Sustain Pedal Transcription: ​	踏板对演奏的影响是很大的，按下时，延音踏板通过将所有制音器从琴弦上移开来维持钢琴上的所有阻尼琴弦，并允许琴弦自由振动。 正在播放的所有音符将继续发声，直到松开踏板。之前的钢琴译谱系统，有些没有做踏板状态的识别，有些做了呢，又没做曲调的识别。因此这篇文章还做了延音踏板译谱的工作，这个和曲调的模型是分开的，分开可以获得更好的效果，最后踏板的值在[0,128]之间。如果小于64，那么表示踏板没有开启，如果大于64，则表示踏板开启了。这样做其实是粗糙的，因为踏板中还有一些特殊技术，比如half pedals，这里是不识别的。踏板可以被当作一个琴键，损失函数和之前的其他琴键是一样的，也一样会有onset，offset和frame-wise结果： 总的损失函数： 精确时间的计算算法如下： 和琴键的onset，offset判断模式是不一样的，如果pedal onset的概率大于一定值，且比上一帧的概率大，那么就会被判断成是进行了onset。offset则是如果offset的概率大于一定值，或者frame-wise的值小于一定值，则判断为offset的帧，offset和onset是成对的。精确的时间的计算方法和琴键的onset，offset的计算方法是一样的（这里感觉琴键和pedal的具体时间的算法的写法有点问题，没有体现onset和offset的成对属性）。 实验: 数据: ​	数据集用的是MAESTRO dataset V2. 0. 0，包含200h钢琴独奏录音和对应的MIDI琴谱，时间精度约3ms。 预处理: ​	所有音乐转为单声道，并将采样频率同步成16kHz，然后将其切割成10s的片段。然后将每个片段通过短时傅里叶变换（Hann窗口大小是2048）转成频谱。 参考： [1]. Leland Roberts, Understanding the Mel Spectrogram, medium, 2020 [2]. BeichenLiu. Polaris，语音特征提取: 看懂梅尔语谱图(Mel-spectrogram)、梅尔倒频系数(MFCCs)的原理，2021 "
    }, {
    "id": 19,
    "url": "http://localhost:4000/UC-Berkeley%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-Implicit-Models-GANs-(%E4%B8%8A)/",
    "title": "UC Berkeley非监督学习--Implicit Models -- GANs (上)",
    "body": "2022/05/10 -     翻译整理一下UC Berkeley非监督学习的课程。这篇翻译第五六讲Implicit Models – GANs。分三篇：上，中，下。     这个课程总共十二讲，官方链接： https://sites. google. com/view/berkeley-cs294-158-sp20/home    目前已整理过：         Lecture 1：UC Berkeley非监督学习–介绍         Lecture 2：UC Berkeley非监督学习–自回归模型         Lecture 3：UC Berkeley非监督学习–流模型（Flow Models）         Lecture 4：UC Berkeley非监督学习–Latent Variable Models – VAE（潜变量模型–VAE）         Lecture 7：自监督学习（Self Supervised Learning）     这篇讲Implicit Models，很多是GANs，内容有点丰富，占了两个Lectures，因此这次会分上中下三篇。整理完这两个，剩下的8，9，10，11，12接下来可能不会整理，也可能会慢慢整理。 这是发明GAN的人。这样看效果图，GAN确实很强啊。大家可以在B站或者油管搜BigGAN的视频，看看变换潜变量可以达到的不同效果。 2018年，一个人从github上下载了些代码，用GAN训练了模型并生成了上面这幅图，卖了$432500，，所以，看完这篇吧，让我们离财富自由更近一点。 图像生成的目标：    可以理解图片中的内容   可以完成从一幅图到另外一幅图的丝滑转换。 可以生成和样本相似却又不同的图。 可以通过调整潜变量生成有不同特征的图（样本图中没有的特征）。      Implicit Models要做的也是，先生成一个潜变量z，再通过模型把潜变量转换成图片。但他训练模型的方式和VAE以及Flow Models是不一样的，它通过评估生成的图片的品质来优化模型。 Implicit Models的基本逻辑： 翻译一下是：  需要样本图片pdata 需要可以生成图片的q，然后生成一堆图片pmodel 需要一个可以比较pmodel和pdata的差异的模型D 获得更好的Φ的方式是努力去缩小pmodel和pdata的差异GAN的第一篇文章发表于2014年： 来看损失函数： G是生成器（z-&gt;x），D是分辨器（分辨输入是G生成的，还是训练样本中的）。D越是确信输入是训练集中的样本，则输出的值越大。D和G存在一个博弈关系，G的目标就是尽量让D(G(z))的输出趋近于1，而D的目标是尽量让D(x)的输出趋近于1，D(G(z))的输出趋于0。  大家可以去下面的网址去观察一下gan训练的可视化效果，还挺有意思的： https://poloclub. github. io/ganlab/     上面这张图是2014年GAN可以做到的效果，右边黄框里的是样本图片，其他是生成的图片，可以发现，当时GAN就可以很好的生成和样本图片近似的图片，但是还不完美，还有很多似是而非的图片产生。     接下来看如何评估GANs模型，到现在生成模型的评估依然是一个问题，而GANs不像之前的生成模型，我们可以用极大似然值来评估，另外，生成图片的优劣其实是一种主观判断。     评估GANs模型：1.   Parzen-Window density estimator / Kernel Density Estimator (KDE) 这是2006年被提出来的一个概念，网上解释很多，这里就不细说了，详细可以看参考[1]。上面公式中K是窗函数，判断x和xi的距离是不是在一定区间内，K也可以是高斯函数等。KDE要输出密度值，越多样本在x周边，给出的密度值越大。 当h设置不同的值时，出来的效果也很不同，用这个方法得选一下窗口值的大小。 2014年的GAN有用这个方法来评估模型，但事实上这个评估方式更适合低维度的数据，不太适合高维度的数据。 上图实验数据，用于6*6，36维的数据评估，可以发现这个评估方法的效果不是很好，而且和k-means这些方法的比较，也可以发现，也许这个方法只能很好的评估聚类效果，而不是生成效果。 2.   Inception Score 看一下Inception Score的基本想法和方法：     绕过高维密度估计   一个好的生成网络可以生成多种多样不同语义图片 给定生成图片，图片中的事物可以被明确地分辨出来，p(y|x)应该是低熵的 通过调整潜变量，可以生成含有不同事物的图片，p(y)应该是高熵的。基于以上考虑，Inception Score公式如下： 值越高越好。 由上面的实验结果可以发现，Inception Score确实可以很好的区分真实图片和生成图片，评估生成图片的质量。但这个方法也有缺点，比如说，背景信息，边边角角的生成质量被忽略了，另外就是如果在生成数据的时候，针对Inception Model可以识别的类，一个类生成一个，那么最后就可以获得很好的Inception Score。 3.   Fréchet Inception Distance Fréchet Inception Distance （FID）的基本想法和方法：     Inception Score不够优秀，包括无法评估生成图片的细节优劣，另外如果你生成有方，模型可识别的类型每样生成一个，只要生成的质量不是太差，都能获得很高的Inception Score   FID可以更多的分辨细节上的优劣。 实现方法就是把生成的图片转换成特征向量，feature embedding，即不是用判断物种类别的结果，而是用判断类别前生成的特征向量来判断生成样本的好坏。同样是用生成样本和训练样本去做比较，比较特征向量的均值和协方差。 值越低越好 IND是越低越好，FID是越高越好，通过上面几幅图比较，明显FID更优秀。 现在我们来总结一下GAN的基本特征：     可以快速生成，随机生成一个潜变量，然后通过训练好的生成网络，即可产生图片   没有推理网络，只有一个生成网络，和一个判断图片是不是生成的网络，不需要像VAE一样再有一个把图片转换成潜变量的网络（GAN变种可以有） 目标函数直接明了。一些原理：     接下来看一些GAN背后的原理。 1.   Bayes-Optimal Discriminator     如果已知训练数据的样本分布和生成数据的样本分布，最佳分辨器是什么？     由此可知，当训练样本分布和生成样本分布都已知的情况下，最佳分辨器是可以直接求出来的。 如果用最佳分辨器来训练生成网络呢？ 由于分辨器已经用了最佳，V肯定是越小越好，按上面的公式，V可以分解为一个常量项和两个KL项，两个KL项大于等于0，则V可能的最小值就是-log(4)，而优化生成模型的时候，其实我们只需要优化两个KL项，即JSD，使其最小化即可。 2.   Jensen-Shannon Divergence （JSD）      我们来看一下JSD和其他方法的比较，如上图所示，原始数据的分布如左边第一幅图，我们现在要用一个高斯函数来拟合这个分布，KLD，MMD，JSD的拟合结果分别用圆线展示出来了，会发现，KLD想尽量囊括所有数据的特征，MMD会倾向于最强的分布，但是又会被小的分布影响到。而JSD则是比MMD更倾向于最强分布，忽略了小的分布。     上图展示了分别用Maximum likelihood和Reverse KL拟合的结果，可以发现前者为折中拟合，后者是二选一拟合。而JSD其实介于这两个方法之间，但是更接近Reverse KL。 左边是Maximum likelihood，中间是Reverse KL，右边是JSD，可以发现，JSD其实是介于前两者之间的一个方法。 那么究竟哪种拟合效果更好，更值得追求呢？目前来看，如果是想保留更多的信息，比如压缩图片，那么第一种可能更好，如果是图像生成，需要尽可能清晰地获取特征，那么可能Reverse KL，JSD会更好。更好的方法当然是尽可能避免用一个高斯函数去拟合复杂的数据分布。但是高纬度的数据复杂度是极高的，我们不太可能用我们的肉眼，包括一些数据分析工具来知道，我们应该用什么样复杂度的模型去拟合，因此，在选择优化模型的方法时，还是得多注意一点，尽量避免Mode collapse，或者效能上的损失。 3.   Mode Collapse 比如上图这种情况，数据分布如最右边的小图所示，仍然是用高斯分布去拟合，会发现，得到的结果会随着训练进程一直在变。 3.   Avoiding Saturation  来看，可不可以先训练D呢？以及，如果我们一开始只优化D，会出现什么问题。     答案是可以先训练D，而且可以训练出来不错的D。但是如果把D先优化好，再来优化G，G会没办法优化。  当D可以很好地识别训练图和生成图后，D是一个很好的二分类分类器，假设D那个时候是一个sigmoid函数。那么它求导以后如右下角所示，当x是假图的时候梯度趋近0。因此G不会被优化。 但这个问题有办法可以解决： 优化G的时候加个负号，最小化变最大化。  D优化后，当输入x是生成图时，梯度趋近于-1。问题解决。 [1] kiyoxi，Parzen window 密度估计——一种非参数概率密度函数估计方法，知乎，2022 "
    }, {
    "id": 20,
    "url": "http://localhost:4000/UC-Berkeley%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-Latent-Variable-Models-VAE-%E6%BD%9C%E5%8F%98%E9%87%8F%E6%A8%A1%E5%9E%8B-VAE/",
    "title": "UC Berkeley非监督学习--Latent Variable Models -- VAE（潜变量模型--VAE）",
    "body": "2022/05/06 -     翻译整理一下UC Berkeley非监督学习的课程。这篇翻译第四讲Latent Variable Models – VAE。     这个课程总共十二讲，官方链接： https://sites. google. com/view/berkeley-cs294-158-sp20/home    目前已整理过：         Lecture 1：UC Berkeley非监督学习–介绍         Lecture 2：UC Berkeley非监督学习–自回归模型         Lecture 3：UC Berkeley非监督学习–流模型（Flow Models）         Lecture 7：自监督学习（Self Supervised Learning）     之前的自回归模型是这样生成的，有了x1之后，生成x2，两个都有了之后生成x3。这样的依赖关系，让生成图片特别慢，要生成一个28*28的图片，就得把模型跑28*28次。     流模型虽然也用了潜变量来生成，但是流模型的潜变量和样本变量是一对一的，也就是说要生成的图片有多大，那么潜变量就得有多大。潜变量在流模型中不是一个更抽象的存在。这一讲中的研究，就会用更抽象的潜变量来生成，如果要生成一个超级大的图，不一定需要一个很大的潜变量。     下面这种图，其实可以被简短的语言描述，三只柯基，每只是什么颜色，有什么与众不同的特征，背景是什么样子的，有些什么，通常你把这些信息告诉一个画家，画家便可以为你创作出一幅画来。因此生成图，其实并不一定需要一个和图差不多大小的潜变量，一个很小的向量，也许就可以。     理论上，潜变量是可以有明确的意义的，比如你通过潜变量来生成人像图，向量第一个值可以代表发色，第二个值代表眼睛的大小，第三个值代表肤色，总之可以让潜变量中的每个值有明确的意义，使得图像生成可以随人心所欲。虽然现在主流生成模型很少去定义潜变量的意义，但这种生成方式正被积极研究。     接下来这一篇，都在讲怎么基于潜变量，生成样本。            若z中的每个值都服从伯努利分布，那么p(x|z)实质上也是服从伯努利分布。     这篇讨论的内容包括：    1. 采样/生成：     2.  模型评估，极大似然函数：  上面的公式就是训练目标。    3. 数据表现(representation)：     样本转换为计算机更容易理解的抽象表达。 模型训练     模型训练的目标：     分两种情况：     一种情况是z的可取值就那么几个，几十个或者几百个，总之不算多，比如下面这种情况，z只有三种可能，且每种可能发生的概率是三分之一：     假设p(x|z)服从多元高斯分布（Multivariate Gaussian Distribution），那么则有如下训练目标：     看一个可以用以上方法解的实例：     通过对训练数据的观察，很容易发现样本集分三个群，有遵循高斯分布的感觉，不过这个图画得不特别明显，可以点再细些，看看是不是每个群离中心点越远，样本越稀疏。如果用上面的假设来训练，最后可以很好地生成类似的样本分布图。当然也可以做其他假设，可以尝试不同假设，然后取效果最好的那个使用。     第二种情况是如果z可以有很多很多不同的值呢？      一种方法是可以采用对z进行采样。z的概率就由采样的状况决定了。但这种方式有一个很大的缺点，如果z的可能性太多，那么z的采样效率会很低很低，比如z是一个长度为100的二值向量，那么z就有2的100次方个可能性，如果对z进行全域均匀采样，每个样本x匹配到对应的z的概率就是2的一百次方分之一。 就现在的计算能力来看，就是没什么可能能让样本x匹配到对应的z。     毫无疑问，均匀采样是不可行的，那么要怎么采样呢？接下来了解一下重要性采样（Importance Sampling）。     如果z服从pz分布，那么函数f(z)的期望值如何计算：     计算这个可能会有两个问题，一个问题是pz这个分布过于特殊或者复杂，或者其他什么原因，让你没有办法正常采样。另一个问题是说z的可能性太多，一般的采样方式效率非常非常低，采样出来的z可以提供的信息非常少。这篇遇到的是第二个问题。     为了解决这些问题，可以进行一些转换： 既然在分布pz上采样不可行，那么自然要找到适合采样的分布q，那么f(z)的期望值(z服从pz)问题则转换成了求(pz(z)/q(z))f(z)的期望值，z服从q分布。       那么训练目标也更新了：     现在的问题在于，如何求这个分布q。理论上，可以如下方式求：     但分母恰是我们本来要求的问题。所以这个方法不可行。求精确的不行，那么求个大概也许可行，比如可以假设q(z)服从高斯分布： 而接下来要做的就是求这个高斯分布的参数，使得q(z)尽可能接近：  但是这个方法呢，也有一个问题，就是对每个样本都得求它对应的z的q分布。而过程又是一致的。因此，可不可以直接交给神经网络来呢？与其对每个样本都来求一次它对应的z的分布，不如求一个一般的神经网络，使得输入样本后便能输出对应的z的分布。 Amortized formulation: （PS：终于绕到这个公式了，以前觉得理所应当就应该这样的，没想到原来是这么九曲回肠地绕过来的。） 如上图所示，生成模型实实在在想做的事情是图中实线所示的事情，但是为了求得能完成实线功能的模型，得把实现虚线功能的模型也求了。 Importance Weighted AutoEncoder (IWAE) 目标是最大化： 而因为z的采样问题，同时需要最小化 所以要做的就是最大化公式一减去公式二的值 定义wi和Lk如下： log是凸函数，根据Jensen不等式，先log再期望小于等于先期望再log。     有研究人员在2015年证明，Lk是一直小于等于logp(x)的，k的数值越大，越接近logp(x)，当k趋近于正无穷时，Lk = logp(x)。 证明如下： Variational Lower Bound (VLB)/ Evidence Lower Bound (ELBO) 以上推理过程如果想把不等式转换成等式，那么不能单单只优化θ，还得优化Φ，反复不断优化θ和Φ，获得更好的q分布和p分布。当q是最佳时： 再有一种写法： 在上面的写法中，如果q不是最佳，那么求得的期望值就会小于等于logp(x)。那么差多少呢？  这种写法和上面那种就是多了后面KL散度的部分，第一种写法我们知道期望值与logp(x)有差，这一种方法我们可以知道差的就是KL散度部分。当KL散度求值结果为0时，那么logp(x)就和后面的期望值一致了。 而在优化的过程中，KL散度是不需要被考虑的，VLB的部分才是需要计算的： 优化算法可以用梯度下降的方式，那么首先我们得求梯度： 课程中介绍了两种方式，这里我们还参考了另外一个课程[3]，和一篇博客[4]，首先，当我们要微分的变量存在于分布中时，我们用likelihood ratio gradient estimator： 当我们要微分的变量存在于待求的期望的函数中时，我们用pathwise derivative： 关于likelihood ratio gradient estimator可以看参考[4]，这里就贴一下推导公式： 上面公式是我们要求梯度的函数，而现实实验中，我们通常没有办法直接得到分布函数，能得到的是一堆样本，因此： 梯度公式的求导过程如下： likelihood ratio gradient estimator在绝大多数场景中都可以使用，而pathwise derivative在一些场景中效果会比likelihood ratio gradient estimator好很多，即使只有极小量数据也能取得好的结果。     我们要用的其实是θ，但要得到好的θ，得同时交叉优化Φ。 极大似然估计（likelihood ratio）：    pathwise derivative （这个方法是有前提的，q得遵循高斯分布，p也是，但两个是不一样的参数）： 来看VAE的生成数字的效果图：  VAE是将概率图模型的思想和AE（Auto Encoder）的结合体。[6] 接下来看几个典型的VAE模型的变种：   VQ-VAE **发表于2017年，几乎没有被关注，大家的注意力都在GAN那里。直到两年后第二篇文章发表，效果非常好，两篇文章才引起了关注。 这个算法就不细讲了，感兴趣的可以看看参考[5]和论文原文，上图中带箭头的线条，蓝色是推理，红色是反向传播。损失函数中有一个比较重要的概念是Straight-Through Estimator，前向传播时，sg符号无用，忽略掉即可，反向传播时，sg标记的部分不进行调整，保持不变。 VQ-VAE重建图片的能力如上图所示，左边是原图，右边是重构的图片。 VQ-VAE图像生成的效果，如上图所示。 2. VQ-VAE 2. 0 这里就直接摘参考[5]的文字描述了：     “这是VQ-VAE的升级版, 可以生成非常清晰的高分辨率图片。主要变化就是把VQ-VAE的encoder和decoder进行了分层, bottom层对local feature进行建模, top层对global feature进行建模; 为了让top层能更有效地提取global信息, 在网络中加入了self attention。除此之外, 在prior上进行采样的时候, 考虑到autoregressive的sample会累积误差, 即如果x1出现了一点误差, 那么x2|x1的误差会更大, 因此加入了rejection sampling, 即生成某类的图片后, 通过一个classifier判断一下生成的图片像不像这个类的sample, 如果不像就弃掉. 文章效果很惊艳, 但是理论上没有特别大的改进。” 那接下来看看惊艳的效果：  ​	可以和BigGAN比了，右边的图是BigGAN生成的，左边的是VQ-VAE生成的。可以发现，左边的图其实更多样化一点。  3. z和decoder ​	到目前为止，所用的decoder都特别简单（p(x|z)遵循的分布都是简单的），因此没有办法携带大量信息，基本上有用的信息都会压缩到潜变量z里。     前面推导过： 因此： VLB最大可以是logp(x)，如果KL部分是0。 如果p(x|z) = pdata(x)，则有： 这个时候，训练模型q(z|x)会去接近p(z)，那么z中就没有什么有用信息了。p(x|z) = pdata(x)要怎么做到呢？即decoder包含了所有信息。因此，如果z中信息越少，decoder越强大，信息越多，那么VLB越接近理想值。因此许多工作都往这个方向去努力了，但是这样的话，就没有办法很好地进行生成了。这个问题在许多工作中有被描述： 为了解决这个问题，需要弱化decoder： 其中，第二个是VLAE，这个方法是想让z去掌握全局信息，而decoder去掌握细节信息，decoder用的是浅层PixelCNN，适合挖掘局部细节信息。但也有很多方法想让z不仅掌握粗粒度的全局信息，还想让他表达细节，这里是一些动态训练的方法： 4.  Disentanglement: Beta VAE ​	与VAE不同的地方就是损失函数KL散度那部分，前面加了一个beta，当beta是1的时候，就是标准的VAE，当beta大于1的时候，z的信息丰富度会降低，但是解纠缠的能力会提高。解纠缠的能力可以理解为representation的能力，即z中的值可以表达特定的意义的能力，比如生成人脸，我们会希望z中的值有特定意义，比如脸的宽度，眼睛的大小等，这样可以通过调整z中某个值来变化人脸的局部特征。当beta变大时，会让训练出来的结果更趋近于高斯分布，而每个高斯分布的参数又相互独立，因此可以更好的解纠缠。下图就展现了，用这种方法训练出的可以通过调整z中的值来生成不同肤色，年龄，性别的人像的模型。 其他 1. Variational Dequantization (Flow++)     在训练这类模型的时候，有一个问题，就是图像数据（像素值，0-255）是离散数据，而我们要输出的概率密度函数，是连续的。如果不对数据进行处理，那么出来的结果可能是整数点上的概率密度值都很高（比如3，78，255），但是他们周边就很低（比如3. 1，77. 9，244. 7），因此需要对输入的数据做一些处理，这个处理叫解量化（Dequantization）。也有其他一些方法，比如Uniform Dequantization： 但由于VAE会用Jensen’s inequality做一个近似转换，这样一来，噪音项又被消掉了。所以不管用。 还有一种方法是，flow++中的Variational Dequantization： 添加一个可训练的噪音项，效果如下图：  2. Mutual Information Estimation      互信息（Mutual Information）： ​	首先来回顾一下什么是信息熵，可以参考Cross Entropy Loss，H(X)是X的信息熵（信息量的期望值，即表达这个事件需要的编码的位数的期望值）： 可以发现，互信息可以用来表达两个变量的依赖程度。 互信息的应用场景： 这些可能之后会提到。VAE中会用到的是，计算z和x的互信息值：       第三行(logp(z   x)-logq(z   x))构成KL散度。为了提高z和x的相关度，我们需要做的是最大化公式最后一行的内容。    声明：文中所有图片和原理均来自课程和参考。同时所有参考都是推荐扩展阅读的内容。 参考： [1] Jeremy Zhang, Importance Sampling Introduction, 2019 [2] Bo’s Blog，Importance Weighted Autoencoders and Jackknife Methods，2019 [3] Katerina Fragkiadaki, ‘Pathwise derivatives, DDPG,Multigoal RL’, in Deep Reinforcement Learning and Control, Carnegie Mellon [4] Timvieira, The likelihood-ratio gradient，Graduate Descent，2019，https://timvieira. github. io/blog/post/2019/04/20/the-likelihood-ratio-gradient/ [5] Elijha，VQ-VAE解读，知乎，2019 [6] 大象, VAE系解纠缠：从VAE到βVAE，再到β-TCVAE, 知乎，2019   "
    }, {
    "id": 21,
    "url": "http://localhost:4000/UC-Berkeley%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E6%B5%81%E6%A8%A1%E5%9E%8B-Flow-Models/",
    "title": "UC Berkeley非监督学习--流模型（Flow Models）",
    "body": "2022/03/21 -     翻译整理一下UC Berkeley非监督学习的课程。这篇翻译第三讲Flow Models，流模型。     这个课程总共十二讲，官方链接： https://sites. google. com/view/berkeley-cs294-158-sp20/home    目前已整理过：         Lecture 1：UC Berkeley非监督学习–介绍         Lecture 2：UC Berkeley非监督学习–自回归模型         Lecture 7：自监督学习（Self Supervised Learning） 1. 1-D     先看一维空间。样本都是一个值，生成也只需要生成一个值。      上一篇讲自回归模型，一直在求概率分布函数，这一节我们来求概率密度函数。本质上他们是一回事，只是前者是用来求离散值的概率，后者是用来求连续值的概率。     如果我们要预测的值是个连续值，而不是离散值，那么理论上一般情况下任何一个特定值发生的概率都趋近于0。所以如果是连续值，那么通常求的是在某个区间发生的概率，p(x)是概率密度函数，如上图右边公式求的是x在[a, b]这个区间的概率。概率密度函数一般满足以下条件：     那怎么基于概率密度函数产生极大似然函数：      求解时可以假设概率密度函数服从高斯分布。     但是很可惜，上面那种方法对高维数据，效果很差。     流模型中输出不再是概率密度函数，而是一个潜在变量（latent variables），我还是喜欢理解为特征向量。 再通过改变潜在向量来让计算机创造图片。 那么如何训练模型： 由于p(x|z)是很难求出来的，因为z可以是无穷的，所以只能换个思路： 概率密度函数的特性，全域积分为1，根据这个特性：  上面公式中f 得是可微且可逆的。可逆才可以根据潜在变量进行生成： 极大似然函数则可以转换为： 来看z遵循不同分布的训练结果(这里讨论的z只有一个值，即在一维空间中即可定位)： 比如均匀分布(Uniform)： 比如Beta(5,5): 比如高斯分布： 那除了训练模型，有没有其他方法可以得到x-&gt;z的函数，当x和z都只有一个变量的时候，是可以的。比如下面这个方法Cumulative Density Function (CDF)： 这样的话，z服从均匀分布，这种转换和采样的方式和上一个Lecture中Hostogram采样的方式理论上是一致的，只是CDF用于连续值，Hostogram用于离散值： 可以发现CDF可以把任何分布通过一个可逆可微函数转换成均匀分布，而可逆可微函数的逆函数依然可逆可微，因此均匀分布可以被转换成任意其他的分布。因此CDF可以实现将一个分布通过两次转换转换成任意其他分布的功能。 以上是z是单变量的情况，如果z是一个长度为2的向量呢？长度为n呢？ 1. 2-D     在上个Lecture中我们详细描述了自回归模型的原理，自回归的原理也可以用在流模型上，称之为自回归流模型（Autoregressive Flow）。既然是自回归的模型，那么就如上图，z1，z2依赖的值是不一样的。     看一些实验效果：     上图中右上角的图是x在二维空间的分布情况，注意一下，颜色只是一个分群的效果，不论紫色或者黄色都是x的样本。转换成z，且在二维空间中均匀分布，那么训练后得到z的分布情况则如右下角组图中左边的图所示。黄色和紫色的点没有区别，也都是样本，只是这样可以和x中的样本表达一个对应关系。     这张也是一样理解，任意分布都可以转换为均匀分布。 1. n-D     那么更高维呢？自回归流模型是否还能做到一样的事情。     如果使用流模型，基于的是逆函数的原理来生成，那么x和z的维度需要一致，左边的图片有多少个值，z就也得有多少个值。（注意：这里的维度概念和我们通常理解的矩阵维度，空间维度是不一样的概念）。 Autoregressive Flows (AF)     如果是自回归流模型，先看看是怎么进行生成的：     可以发现和自回归模型是一致的，先由由逆函数通过z1得到x1，再基于z2，x1得到x2，这样一直下去。而训练过程中的似然函数，和自回归模型中的也是一致的：  其中绝对符内的是f 的雅可比矩阵（Jacobian determinant），关于它的计算方式，可以参考[1]。f 必须是可微的，因此像一些不可微的激励函数比如ReLU，是不可以在这类模型中用的。另外，增加模型的深度可以直接将Flows接起来： Inverse Autoregressive Flows** (IAF)**     新的算法IAF，和AF不一样的地方很容易发现，生成的时候，只依赖z了，所以可以一下子就根据z把图片生成出来，而不需要一个像素点一个像素点顺序生成了，但是训练的时候，却需要一个像素点一个像素点把完整的z计算出来。也就是说AF是训练可并行，生成得顺序，而IAF是训练得顺序，生成可并行。 Affine Flows Affine Flows的参数就是一个可以把x-&gt;z的矩阵和一个偏置向量。训练就是求这个矩阵和这个偏置向量的值。 Elementwise Flows  这种方式可以大大减少计算量。 NICE/RealNVP 上面的公式很容易理解，就是把x分两半，前面一半直接给z，后面一半本质上是经过了一个Elementwise Affine Transformation。s和t都可以直接计算出来，s和t应该都是长度为d/2的向量。 因此，计算量大大减少了。  上图是RealNVP的效果，可以发现还是不错的。 那怎么把输入分两半呢： 如上图所示，可以不同位置分，但其实还可以不同通道分，可以不止分一种，可以分很多很多种，一起训练。另外就是RealNVP也会像其他神经网络一样，在一层一层网络推进的时候，减小每个通道的尺寸，增加通道的数量来更好地进行特征提取。如下图所示： 输入是3232c，通过第一层网络后，输出为两个16162c，通过第二层后输出为两个884c，再通过第三层，最后输出为4416c。 由上图可以发现，把图片分两半的方式是很重要的，左边是通过checkerboard去分，右边则是上下左右分，可以发现，右边这样粗暴的分法带来的结果并不如左边精细的来得效果好。 Flow++ = MoL transformation + self-attention in NN bits/dim越低越好 Glow （Invertible 1x1 convolutions + Large-scale training）     Glow是OpenAI的作品，可以生成十分真实的人脸。还是那句话，图像生成，不止GAN可以做到。 Continuous time flows (FFJORD)     允许不受限的架构，且保证了快速的概率计算。 反量化（Dequantization）     如果把概率密度函数应用在离散数据的数据集上，会出现一些问题： 理论上来说，密度值可以无穷大，所以很容易出现上图中间的情况，出现和周边密度值完全脱离的特别大的密度值，这样也可以loss很小，但很明显是不合理的。解决办法之一是把概率密度函数转换成概率分布函数来做这件事情： 但这样很麻烦，简单的是给数据集增加噪音，进行反量化（Dequantization）。 老实说上面公式没看懂，感兴趣的可以找论文看，本质上就是把离散分布转换得连续一点： 还是之前的数据集，经过反量化转换后，就不容易在概率密度函数上产生很突兀的峰值了。 结语：     流模型就到这里了，那么这类模型其实还有很多工作可以做，从上面的内容来看，关于她的工作还是比较少的，她还是有很大潜力可以变得更好，比如更快地生成更好的图片，更快地推理，更快地训练，更好地压缩图片等等。 声明：文中所有图片和原理均来自课程和参考。 参考： [1] Lil’Log, Flow-based Deep Generative Models, 2018 "
    }, {
    "id": 22,
    "url": "http://localhost:4000/UC-Berkeley%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E8%87%AA%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/",
    "title": "UC Berkeley非监督学习--自回归模型",
    "body": "2022/03/09 -     想系统学习一下生成网络，开个新坑，翻译整理一下UC Berkeley非监督学习的课程。这篇翻译第二讲Autoregressive Models，自回归模型。     这个课程总共十二讲，官方链接： https://sites. google. com/view/berkeley-cs294-158-sp20/home    目前已整理过：         Lecture 1：UC Berkeley非监督学习–介绍         Lecture 7：自监督学习（Self Supervised Learning）     这篇会讲一些非常简单的生成网络，比如histogram，还有最新的一些基于神经网络的自回归模型。     我们先考虑一个问题，一个长宽分别为128个像素的彩色图片，可以用多少维空间中的一个位置（点）来表达，答案是大概是五万维。模型的目标是可以得到复杂高维数据的分布。并实现计算上和统计上的高效性，计算上的高效性是计算可以快速完成，统计上的高效是指不需要太多数据，模型就可以得到正确的分布。      看具体的模型。 1.  Histogram 这里举个例子说这个算法可能会容易懂一点，假设你接了一个案子，预测华山山顶每天的最低气压，精确到个位数就行，然后你得到了最近三年每一天华山山顶上的最低气压，但只有一个气压值，对应的时间丢失了，然后你要用Histogram算法来预测明天华山山顶的最低气压值。有了这些数据，你可以得到每个气压值在过去三年中出现的次数，频率。也就得到了最低气压值的概率分布。在没有其他常识或者先验知识的情况下，你根据Histogram算法给出的答案，就是有最高概率的最低气压值。 接下来看Inference和Sampling要怎么完成： Inference是推理出数据的概率，比如你有一个气压值，Inference就是推理这个气压作为最低气压值的概率。 Sampling是抽样，抽样的方法是，首先计算总概率，一般是1，然后在0-1之间按均匀分布随机取一个数u，Fi计算的是P1-Pi的总和，所以从P1开始加上P2，加上P3，当加到Pi时，发现Fi恰好大于或者等于产生的随机数u，那么这个i就是要抽出的样本。 一维空间的数据可以用这样的算法解决，那么多维空间的呢？ 多维空间就不行，举个例子，比如我们有个数据集叫MNIST，里面都是二值图，即每个像素点的值都是0或1，那么可能的图有多少种呢？2的784次方个。所以如果你对这样一个数据集画直方图，横坐标有2的784次方个值，而这个数据集中的图片远没有这么多，因此就目前的现实情况，根本无法用Histogram的方式来计算概率分布。 那么对于一维空间的数据，Histogram是个不错的算法吗？很明显也不是的。通常都会像下图中左边的样子，train set和test set，Histogram的结果虽然看着形状相似，但是有些值差别会特别大。 因此，如果可以产生一个更平滑的分布结果，综合考虑每个值周边的概率分布来确定自己的概率分布的话，也许能更好地匹配测试集的概率分布。 2.  Parameterized Distributions and Maximum Likelihood  因此，与其去求变量的概率分布，不如去求概率分布函数。 每个变量理论上都有一个概率分布函数，而通常这个函数是不容易被解出来的，我们可以做到的就是做很多很多假设，看看每个假设在现有数据集上的效果，然后用在现有数据集上表现最佳的那个假设来作为变量的概率分布函数。 要完成上面的任务，需要解决两个问题，一是怎么做假设，另外一个问题是怎么怎么用现有的数据集评估假设。 假设的话，可以根据一些经验设定，比如解决图像识别问题，可以用卷积神经网络。而假设的评估，则需要损失函数。 一般用来设计损失函数，有极大似然估计和KL散度（参考：Cross Entropy Loss）。这里用极大似然估计说明，我们的目的是求概率分布函数，比如一个图像识别问题，其中一个假设是一个三层的卷积网络可以做这个任务，但构成这个网络需要一堆参数，这些参数一开始是未知的，所以问题就变成了，如何求这些参数，使得这个函数可以更好地用作现有数据的概率分布函数。那这个待求参数的网络，假设（其实带不同参数的网络也可以被看成不同的假设），函数，我们也可以称为似然函数。那如何评估不同参数下的函数，方法其实很简单，就是最大化现有样本的出现的概率（既然这个样本曾经出现过，那么我们认为，它有很高的概率再次出现，如果这个样本曾经多次出现，那么它应该会有更高的概率会再次出现）。 完成极大似然估计或者KL散度，可以用SGD算法。 除了极大似然估计，在设计损失函数的时候，用贝叶斯加入先验函数也是常使用的方法。 3.  Autoregressive Model 极大似然估计是频率派，也就是说是完全基于现有数据来评估模型，而贝叶斯属于贝叶斯派，会加入一些先验知识。 举个例子，比如事件A是世界即将毁灭，假设P(A) = 0. 000001，事件B是太阳从西边升起，假设P(B) = 0. 0000001，假设如果世界即将毁灭，太阳会从西边升起的概率是0. 001，即P(B|A) = 0. 001，那么当我们看到太阳从西边升起时，我们可以计算出世界即将毁灭的概率P(A|B) = P(B|A)P(A)/P(B) = 0. 01，这个公式我们称之为贝叶斯公式。也就是说一件事情发生的概率，会因为一些已经发生的事情而产生改变，当我们判断一件事情发生的概率的时候，需要综合考虑相关的事件。而自回归模型则正是基于这种理念。 来看一个只有两个值的自回归模型： 那这种自回归模型有什么问题呢？ 首先，对于高维空间数据，依然不友好，另外作为条件的数据都是同等对待的，但他们其实和要预测的事件也有亲疏远近之分，而在之前的自回归模型中没有体现出来。解决方案有两个，一个是用Reccurent Neuarl Network，一个是Masking。 4.  Recurrent Neural Nets （RNN） 这是上一篇中提到的，在NLP领域十分有效的CHAR-RNN，图右边是一个非常典型的RNN网络。这是一个自回归模型。本质是根据之前出现的字母来推测下一个字母。 RNN在图片生成上的应用，如果单纯根据之前的像素点（像素点依序排列）来推测下一个像素点，可以做到如下效果： 但图像和文字有所不同，文字是一维的，黑白图像是二维的，所以如果依序输入像素点的时候能同时输入每个像素点在图片中的位置，那么训练效果会好很多（很多最先用在NLP领域的模型在被尝试用在CV领域时，都会加入position encoding，典型如VIT）： 5.  Masking-based Models （1）Masked Autoencoder for Distribution Estimation (MADE) MADE的运作方式就是在一般的aotoencoder上加mask，左边是一个很普通的全连接网络，最后预测出来的每个值和输入的每个值都是相关的，模型会去根据输入样本的所有数据来产生输出，但每层网络加上mask之后，就不一样了，mask不是随机产生的，而是根据需求设计过的，如右边添加了mask后的模型。MADE做自回归模型要输出的和RNN做自回归模型要输出是一致的，都是为了完成下面的计算：                  为了实现这样的目标，那么MADE网络最后输出的值，比如上图中网络最后一层的1的输出，它不依赖于任何输入的值，因此它表示一个完全独立的概率分布p(x2)，而2的输出是依赖于x2这个输入的，3的输出则同时依赖于x2，x3的输入，因此可以用来表达条件概率分布。这样MADE就可以获得要用来做自回归的所有概率分布。 这是MADE用于数字生成的效果图，训练集是MNIST。 MADE和其他方法的比较。 我们希望MADE具有图像生成的能力，即可以通过训练，学习到特征，从而生成符合预期但却没有过的东西（模型没有见过），而不是直接把看过的图片复制出来，因此研究者会比较生成的图片和训练集中和这些图片最接近的图片。如果完全一致，模型其实只是记忆和复刻了训练集，而如果是特征一致，但细节不一致，就说明有生成的能力。上图左边是模型生成的图片，右边是和生成的图片最接近的训练集中的图片，发现大多数其实是特征一致，但是细节不一致，所以我们可以初步得到结论，MADE具有生成的能力。 自回归模型图片中数据输入是有一个顺序的，那么怎么给一个图片中的数据排个序，怎么排序才能使模型有更好的生成能力，这也是一个问题。上图中尝试了不同的像素排序方式，比如完全随机，比如先单数后双数，比如一行一行扫描，一列一列扫描等，效果大家自己观察了，PPT上的这个图和视频课程里的这个图出入有点大。 Mask的数量和训练效果的关系，如上。不过值得注意的是，因为训练的是生成网络，所以模型的优劣有时候不太能直接用这种硬指标来衡量。 视频课程也有描述怎么用训练好的模型sample，这里基于一开始的那张图简单说一下，x1可空，然后产生p(x2)，根据p(x2)确定x2的值，作为输入，输出p(x3|x2)，根据p(x3|x2)确定x3的值，再输入x3，再跑模型得到p(x1|x2,x3)，以此确定x1的值。可以发现，这个模型在sample的时候其实很像RNN，比如上面生成数字图的任务，要一个一个像素生成，要生成一张（28，28）大小的二值图，需要把这个模型跑784次。 （2）Wavenet [Masked Temporal (1D) Convolution] 上图就是一个简单的Wavenet网络，每一层都有一个一维的长度为2的卷积核，步长为1。这个方法的优点是容易实现，不论输入有多长，参数量没有变化。缺点是计算出的概率分布，能观察到的输入是固定的，比如上图中，虽然输出是p(xi+1|x&lt;=i)，但实际上计算的分布是p(xi+1|x（i-4) - i)，如果想要p(xi+1|x&lt;=i)，那么还是需要调整输入的大小，或者重新训练匹配输入大小的网络。当然也可以有其他解决方式，比如下面这个网络，用了Dilated Conv。  这是Wavenet的生成效果。 这是加入position encoding之后，Wavenet的生成效果。 （3）PixelCNN [Masked Spatial (2D) Convolution] 上面的方法都是一维的方式加上position encoding来完成图像生成，那么当然就有人会尝试二维卷积的方式来做这件事情。PixelCNN是2016年的时候被提出来的一个网络结构，这个网络依然是一个像素点一个像素点依序生成，如上图所示，现在想要生成右图中红色那个位置的像素值，如果用一个3*3的卷积核，那么会被参考的像素值是它周边的四个已经生成的像素值（蓝色位置），其他五个不参考是因为他们还没有被生成出来。这是一层，一般会有多层。这里先看一个Sample的效果，是一个一个像素点生成的：  如上图所示，已经生成的图片如左图所示，左边是输入，而输出是一个概率分布，如果像素值的区间是[0,255]，那么就是该点像素值在0到255区间的概率分布，如果是二值图，那么横坐标只有两个值，0和1，输出就是像素值在[0,1]区间的概率分布。像素值是离散值。如果是彩色图，输出应该是三个概率分布。 （3）Gated PixelCNN 如果PixelCNN网络有不同层数，那么会被参考的像素值如下图[1]所示： 按现在的结构，网络层数越多，生成这个像素值会参考到的范围越大，但是会发现，因为卷积核本身的结构特质，会有同样也很近的像素值，没有被平等地参考。应该但是没有被参考的部分被称之为Blind spot。 为了解决这个问题，Gated PixelCNN被提了出来（图来自参考[1]）： 为了规避Blind Spot，这里设计了两个卷积核，一个是上图中绿色部分，中心点是m，m是要生成的r的正上方的位置。卷积核是33大小，但是最后一行是0，这样不管网络有多少层，右边都不会再出现Blind Spot。但这样会忽略掉左边的已经生成的像素值，因此还有一个卷积核是13，只是右边两个值是0。不同层数的Gated PixelCNN参考的像素值范围如下图[1]所示： 这样就没有Blind Spot了。  Gated PixelCNN网络架构如上图所示。之所以这个网络前面加了Gated字眼，不是因为卷积核的变化，而是网络结构中加入了类似LSTM中记忆门（forget gate）的机制，上图中tanh函数生成的是概率分布值，而σ生成的是遗忘值，决定tanh生成的概率分布该如何保留。 上图是一些方法的比较。 （4）PixelCNN++ PixelCNN或者Gated PixelCNN每次输出的是一个分布，如果是彩色图，输出的是三个在[0, 255]区间上的概率分布。这存在的问题是，首先，输出的量大，计算量也就大，另外一个问题是这个像素是254的概率和这个像素是255的概率其实是强相关的，但这种方法有点像在解两个问题。因此PixelCNN++做了改变。 本质上是把一个求离散概率分布的问题变成了求连续概率分布的问题，如上面两个公式，最后得到的不是一个概率分布，而是一个概率分布函数，u决定了这个函数的位置，s决定了这个分布的形状，如下图所示： PixelCNN++还在通道上做了改进，PixelCNN或者Gated PixelCNN是分别输出三个概率分布，但颜色之间也是有相关性的，PixelCNN++则同时参考了不同颜色，输出的也是联合概率分布。 PixelCNN或者Gated  PixelCNN无法依赖远距信息，因此PixelCNN++为每一层添加了一个下采样，这样虽然损失了信息，但是扩大了生成像素可以参考的范围，而损失的信息可以用额外的方式来补偿，如下图所示： 模型的测试结果如下： 更详细可以参考[2]或者看原paper。 （5）PixelSNAIL 还有融入了残差模块和注意力机制的PixelSNAIL，这部分课程讲的不太清楚，但提出这个网络的有公布论文和代码。这里就简单描述一下这个网络，主要是融入了attention机制：  卷积让网络只能参考到一部分信息，但是attention可以参考所有信息。 进一步说，应该是masked-attention。这种机制让这个模型相较于其他模型，在生成一个像素值的时候，几乎可以参考之前已经生成的所有像素值。 这种方法可以允许模型用不同的顺序生成图片，比如上面这种顺序。 网络的核心就是一个Residual Block和一个Attention Block。 他的效果超越了之前的模型：  （5）其他  Class-Conditional PixelCNN 可以指定数字生成。 Hierarchical Autoregressive Models with Auxiliary Decoders可以生成真实到肉眼无法分辨的图片。  PixelCNN可以用来提高图片清晰度 4.  PixelCNN可以用来黑白转彩   PixelCNN一个一个像素点生成有点慢？可以加速 分块加速，比如把图片分成四块，如上图所示的方法加速。 也可以尝试其他技术加速，比如cache activations。  以上是加速效果。  也可以做视频生成  也可以做人脸生成 PixelCNN因为是一个像素一个像素输入，一个概率分布一个概率分布输出，所以我们很难得到一个特征向量来代表一个人脸。然后通过改变特征向量来生成不同的人脸。加入了Fisher Score，自回归模型也能很好的做一张脸到另一张脸的转换，如上图所示，左边做人脸的变化，出来的效果像是把两张人脸叠起来，先让一张脸显现，弱化另外一张脸，再逐渐让另外一张脸显现，但右边的图不一样，从一张脸到另外一张脸的变化中，产生的都是真实度高的不同的人脸。 下面一行用了Fisher Score的方法，可以发现，比起上面那行，下面这行在慢慢变化的过程中，产生的都是真实度高的人脸。 声明：以上所有图片和技术内容，均来自UC Berkeley非监督学习课程和参考文章，没有原创理论和图片。 参考： [1] Jessica Dafflon,  PixelCNN’s Blind Spot, towards data science, 2019 [2] Harshit Sharma, Auto-Regressive Generative Models (PixelRNN, PixelCNN++), 2017 "
    }, {
    "id": 23,
    "url": "http://localhost:4000/UC-Berkeley%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E4%BB%8B%E7%BB%8D/",
    "title": "UC Berkeley非监督学习--介绍",
    "body": "2022/03/01 -     想系统学习一下生成网络，开个新坑，翻译整理一下UC Berkeley非监督学习的课程。     这个课程总共十二讲，官方链接： https://sites. google. com/view/berkeley-cs294-158-sp20/home    之前为了搞明白自监督学习的概念，整理过Lecture 7：自监督学习（Self Supervised Learning），这次没有意外的话至少会整理Lecture 1-6。     这篇整理这个课程的Lecture 1：Introduction。课程的介绍。     那什么是非监督学习(Unsupervised Learning):     非监督学习，用没有标签的数据训练深度神经网络，学习特征，主要分两类，一类是生成网络，像GAN，VAE这些，一类是自监督学习，label是机器自己产生的，不是人工标注的。     那么为什么要用非监督学习呢？     说到底，还是因为监督学习，效率太低。上图是Hinton的思考，Hinton也有一门神经网络课程，他喜欢通过对人脑运行机制的思考来探索人工智能的解决方案，会通过理解人脑的运行机制和特质来思考智能的来源，人大概活10^9秒，但是我们有大约10^14个神经触突。那么特别明显，如果我们是通过与世界的互动来获得标签，训练大脑，那么我们应该来不及聪明就死去了。所以人类获得智能的主要方式可能不是监督学习，而是非监督学习。     这里插一些题外话，关于Hinton，Hinton最初关于神经网络的灵感来源于高中时候获知的一些生物学上对大脑的研究结果，感兴趣的可以找Hinton的课程或者演讲看一看，比起枯燥的算法，这些算法的灵感来源，科学家们的思维方式，如何觉察，如何获得的灵感和毕生的执着和方向。这些可能是更值得学习的“算法”。    另外一位科学家也有相同的想法：     如果把深度学习领域比喻成一块蛋糕，那么LeCun认为，强化学习只是蛋糕上的樱桃，监督学习是蛋糕里的冰，而非监督学习才是蛋糕本身。     也有人从不同的角度来思考这个问题，比如思考理想化的智能是什么，就是压缩，即输入信息后，要能够提取所有特征，提取特征即产生一个简短的表达或描述，那么这个过程本质上就是对信息的压缩。根据奥卡姆剃刀原理，使用越少假设的理论越有可能是对的，延伸到深度学习，Solomonoff 的归纳推理理论是一个数学证明，即如果一个宇宙是由算法生成的，对该宇宙的观察，编码为一个数据集，最好由该数据集的最小可执行档案来预测，通俗来说就是程序越短，越有可能是适用的。AIXI是一个通用人工智能的数学理论形式，结合了Solomonoff Induction和顺序决策理论，是一个强化学习代理，最大化从环境中获得的期望总回报。AIXI同时考虑了所有可计算的假设（或环境），每一步，会用每个程序计算下一步动作和评估奖励，承诺奖励会根据该程序构成真实环境的主观信念进行加权。这种信念是根据程序的长度计算得出的，根据Solomonoff Induction，越长的程序越不可能是构成真实环境的程序。AIXI会在所有这些程序的加权和中选择具有最高期望总奖励的动作来执行。     这几个概念感兴趣的可以自己再梳理下，课程里只是一语带过了，没有仔细讲。     撇开理论上的，我们来看一看非监督学习的一些应用： 首先非监督学习可以产生新的数据，也可以根据一些输入来合成数据，比如输入一句话，产生一幅图，非监督学习可以用于信息压缩，训练得越好的模型压缩效果越好，压缩得到的特征向量可以用于下游任务的模型开发和训练，下游任务如果是监督学习，通常可以大大减少需要的人工标注量。另外，非监督学习中成功的模型架构也可以用于监督学习或者强化学习，或者给开发者以启发。      这里列一些非监督学习目前的成果（这些可以去看视频课程，因为没有什么难懂的点，另外有些示例是视频，这里没有上传）：     1. 图像生成： 最早，可能是Hinton他们尝试用神经网络生成手写的数字。这个是2006年的事情了。 接下来VAE在2013年被提出来了，可以发现VAE产生的图片是模糊的，但是VAE比之后出来的GAN有自己的优点，这个之后说。    紧接着，2014年，GAN被发表，开始尝试生成人脸和风景，一开始效果并不好，但结果已经足够让人震撼。 GAN开始产生各式各样的变种，尝试去完成各种各样的任务，比如生成室内图。 更好的人脸生成，但很多生成的不是很好。 由低解析度的图片产生高解析度的图片，把模糊图片变清晰。 改造，把马变成斑马。 改变特征向量，产生不同的图。 清晰，高真人脸。     2.  语音生成     模型产生的语音和真实的人的语音是很难被区别开的。     3. 视频生成     4.  文本生成 Char-rnn是个非常优秀的文本生成模型，他不挑内容，公式也能生成，代码也能生成，英语可以，其他语言也可以，网络结构是基于LSTM，也非常简单。但这个模型现在被用得比较少了。现在大家的注意力都在Transformer上。基于Transformer，OpenAI开发了GPT-2，机器人可以产生有意义的文字，甚至可以说故事。 https://app. inferkit. com/demo有空可以去上面的网址逗逗机器人。     5.  压缩 以上是无损压缩，压缩按是否有信息损失可以分为无损压缩和有损压缩，无损压缩时，没有信息会在压缩过程中损失，所以比较的是各个方法可以把文件压缩到多小，有损压缩在信息压缩的过程中会造成信息损失，但通常可以把文件压很小，但也因此信息质量会变差，因此会比较压缩到同样大小文件时，不同压缩方法压缩出来的文件品质如何，比较的是压缩后的信息质量。 这是有损压缩。有损压缩更受欢迎。     6.  下游任务 – 情感计算 比如上面这段话，不同的句子有不同的感情色彩，前几句，情感是正面的，后面几句情感则是负面的。下游任务是说，这个功能是用训练好的语义模型进行再训练或者从训练好的模型中挖掘有用信息等方式来完成的。     7. 下游任务 – 语义理解 注意一下，9是Human Level，但这不是说现在机器的语义理解就已经强过人类了，只是在特定的数据集上或任务上如此表现。 ​	8. 下游任务 – 图像识别 继语义之后，非监督学习在图像领域也获得了巨大成功。这个可以看自监督学习（Self Supervised Learning）。 最后老师总结了一下： [1] Alex_Altair, An Intuitive Explanation of Solomonoff Induction, LessWrong, 2012 "
    }, {
    "id": 24,
    "url": "http://localhost:4000/%E4%BB%A5%E5%9B%BE%E7%89%87%E4%B8%BA%E7%9B%AE%E6%A0%87%E7%9A%84%E8%A7%86%E8%A7%89%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90%E4%B8%8A%E7%AF%87(rlkit%E5%AE%89%E8%A3%85%E5%8F%8A%E9%83%A8%E5%88%86%E4%BB%A3%E7%A0%81%E5%8F%8A%E5%87%BD%E6%95%B0%E7%9A%84%E8%A7%A3%E6%9E%90%E5%92%8C%E4%BD%BF%E7%94%A8)/",
    "title": "以图片为目标的视觉强化学习 -- 代码解析上篇(rlkit安装及部分代码及函数的解析和使用)",
    "body": "2022/02/08 -     之前有讲过一篇论文，今天来讲讲它的代码： 论文解析的链接是：以图片为目标的视觉强化学习     这篇研究以及这篇研究的后续研究，源代码都有公开，我们的目标是把这个库都解析一遍，感兴趣的话，关注吧。下面是相关源代码的链接： https://github. com/rail-berkeley/rlkit现在要说的这个工作我们称呼它RIG，RIG适用的rlkit版本和最新的两个工作适用的rlkit版本不一样，我们先看RIG适用的rlkit。     论文开发者直接开发了一个工具，里面有众多强化学习的算法，方便做相关研究的渣渣们，比如我，直接拿来用，但前提你得看得懂。那么看懂就交给我了。     我们先来安装和解析一下这个工具，首先看安装指引： cp rlkit/launchers/config_template. py rlkit/launchers/config. py    然后我们看看config. py里面都做了些什么： import osfrom os. path import joinimport rlkit# 设定项目目录rlkit_project_dir = join(os. path. dirname(rlkit. __file__), os. pardir)LOCAL_LOG_DIR = join(rlkit_project_dir, 'data')# 接下来设定大部分与远程服务器相关# 代码目录，可能不止一个，不止一个就添加在后面CODE_DIRS_TO_MOUNT = [  rlkit_project_dir,  # '/home/user/python/module/one', Add more paths as needed]# 工作需要用到mujoco模拟器，如果你mujoco安装的位置特殊，这里可能要改DIR_AND_MOUNT_POINT_MAPPINGS = [  dict(    local_dir=join(os. getenv('HOME'), '. mujoco/'),    mount_point='/root/. mujoco',  ),]# 跑程序的代码地址RUN_DOODAD_EXPERIMENT_SCRIPT_PATH = (  join(rlkit_project_dir, 'scripts', 'run_experiment_from_doodad. py')  # '/home/user/path/to/rlkit/scripts/run_experiment_from_doodad. py')# 接下来是AWS，SLURM，和GCP的一些设定，# 我目前都没用到，暂时不提，用到的小伙伴可以看一下关于config. py文件，不用doodad的话，要把下面两行改一下，不然之后跑程序会报错： # If not set, default will be chosen by doodad#AWS_S3_PATH = 's3://bucket/directory改为（其实就是把注释去掉，再补个冒号），我的环境里它不会被用到，只是调试起来方便一些： # If not set, default will be chosen by doodadAWS_S3_PATH = 's3://bucket/directory'接下来创建一个虚拟代码环境，他提供了三个虚拟环境配置文件：linux-cpu-env. yml，linux-gpu-env. yml，mac-env. yml。一般是linux-gpu： conda env create -f environment/linux-gpu-env. yml在跑之前我们看里面写了点啥，name是虚拟环境的名称，channels是虚拟环境安装软件的源，下面的dependencies是虚拟环境依赖的软件，这里就不列全了：    name: rlkitchannels:- kne # for pybox2d- pytorch- anaconda # for mkldependencies:- cython- ipython  # technically unnecessary我自己不习惯用别人写好的脚本建立环境，喜欢一边看代码一边解决依赖问题。 首先我们要解决最重要的一个依赖，MuJoCo，强化学习的环境模拟常用的软件，以前是个付费软件，现在免费了，可能是有大佬买了，现在似乎属于DeepMind，DeepMind属于Google： https://mujoco. org/代码里用的MuJoCo版本是1. 5，我喜欢用最新的，我们试试2. 1，不行再回来换1. 5，MuJoCo安装起来还是挺费劲的，遇到问题耐心些，多搜一搜： mkdir -p ~/. mujococd ~/. mujoco# 到mujoco官网-&gt;download页面-&gt;选2. 1. 0版本-&gt;下载linux版wget https://github. com/deepmind/mujoco/releases/download/2. 1. 0/mujoco210-linux-x86_64. tar. gztar -xvf mujoco210-linux-x86_64. tar. gzecho 'export LD_LIBRARY_PATH=$HOME/. mujoco/mujoco210/bin:$LD_LIBRARY_PATH' &gt;&gt; ~/. bashrcsource ~/. bashrc# 以上mujoco就下载安置好了# 接下来安装mujoco-pygit clone https://github. com/openai/mujoco-py. gitcd mujoco-pypip3 install -r requirements. txtpip3 install -r requirements. dev. txtpython3 setup. py install#----------------------------------------# 接下来是一些你可能import mujoco_py失败，甚至上面就没装成功，而需要进行的步骤# 按错误提示选择性执行sudo apt-get install gccsudo apt install libosmesa6-devsudo apt-get install python3-develsudo apt-get update -ysudo apt-get install -y patchelfsudo apt-get install libglew-devecho 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/nvidia' &gt;&gt; ~/. bashrcecho 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HOME/. local/bin' &gt;&gt; ~/. bashrcecho 'export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libGLEW. so' &gt;&gt; ~/. bashrcsource ~/. bashrc如果可以import mujoco_py执行成功，且可以跑一些示例代码，就表示安装成功了。     再回过头看rlkit，他除了提供虚拟环境的方案，还提供了Docker文件，Docker我不喜欢用，当然，主要是因为没下功夫，用不好，所以只能不喜欢，对这些比较擅长的可以试着用Docker。     rlkit调用GPU的方式： import rlkit. torch. pytorch_util as ptuptu. set_gpu_mode(True)     如果用doodad来执行方法，则直接用下面的方式调用gpu，然而我测试发现，不用doodad，下面这种方式也可以成功调用gpu： run_experiment(. . . , use_gpu=True)    他们会建议用doodad来执行python程序，这里我们更想偏向于原理性代码的讨论，doodad部分的代码就不涉及了。     接下来我们看RIG的代码，在examples/rig/目录下，有两个目录，四个文件，我们一个一个开始，首先看examples/rig/pointmass/rig. py： from multiworld. envs. pygame. point2d import Point2DWallEnvfrom rlkit. launchers. launcher_util import run_experimentfrom rlkit. launchers. rig_experiments import grill_her_td3_full_experiment要安装一下multiworld，这个工具也是该团队开发的： git clone https://github. com/vitchyr/multiworld. gitcd multiworldpython setup. py installpip install gympip install pygamepip install boto3pip install gtimerpip install sk-videopip install gitpython关于安装rlkit，rlkit依赖版本较低的python和tensorflow，再加上跑不同程序要用到的rlkit版本还不太一样，我们反正要解析源代码，这里就不安装rlkit了，直接把rlkit的源码包放在主程序的目录下。如果想安装rlkit，可以按照他们给的方法操作，之后直接pip install rlkit就行。亲测不安装是可以的。     接着看main函数，先定义了一个字典，里面都是参数，字典里还嵌套字典，层层叠叠，难怪AI工程师会被调侃调参大师，这里暂时不一一看，之后用到了会提：   variant = dict(        imsize=84,        . . . . . . .         algorithm='RIG',    )    配置完参数之后，就是跑训练代码了：   run_experiment(        grill_her_td3_full_experiment, #rig算法实验的函数，以variant为参数        exp_prefix='rlkit-pointmass-rig-example', #实验名称        # 实验方式：默认是local，我们这里没装doodad。        # 单机用here_no_doodad可以跑，也可以调用GPU        # 其他方式暂时不研究了，有兴趣的可以研究        # “local”，        # “docker_local”，        # “ec2”，        # 'here_no_doodad'，     mode='here_no_doodad',         variant=variant, # 参数        use_gpu=True,  # 要用GPU跑就打开  )    在这样的配置下，我们看代码是怎么跑的（会略过不太重要的代码），下面是run_experiment里比较重要的代码：   run_experiment_kwargs = dict(        # 实验名称，默认会带上时间    exp_prefix=exp_prefix,    # 参数    variant=variant,    # 实验ID    exp_id=exp_id,        # 随机种子，不设定就随机产生    seed=seed,    use_gpu=use_gpu,        # 默认是‘last’,作用之后说    snapshot_mode=snapshot_mode,        # 默认是1，作用之后说    snapshot_gap=snapshot_gap,        # 没装git就是None,暂时就None吧    git_infos=git_infos,        #这里应该是rig. py     script_name=main. __file__,  )  if mode == 'here_no_doodad':    run_experiment_kwargs['base_log_dir'] = base_log_dir    return run_experiment_here(      method_call,      **run_experiment_kwargs        )接下来是run_experiment_here函数，依然是只看重要一点的代码：   # 这个函数里只有一行代码：  # logger. reset()  # logger也是他们自己开发的，reset就是回归初始状态  reset_execution_environment()    # 这里做的事情包括：创建log目录，将variant参数记录下来，    # 以及添加各式各样的路径和参数  actual_log_dir = setup_logger(    exp_prefix=exp_prefix,    variant=variant,    exp_id=exp_id,    seed=seed,    snapshot_mode=snapshot_mode,    snapshot_gap=snapshot_gap,    base_log_dir=base_log_dir,    log_dir=log_dir,    git_infos=git_infos,    script_name=script_name,    **setup_logger_kwargs  )    # 启用seed  set_seed(seed)    # 启用GPU，默认用id为0的GPU    # 如果想选择用其他gpu，可以set_gpu_mode(use_gpu,gpu_id)    set_gpu_mode(use_gpu)      run_experiment_here_kwargs = dict(    variant=variant,    exp_id=exp_id,    seed=seed,    use_gpu=use_gpu,    exp_prefix=exp_prefix,    snapshot_mode=snapshot_mode,    snapshot_gap=snapshot_gap,    git_infos=git_infos,    script_name=script_name,    base_log_dir=base_log_dir,    **setup_logger_kwargs  )    # 把参数信息存成actual_log_dir下面的experiment. pkl文件  save_experiment_data(    dict(      run_experiment_here_kwargs=run_experiment_here_kwargs    ),    actual_log_dir  )    # 好了，终于到我们的算法函数了。    # 其实是这样的 grill_her_td3_full_experiment(variant)    return experiment_function(variant)接下来到这里：‍ def grill_her_td3_full_experiment(variant):    # 看名字就知道功能了，预处理  full_experiment_variant_preprocess(variant)    # 训练vae，一边训练还会一边自己调参哦  train_vae_and_update_variant(variant)  # 这个看了再说。    grill_her_td3_experiment(variant['grill_variant'])这篇先到这里吧，我累了。下篇会讲具体模型算法还有训练测试代码，希望一个下篇就解决，不需要再分个中下。 感兴趣的小伙伴们就关注吧。觉得有用右下角帮忙点个赞哦。 "
    }, {
    "id": 25,
    "url": "http://localhost:4000/Facebook-DeiT(Data-efficient-Image-Transformers)-%E8%A7%A3%E6%9E%90/",
    "title": "Facebook DeiT(Data-efficient Image Transformers) 解析",
    "body": "2021/11/23 -     今天看Facebook AI的DeiT。比起Moco v3训练出来的模型，DeiT胜在模型小，训练和推理都更加迅速。且精准度还有了很大提高。     迅速到什么程度呢？用一个8-GPU服务器，训练3天就可以（其中预训练占用了53小时，fine-tuning占20个小时）。     看下图的效能比对，可以发现，DeiT确实是强悍的。     接下来我们看DeiT的具体原理，首先，它的主干模型还是ViT，关于ViT可以看这篇：ViT (Vision Transformer)原理及代码解析，这里就不赘述了。     它可以训练的这么快的原因是用了蒸馏技术（distillation）。 那什么是蒸馏技术呢？就是把一个模型的知识往另外一个模型迁移的过程。被迁移知识的那个模型，我们叫他teacher，是训练好的。从别的模型学到知识的模型，我们叫他student。通常是把大模型的知识往小模型上迁移。以期在不过分损失精准度的前提下，使推理速度大大提高。迁移的方式通常是让小模型和大模型有一样的输出。接下来看两种蒸馏方式： 软蒸馏（soft distillation）：     Zt是teacher模型的输出，Zs是student模型的输出。ψ表示的是softmax，KL是KL散度（Kullback-Leibler），又叫相对熵，Lce是交叉熵（cross-entropy）。y是真实标签（ground truth label）。     这里参考Cross Entropy Loss简单说一下KL散度和交叉熵的区别，这里就不上公式了，用大白话说，如果觉得有公式更容易理解，可以看参考文章中的公式。     首先说一下信息量的概念，信息量指的是一个事件发生含的信息量，发生的概率越小，含的信息量就越大，比如太阳有天从西边升起来了，那么这个事件含的信息量就超级大了，全世界都得炸。而计算机里的信息量的大小指的是，描述一个事件发生所需要的位元（bits）。这里的描述和我们通常的语言描述是不一样的。而信息熵是信息量的期望值，还是太阳升起这个问题，它有可能从东边升起，也有可能从西边，南边，北边升起，虽然概率无限趋近于0，但是墨菲定律告诉我们，只要有概率，就一定会发生。那么太阳升起的方向这个事件有一个信息量的期望值，我们怎么理解期望值呢？太阳升起是从东南西北哪个方向呢，每个方向都会由一个概率，假设这件事情遵循特定的概率分布，如果这件事情发生无数次，那么平均每次我们需要用多少个位元来描述这件事情呢。平均每次，事件携带的信息量的大小，就是信息熵。     接下来我们解释相对熵，也就是KL散度。比如太阳升起是从东南西北哪个方向，假设这件事情遵循特定概率分布p，假设我们不知道这个p，现在我们自己估一个概率分布q，是根据模型或者自己的认知设定的，那么用真实的概率分布会有一个信息熵，用我们估计的概率分布也会有一个信息熵，用估计的信息熵减去真实的信息熵，就是相对熵。相对熵计算的是，如果用我们估计的这个概率分布来替代真实的概率分布，如果这个事件会发生无数次，那么平均传输和描述这个事件需要多出多少位元。     而交叉熵就是，如果用这个估计的分布替代真实的分布，如果这个事件发生无数次，平均需要多少位元来描述和传输这个事件。     这两个都可以用来比较两个分布的差异，差异越大，相对熵或者交叉熵就越大，差异越小，相对熵或者交叉熵就越小。     到这里我们来理解一下，为什么上面的公式，前面用的是交叉熵，后面用的是相对熵，因为前面y是固定的，不管我们怎么对样本进行变化，y都是不变的，因此由真实概率分布所得的信息熵是固定的，没有必要再去减一个固定值。但是后面的ψ(Zt/τ)则不一样了，如果我们用样本增强，或者我们不止有一个teacher，改变τ的值，那么ψ(Zt/τ)的值就会发生变化，信息熵也会发生变化，出于这种考虑，用相对熵保留差异部分可能可以更好的排除一些其他因素对loss造成的干扰。     当然，我这种理解不一定对。 硬蒸馏 (hard-label distillation)：      与软蒸馏不同的地方是，这里的yt是teacher的hard decision，yt = argmaxcZt(c)，也就是说yt不再是一个概率分布，而是一个根据最高概率做出的一个决策结果（但其实也可以看成一个概率分布，这里只是相对而言）。     与软蒸馏不同的，还有损失函数的选择。在硬蒸馏里，yt和y的关系是对等的，对结果起到的作用是对等的。大家会更偏好硬蒸馏，硬蒸馏参数更少，容易理解。这里的yt也会因为数据增强而产生不同结果，不是一定的。     接下来看，DeiT是怎么实现软硬蒸馏的。     在ViT的结构中，增加了一个和class token一样功能的distillation token。具体的代码实现可以看ViT (Vision Transformer)原理及代码解析，在一般的硬蒸馏里，用来和y计算差异的Zs和用来和yt计算差异的Zs是一致的，但是在DeiT里，分开了。 # https://github. com/facebookresearch/deit/blob/main/models. py        if self. training:            # x是class token, x_dist是distillation token       return x, x_dist    else:      # during inference, return the average of both classifier predictions            return (x + x_dist) / 2    根据DeiT模型里的代码，训练的时候返回的是cls_token，和distillation token，而在推理的时候，返回的是cls_token和distillation_token的均值。也就是说DeiT中的class token和distillation_token被认定有等价的推理价值。取均值会让推理更准确。 # https://github. com/facebookresearch/deit/blob/main/losses. py    def forward(self, inputs, outputs, labels):           Args:      inputs: The original inputs that are feed to the teacher model      outputs: the outputs of the model to be trained. It is expected to be        either a Tensor, or a Tuple[Tensor, Tensor], with the original output        in the first position and the distillation predictions as the second output      labels: the labels for the base criterion               outputs_kd = None    if not isinstance(outputs, torch. Tensor):      # assume that the model outputs a tuple of [outputs, outputs_kd]            # class token，distillation token      outputs, outputs_kd = outputs        # class token和groud-truth labels产生base loss    base_loss = self. base_criterion(outputs, labels)    if self. distillation_type == 'none':      return base_loss    if outputs_kd is None:      raise ValueError( When knowledge distillation is enabled, the model is                  expected to return a Tuple[Tensor, Tensor] with the output of the                  class_token and the dist_token )    # don't backprop throught the teacher    with torch. no_grad():      teacher_outputs = self. teacher_model(inputs)    if self. distillation_type == 'soft':      T = self. tau      # taken from https://github. com/peterliht/knowledge-distillation-pytorch/blob/master/model/net. py#L100      # with slight modifications      distillation_loss = F. kl_div(        F. log_softmax(outputs_kd / T, dim=1),        #We provide the teacher's targets in log probability because we use log_target=True         #(as recommended in pytorch https://github. com/pytorch/pytorch/blob/9324181d0ac7b4f7949a574dbc3e8be30abe7041/torch/nn/functional. py#L2719)        #but it is possible to give just the probabilities and set log_target=False. In our experiments we tried both.         F. log_softmax(teacher_outputs / T, dim=1),        reduction='sum',        log_target=True      ) * (T * T) / outputs_kd. numel()      #We divide by outputs_kd. numel() to have the legacy PyTorch behavior.       #But we also experiments output_kd. size(0)       #see issue 61(https://github. com/facebookresearch/deit/issues/61) for more details    elif self. distillation_type == 'hard':      distillation_loss = F. cross_entropy(outputs_kd, teacher_outputs. argmax(dim=1))    loss = base_loss * (1 - self. alpha) + distillation_loss * self. alpha    return loss    看DeiT中的代码，软蒸馏和硬蒸馏都被定义了，inputs是样本，由teacher_model产生teacher_outputs，outputs中包含了class token和distillation token，class token和ground-truth label产生base_loss，而distillation token和teacher model产生的结果产生distillation_loss。     DeiT的原理大概说清楚了，实验的话，实在是懒，作者做了大量实验，如果想用DeiT，还是建议仔细看实验的。之后可能会写一篇，怎么用DeiT做分类和Transfer的文章。期待的话就关注吧。 "
    }, {
    "id": 26,
    "url": "http://localhost:4000/Facebook%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0Visual-Transformers(ViT)%E7%9A%84%E8%AE%AD%E7%BB%83%E7%BB%8F%E9%AA%8C(Moco-v3)-Moco%E4%B8%AD%E7%9A%84ViT/",
    "title": "Facebook自监督学习Visual Transformers(ViT)的训练经验(Moco v3) -- Moco中的ViT",
    "body": "2021/11/07 -     系列首篇：自监督学习Visual Transformers(ViT)的训练经验(Moco v3) – 论文解析     系列上篇：Facebook自监督学习Visual Transformers(ViT)的训练经验(Moco v3) – 训练代码解析     Moco v3 代码链接： https://github. com/facebookresearch/moco-v3    看vits. py： from timm. models. vision_transformer import VisionTransformer, _cfgfrom timm. models. layers. helpers import to_2tuplefrom timm. models. layers import PatchEmbed# 释出的代码定义了四种不同的vit模型给moco v3__all__ = [   'vit_small',    'vit_base',   'vit_conv_small',    'vit_conv_base',]    vits. py定义了四种ViT，分别是vit_small，vit_base，vit_conv_small，vit_conv_base。继承的都是timm里的VisionTransformer。     要先了解moco v3中的vit，首先我们得了解vit的基本原理，可以看ViT (Vision Transformer)原理及代码解析     在ViT (Vision Transformer)原理及代码解析这篇我们说过，timm里的Position Embedding初始是随机数，到Moco v3中，又把它改成了2d sin cos。 def build_2d_sincos_position_embedding(self, temperature=10000. ):    # grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])    # num_patches = grid_size[0] * grid_size[1]    # grid_size即横纵坐标上patch数量    h, w = self. patch_embed. grid_size    # grid_w = tensor([0. , 1. , 2. , . . . , w-1])        grid_w = torch. arange(w, dtype=torch. float32)    # grid_h = tensor([0. , 1. , 2. , . . . , h-1])    grid_h = torch. arange(h, dtype=torch. float32)    # h*w    # grid_w = tensor([[0. , 0. , . . . , 0. ], [1. , 1. , . . . , 1. ], . . . , [w-1, w-1, . . . , w-1]])    # grid_h = tensor([[0. , . . . , h-1], [0. , . . . , h-1], . . . , [0. , . . . , h-1]])    grid_w, grid_h = torch. meshgrid(grid_w, grid_h)    assert self. embed_dim % 4 == 0, 'Embed dimension must be divisible by 4 for 2D sin-cos position embedding'    pos_dim = self. embed_dim // 4    # pos_dim    omega = torch. arange(pos_dim, dtype=torch. float32) / pos_dim    omega = 1.  / (temperature**omega)    # 外积 12, 16 -&gt; (12, 16)    out_w = torch. einsum('m,d-&gt;md', [grid_w. flatten(), omega])    out_h = torch. einsum('m,d-&gt;md', [grid_h. flatten(), omega])    # (1, 12, embed_dim)    pos_emb = torch. cat([torch. sin(out_w), torch. cos(out_w), torch. sin(out_h), torch. cos(out_h)], dim=1)[None, :, :]    assert self. num_tokens == 1, 'Assuming one and only one token, [cls]'    pe_token = torch. zeros([1, 1, self. embed_dim], dtype=torch. float32)    self. pos_embed = nn. Parameter(torch. cat([pe_token, pos_emb], dim=1))    self. pos_embed. requires_grad = False Moco v3 中 position embedding 的图示效果见上图，第一行是cls token的position embedding，是全0，第一个patch的position embedding，则是分四块0，1交替，和ViT (Vision Transformer)原理及代码解析中分两块和单双数0，1交替都不一样。这边为什么要这么设计position embedding呢？我的理解，如果是做语义处理，patch和patch之间只有一个距离，patch的坐标是一个一维向量，因为句子是词组成的一个序列。而做图像处理的时候，图像是一个二维信息块，切割成patch，每个patch的坐标也是二维的，而用于语义处理的position embedding并不能很好地表达这个二维信息。Moco v3中的position embedding代码则能很好的表达这个二维的位置信息，从上图不难看出，embedding的前半部分要表达的是行信息，而embedding的后半部分要表达的是列信息。我们再看一个（8，8）结构的patch的position embedding： class VisionTransformerMoCo(VisionTransformer):  def __init__(self, stop_grad_conv1=False, **kwargs):    super(). __init__(**kwargs)    # Use fixed 2D sin-cos position embedding        # 初始化position embedding        self. build_2d_sincos_position_embedding()            # weight initialization    for name, m in self. named_modules():            # 挑拣出所有的全连接层      if isinstance(m, nn. Linear):                # 挑拣出qkv字符串在名字里的全连接层                # blocks. i. attn. qkv,其中i是block的id，从0-block总数        if 'qkv' in name:          # treat the weights of Q, K, V separately          val = math. sqrt(6. / float(m. weight. shape[0] // 3 + m. weight. shape[1]))          # 使值服从均匀分布U(a,b)          nn. init. uniform_(m. weight, -val, val)                # blocks. i. attn. qkv, blocks. i. mlp. fc1, blocks. i. mlp. fc2                # head                else:                  # xavier初始化方法中服从均匀分布U(−a,a)          nn. init. xavier_uniform_(m. weight)        nn. init. zeros_(m. bias)    nn. init. normal_(self. cls_token, std=1e-6)    if isinstance(self. patch_embed, PatchEmbed):      # xavier_uniform initialization      val = math. sqrt(6. / float(3 * reduce(mul, self. patch_embed. patch_size, 1) + self. embed_dim))      nn. init. uniform_(self. patch_embed. proj. weight, -val, val)      nn. init. zeros_(self. patch_embed. proj. bias)            # Moco v3的核心，即patch embedding那层不参与收敛      if stop_grad_conv1:        self. patch_embed. proj. weight. requires_grad = False        self. patch_embed. proj. bias. requires_grad = False这一段主要是Moco v3调整模型初始化的一些方式，以及允许将产生patch embedding的本来可以参与反向传播调整参数的proj参数固定，不参与整个反向传播，不调整参数，这是Moco v3的核心。     这里有些问题可以细究一下，比如为什么要这么调整这些初始化方式，其中包括，position embedding的方式，还有各个全连接层的调整方式。     这里还引入了patch embedding的改进方式ConvStem，由于代码只有proj相关的部分不同，因此这里也只贴proj这部分出来： class ConvStem(nn. Module):        ConvStem, from Early Convolutions Help Transformers See Better, Tete et al. https://arxiv. org/abs/2106. 14881       def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, norm_layer=None, flatten=True):        super(). __init__()    assert patch_size == 16, 'ConvStem only supports patch size of 16'        assert embed_dim % 8 == 0, 'Embed dimension must be divisible by 8 for ConvStem'    # build stem, similar to the design in https://arxiv. org/abs/2106. 14881    stem = []    input_dim, output_dim = 3, embed_dim // 8    for l in range(4):            # 卷积：输入通道，输出通道，卷积核大小，步长            # (B, 3, H, W) -&gt; (B, embed_dim // 8, (H+1)//2, (W+1)//2)            # (B, embed_dim // 8, H', W')-&gt;  (B, embed_dim // 4, (H+1)//2, (W+1)//2)            # (B, embed_dim // 4, H'', W'')-&gt;  (B, embed_dim // 2, (H+1)//2, (W+1)//2)            # (B, embed_dim // 2, H''', W''') -&gt; (B, embed_dim, (H+1)//2, (W+1)//2)      stem. append(nn. Conv2d(input_dim, output_dim, kernel_size=3, stride=2, padding=1, bias=False))      stem. append(nn. BatchNorm2d(output_dim))      stem. append(nn. ReLU(inplace=True))      input_dim = output_dim      output_dim *= 2        # (B, embed_dim, H'''', W'''') -&gt; (B, embed_dim, H'''', W'''')    stem. append(nn. Conv2d(input_dim, embed_dim, kernel_size=1))        # proj总共有五层卷积        self. proj = nn. Sequential(*stem)由此可见在做patch embedding的时候，ConvStem将原本的全连接层替换成了四层卷积层（最后一层不太能算）。有机会我们可以详细解说一下ConvStem，对了ConvStem同样也是Facebook的成果。 最后我们来看一下Moco v3定义的四种vit分别长什么样子，以及有哪些不同： def vit_small(**kwargs):  model = VisionTransformerMoCo(    patch_size=16, embed_dim=384, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,    norm_layer=partial(nn. LayerNorm, eps=1e-6), **kwargs)  model. default_cfg = _cfg()  return modeldef vit_base(**kwargs):  model = VisionTransformerMoCo(    patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,    norm_layer=partial(nn. LayerNorm, eps=1e-6), **kwargs)  model. default_cfg = _cfg()  return model这两个模型只有embed_dim的区别，一个小一个大。 def vit_conv_small(**kwargs):  # minus one ViT block  model = VisionTransformerMoCo(    patch_size=16, embed_dim=384, depth=11, num_heads=12, mlp_ratio=4, qkv_bias=True,    norm_layer=partial(nn. LayerNorm, eps=1e-6), embed_layer=ConvStem, **kwargs)  model. default_cfg = _cfg()  return modeldef vit_conv_base(**kwargs):  # minus one ViT block  model = VisionTransformerMoCo(    patch_size=16, embed_dim=768, depth=11, num_heads=12, mlp_ratio=4, qkv_bias=True,    norm_layer=partial(nn. LayerNorm, eps=1e-6), embed_layer=ConvStem, **kwargs)  model. default_cfg = _cfg()  return model这里的主要区别是embed layer的区别，有没有用ConvStem，另外depth也浅了一层。 Moco v3 paper中的vit更丰富一点，也会有各种不同参数vit的训练效果，想要详细了解的可以去看一下。 参考： [1] 我是啤酒，ViT (Vision Transformer)原理及代码解析，Chaos万有引力，2021  [2] Jay Alammar, The Illustrated Transformer, jalammar. github. io, 2018 "
    }, {
    "id": 27,
    "url": "http://localhost:4000/ViT-(Vision-Transformer)%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/",
    "title": "ViT (Vision Transformer)原理及代码解析",
    "body": "2021/10/31 -     今天我们来详细了解一下Vision Transformer。基于timm的代码。 1. Patch Embedding Transformer原本是用来做NLP的工作的，所以ViT的首要任务是将图转换成词的结构，这里采取的方法是如上图左下角所示，将图片分割成小块，每个小块就相当于句子里的一个词。这里把每个小块称作Patch，而Patch Embedding就是把每个Patch再经过一个全连接网络压缩成一定维度的向量。 这里是VisionTransformer源代码中关于Patch Embedding的部分： # 默认img_size=224, patch_size=16，in_chans=3，embed_dim=768，self. patch_embed = embed_layer(    img_size=img_size, patch_size=patch_size,   in_chans=in_chans, embed_dim=embed_dim)而embed_layer其实是PatchEmbed： class PatchEmbed(nn. Module):        2D Image to Patch Embedding         def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, norm_layer=None, flatten=True):    super(). __init__()        # img_size = (img_size, img_size)        img_size = to_2tuple(img_size)    patch_size = to_2tuple(patch_size)    self. img_size = img_size    self. patch_size = patch_size    self. grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])    self. num_patches = self. grid_size[0] * self. grid_size[1]    self. flatten = flatten        # 输入通道，输出通道，卷积核大小，步长        # C*H*W-&gt;embed_dim*grid_size*grid_size    self. proj = nn. Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)    self. norm = norm_layer(embed_dim) if norm_layer else nn. Identity()  def forward(self, x):    B, C, H, W = x. shape    assert H == self. img_size[0] and W == self. img_size[1], \      f Input image size ({H}*{W}) doesn't match model ({self. img_size[0]}*{self. img_size[1]}).      x = self. proj(x)    if self. flatten:      x = x. flatten(2). transpose(1, 2) # BCHW -&gt; BNC    x = self. norm(x)    return xproj虽然用的是卷积的写法，但其实是将每个patch接入了同样的全连接网络，将每个patch转换成了一个向量。x的维度是（B，C，H，W）其中B是batch size，C通常是三通道，H和W分别是图片的高和宽，而输出则是（B，N，E），B依然是batch size，N则是每张图被切割成了patch之后，patch的数量，E是embed_size，每个patch会通过一个全连接网络转换成一个向量，E是这个向量的长度，根据卷积的原理，也可以理解为每个patch的特征数量。 2.  Positional Encoding     把图片分割成了patch，然后把每个patch转换成了embedding，接下来就是在embedding中加入位置信息。产生位置信息的方式主要分两大类，一类是直接通过固定算法产生，一种是训练获得。但加位置信息的方式还是比较统一且粗暴的。 产生一个位置向量，长度和patch embedding一致，然后直接相加。那么这个位置向量大概长什么样呢？ 比如patch embedding长度为4，那么位置向量长度也为4，每个位置有一个在[-1,1]的值。 假设你现在一张图切成了20个patch，embedding的长度是512，那么位置向量可以是上面这样的（tensor2tensor中的get_timing_signal_1d函数），每一行代表一个位置向量，第一行是位置0的位置向量，第二行是位置1的位置向量。 位置向量也可以是下面这样的（参考[1], [4]）： 公式如下： pos是单词在句子中的位置，或者patch在图中的位置，而i对应的是embedding的位置，dmodel对应的是patch embedding的长度，。这里说一下为什么要加这个位置编码，以及加上以后会有什么效果，我们观察上两幅图，可以发现，位置编码是随位置而改变的，位置差别越大的，那么向量差别也越大。在NLP课程里说过，把一个词转换成向量，就好像把一个词映射到了一个高维空间的位置，意思相近的词会在高维空间内比较靠近，而加上位置向量，会让位置相近的词更靠近，位置远的词离得更远。再来，为什么用cos，sin这种方式，作者的解释是，使用sin和cos编码可以得到词语之间的相对位置。  这儿我是这么理解的，根据这两个公式，当我们知道了sin(pos+k)，cos(pos+k)，再知道了sin(pos)和cos(pos)，那么k的值我们是可以算出来的，而且用加减乘除就可以算出来。因此这样的编码方式不但能表达单词的位置，还能表达单词与单词之间的相对位置。 再看timm中对positional encoding的实现： 可以发现timm中的positional encoding是随机数，也就是说没有做positional encoding，可能只是给你留了个位置，而默认的值服从某种正太分布，且限于很小的数值区间，这里就不上代码和详细解释了。至于这里为什么是随机数。一个是保留位置，便你扩展，二是本来positional encoding就有两类方式可以实现，一种是用一定的算法生成，另外一种就是通过训练调整获得。timm应该是默认是通过训练来调整获得。    3. Self-Attention     接下来看ViT中的Attention，这和Transformer中的self-attention应该是一致的，我们先看看参考[1]是如何介绍self-attention的。参考[1]举了一个语义处理的例子，有一句话是这样的     “The animal didn’t cross the street because it was too tired. ” 我们人很容易理解，后面的it是指animal，但是要怎么让机器能够把it和animal关联起来呢？ Self-attention就是在这种需求下产生的，如上图所示，我们应当有一个结构能够表达每个单词和其他每个单词的关系。那这里我们处理的是图像问题，Self-attention的存在就可以理解成，我们应当有一个结构能够表达每个patch和其他patch的关系。之前说过，图像中的patch和语义处理中的词可以同等来看。     我们再来看具体怎么实现的：     1. 基于输入向量创建三个向量：query向量，key向量和value向量。     2. 由query向量和key向量产生自注意力。     Thinking和Machine可以理解为图片被切分的两个patch，现在计算Thinking的自注意力，通过q乘k，除以一定系数（scaled dot-product attention，点积得到的结果值通常很大，使得softmax结果不能很好地表达attention值。这时候除以一个缩放因子，可以一定程度上减缓这种情况。），通过softmax之后会得到一个关于Thinking的注意力向量，比如这个例子是[0. 88, 0. 12]，这个向量的意思是，要解释Thinking这个词在这个句子中的意思，应当取0. 88份Thinking原本的意思，再取0. 12份Machine原本的意思，就是Thinking在这个句子中的意思。最后图中Sum之后的结果所表达的就是每个单词在这个句子当中的意思。整个过程可以用下面这张图表达： 4. Multi-Head Attention timm中attention是在self-attention基础上改进的multi-head attention，也就是在产生q，k，v的时候，对q，k，v进行了切分，分别分成了num_heads份，对每一份分别进行self-attention的操作，最后再拼接起来，这样在一定程度上进行了参数隔离，至于这样为什么效果会更好，我觉得应该是这样操作会让关联的特征集中在一起，更容易训练。     class Attention(nn. Module):  def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0. , proj_drop=0. ):    super(). __init__()    self. num_heads = num_heads        # q,k,v向量长度    head_dim = dim // num_heads        self. scale = head_dim ** -0. 5    self. qkv = nn. Linear(dim, dim * 3, bias=qkv_bias)    self. attn_drop = nn. Dropout(attn_drop)    self. proj = nn. Linear(dim, dim)    self. proj_drop = nn. Dropout(proj_drop)  def forward(self, x):        # 这里C对应上面的E，向量的长度    B, N, C = x. shape        # (B, N, C) -&gt; (3，B，num_heads, N, C//num_heads), //是向下取整的意思。    qkv = self. qkv(x). reshape(B, N, 3, self. num_heads, C // self. num_heads). permute(2, 0, 3, 1, 4)        # 将qkv在0维度上切成三个数据块，q,k,v:(B，num_heads, N, C//num_heads)        # 这里的效果是从每个向量产生三个向量，分别是query，key和value    q, k, v = qkv. unbind(0)  # make torchscript happy (cannot use tensor as tuple)        # @矩阵相乘获得score (B,num_heads,N,N)    attn = (q @ k. transpose(-2, -1)) * self. scale    attn = attn. softmax(dim=-1)    attn = self. attn_drop(attn)        # (B,num_heads,N,N)@(B,num_heads,N,C//num_heads)-&gt;(B,num_heads,N,C//num_heads)        # (B,num_heads,N,C//num_heads) --&gt;(B,N,num_heads,C//num_heads)        # (B,N,num_heads,C//num_heads) -&gt; (B, N, C)    x = (attn @ v). transpose(1, 2). reshape(B, N, C)        # (B, N, C) -&gt; (B, N, C)    x = self. proj(x)    x = self. proj_drop(x)    return xmulti-head attention的总示意图如下： 5. Layer Normalization     Layer normalization对应的一个概念是我们熟悉的Batch Normalization，这两个根本的不同在于，Layer normalization是对每个样本的所有特征进行归一化，而Batch Normalization是对每个通道的所有样本进行归一化。     为了便于理解，这里贴一下官网给LN的示例代码： # NLP Examplebatch, sentence_length, embedding_dim = 20, 5, 10embedding = torch. randn(batch, sentence_length, embedding_dim)# 指定归一化的维度layer_norm = nn. LayerNorm(embedding_dim)# 进行归一化layer_norm(embedding)# Image ExampleN, C, H, W = 20, 5, 10, 10input = torch. randn(N, C, H, W)# Normalize over the last three dimensions (i. e.  the channel and spatial dimensions)# as shown in the image belowlayer_norm = nn. LayerNorm([C, H, W])output = layer_norm(input) 在ViT中，虽然LN处理的是图片数据，但在进行LN之前，图片已经被切割成了Patch，而每个Patch表示的是一个词，因此是在用语义的逻辑在解决视觉问题，因此在ViT中，LN也是按语义的逻辑在用的。关于这个概念的详细细节，可以参考[3]和[2]。 6. Drop Path     Dropout是最早用于解决网络过拟合的方法，是所有drop类方法的始祖。方法示意图如下： 在向前传播的时候，让神经元以一定概率停止工作。这样可以使模型泛化能力变强，因为神经元会以一定概率失效，这样的机制会使结果不会过分依赖于个别神经元。训练阶段，以keep_prob概率使神经元失效，而推理的时候，会保留所有神经元的有效性，因此，训练时候加了dropout的神经元推理出来的结果要乘以keep_prob。     接下来以dropout的思路来理解drop path，drop path没找到示意图，那直接看timm上的代码： def drop_path(x, drop_prob: float = 0. , training: bool = False):  if drop_prob == 0. or not training:    return x    # drop_prob是进行droppath的概率  keep_prob = 1 - drop_prob  # work with diff dim tensors, not just 2D ConvNets    # 在ViT中，shape是(B,1,1),B是batch size  shape = (x. shape[0],) + (1,) * (x. ndim - 1)    # 按shape,产生0-1之间的随机向量,并加上keep_prob    random_tensor = keep_prob + torch. rand(shape, dtype=x. dtype, device=x. device)    # 向下取整，二值化，这样random_tensor里1出现的概率的期望就是keep_prob  random_tensor. floor_() # binarize    # 将一定图层变为0  output = x. div(keep_prob) * random_tensor  return output由代码可以看出，drop path是在batch那个维度，随机将一些图层直接变成0，以加快运算速度。 7. Encoder Transformer的架构图：     Transformer是由一堆encoder和decoder形成的，那encoder一般的架构图如下： Encoder在ViT中的实现细节如下面代码所示（layer normalization -&gt; multi-head attention -&gt; drop path -&gt; layer normalization -&gt; mlp -&gt; drop path），换了个名字，叫block了： class Block(nn. Module):  def __init__(self, dim, num_heads, mlp_ratio=4. , qkv_bias=False, drop=0. , attn_drop=0. ,         drop_path=0. , act_layer=nn. GELU, norm_layer=nn. LayerNorm):        super(). __init__()        # 将每个样本的每个通道的特征向量做归一化        # 也就是说每个特征向量是独立做归一化的        # 我们这里虽然是图片数据，但图片被切割成了patch，用的是语义的逻辑    self. norm1 = norm_layer(dim)    self. attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here    self. drop_path = DropPath(drop_path) if drop_path &gt; 0. else nn. Identity()    self. norm2 = norm_layer(dim)    mlp_hidden_dim = int(dim * mlp_ratio)        # 全连接，激励，drop，全连接，drop,若out_features没填，那么输出维度不变。    self. mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)    def forward(self, x):        # 最后一维归一化，multi-head attention, drop_path        # (B, N, C) -&gt; (B, N, C)    x = x + self. drop_path(self. attn(self. norm1(x)))        # (B, N, C) -&gt; (B, N, C)    x = x + self. drop_path(self. mlp(self. norm2(x)))    return x在ViT中这样的block会有好几层，形成blocks： # stochastic depth decay ruledpr = [x. item() for x in torch. linspace(0, drop_path_rate, depth)]self. blocks = nn. Sequential(*[    Block(        dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, drop=drop_rate,        attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, act_layer=act_layer)    for i in range(depth)])如果drop_path_rate大于0，每一层block的drop_path的会线性增加。depth是一个blocks里block的数量。也可以理解为blocks这个网络块的深度。 8.  Forward Features Patch embedding -&gt; 加cls -&gt; 加pos embedding -&gt; 用blocks进行encoding -&gt; layer normalization -&gt; 输出图的embedding def forward_features(self, x):    # x由（B，C，H，W）-&gt;（B，N，E）    x = self. patch_embed(x)    # stole cls_tokens impl from Phil Wang, thanks    # cls_token由(1, 1, 768)-&gt;(B, 1, 768), B是batch_size    cls_token = self. cls_token. expand(x. shape[0], -1, -1)    # dist_token是None,DeiT models才会用到dist_token。    if self. dist_token is None:        # x由(B, N, E)-&gt;(B, 1+N, E)        x = torch. cat((cls_token, x), dim=1)    else:        # x由(B, N, E)-&gt;(B, 2+N, E)        x = torch. cat((cls_token, self. dist_token. expand(x. shape[0], -1, -1), x), dim=1)    # +pos_embed:(1, 1+N, E)，再加一个dropout层    x = self. pos_drop(x + self. pos_embed)    x = self. blocks(x)    # nn. LayerNorm    x = self. norm(x)    if self. dist_token is None:        # 不是DeiT，输出就是x[:,0]，(B, 1, 768)，即cls_token        return self. pre_logits(x[:, 0])    else:        # 是DeiT，输出就是cls_token和dist_token        return x[:, 0], x[:, 1]这里在patch 那个维度加入了一个cls_token，可以这样理解这个存在，其他的embedding表达的都是不同的patch的特征，而cls_token是要综合所有patch的信息，产生一个新的embedding，来表达整个图的信息。而dist_token则是属于DeiT网络的结构。 9. Forward 这就是这个模型的总流程了：forward features -&gt; 最终输出   def forward(self, x):        #（B，C，H，W）-&gt; (B, 1, 768)        # (B,C,H,W) -&gt; (B, 1, 768), (B, 1, 768)    x = self. forward_features(x)              if self. head_dist is not None:            # 如果num_classes&gt;0, (B, 1, 768)-&gt;(B, 1, num_classes)      # 否则不变            x, x_dist = self. head(x[0]), self. head_dist(x[1])            if self. training and not torch. jit. is_scripting():        return x, x_dist      else:                # during inference,         # return the average of both classifier predictions        return (x + x_dist) / 2    else:            # 如果num_classes&gt;0, (B, 1, 768)-&gt;(B, 1, num_classes)            # 否则不变      x = self. head(x)    return x这样ViT算是给我说完了，DeiT又涉及到很多新的概念，之后也会参考代码，进行详细解说。     觉得有用，关注，在看，点赞，转发分享来一波哦！ 参考： [1] Jay Alammar, The Illustrated Transformer, jalammar. github. io, 2018 [2] 简枫，聊聊 Transformer，知乎，2019 [3] 大师兄，模型优化之Layer Normalization，知乎，2020 [4] TensorFlow Core，理解语言的 Transformer 模型，TensorFlow，https://www. tensorflow. org/tutorials/text/transformer "
    }, {
    "id": 28,
    "url": "http://localhost:4000/Facebook%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0Visual-Transformers(ViT)%E7%9A%84%E8%AE%AD%E7%BB%83%E7%BB%8F%E9%AA%8C(Moco-v3)-%E8%AE%AD%E7%BB%83%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/",
    "title": "Facebook自监督学习Visual Transformers(ViT)的训练经验(Moco v3) -- 训练代码解析",
    "body": "2021/10/18 -      我们来讲Moco v3的代码。     论文的主要内容，参考系列首篇：自监督学习Visual Transformers(ViT)的训练经验(Moco v3) – 论文解析     官方代码链接： https://github. com/facebookresearch/moco-v3    但现在最佳的模型是微软的EsViT(Swin-B)，然后才是Moco v3，下面是来自https://paperswithcode. com/的统计：     这张图最后边的点是EsViT(Swin-B)，图中文字没显示出来。     这个模型也公开了源代码： https://github. com/microsoft/esvit    这个代码也会解析哦，心动就关注吧，又给自己挖了一个坑。     公开的Moco v3是用Pytorch实现的，包含自监督学习的Resnet和ViT。而原始的Moco v3其实是用tensorflow实现的，在TPU上做的实验。     准备工作有安装Python，Pytorch，等相关软件，下载ImageNet等，这里就不展开说了。     我们直接开始看预训练主函数main_moco. py，开始逐行解析（解析都在注释里）： import torchvision. models as torchvision_modelstorchvision_model_names = sorted(name for name in torchvision_models. __dict__  if name. islower() and not name. startswith( __ )  and callable(torchvision_models. __dict__[name]))# 产生所有可以用来进行预训练的模型的名称的集合model_names = ['vit_small', 'vit_base', 'vit_conv_small', 'vit_conv_base'] + torchvision_model_names    torchvision. models还可以用来加载预训练模型： # arch是需要加载的预训练模型名，比如：resnet18model = torchvison. models. __dict__[arch](pretrained=True)     接下来看main函数： import torch. backends. cudnn as cudnndef main():  # 存入参数  args = parser. parse_args()    # seed默认是None    # 关于种子的正确设定方式    # 可以参考 https://pytorch. apachecn. org/docs/1. 0/notes_randomness. html          if args. seed is not None:    random. seed(args. seed)    torch. manual_seed(args. seed)    cudnn. deterministic = True    warnings. warn('You have chosen to seed training. '           'This will turn on the CUDNN deterministic setting, '           'which can slow down your training considerably! '           'You may see unexpected behavior when restarting '           'from checkpoints. ')        # cudnn. benchmark = False 这里应该还要加一行这个            # 默认是None，因为这个模型预训练工作量比较大，作者都是用几百片GPU或者TPU训练的。  if args. gpu is not None:    warnings. warn('You have chosen a specific GPU. This will completely '           'disable data parallelism. ')    # dist_url默认值是'tcp://224. 66. 41. 62:23456'，应当是作者服务器第一个节点的地址    # world_size默认值是-1    # WORLD_SIZE由torch. distributed. launch. py产生 具体数值为 nproc_per_node*node(服务器数量或者节点数)  if args. dist_url ==  env://  and args. world_size == -1:    args. world_size = int(os. environ[ WORLD_SIZE ])    # multiprocessing_distributed的默认值为False    # 需要多进程运行程序时一定要使multiprocessing_distributed为True  args. distributed = args. world_size &gt; 1 or args. multiprocessing_distributed    # 返回显卡数量    ngpus_per_node = torch. cuda. device_count()  if args. multiprocessing_distributed:        # Since we have ngpus_per_node processes per node，        # the total world_size needs to be adjusted accordingly        # 计算总的GPU的数量    args. world_size = ngpus_per_node * args. world_size    # Use torch. multiprocessing. spawn to launch distributed processes:         # the main_worker process function        # 开启多进程，每个进程调用main_worker函数，控制一个GPU。    mp. spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args))  else:    # Simply call main_worker function        # args. gpu默认是None，如果不采用分布式，则通过这个参数输入用来计算的GPU的编号        main_worker(args. gpu, ngpus_per_node, args)    这里需要注意的是，mp. spawn调用的main_worker其实有三个参数，但是mp. spawn后面的args却只给了后两个参数。而第一个参数的gpu的值会自动产生，调用的哪个gpu，就是那个gpu的编号。     接下来看main_worker函数，因为这个函数太长，会切成若干个代码框： import torch. distributed as distimport vitsdef main_worker(gpu, ngpus_per_node, args):    #gpu是运行这个函数使用的gpu编号，如果是None，则不调用gpu    args. gpu = gpu  # suppress printing if not first GPU on each node    # 如果不是主节点上的GPU 0，则抑制打印    # rank默认是-1，所以在第一个主机运行程序的时候要指定rank为0    # 可以打印的进程是在第一个主机控制着GPU 0的进程。  if args. multiprocessing_distributed and (args. gpu != 0 or args. rank != 0):    def print_pass(*args):      pass    builtins. print = print_pass     if args. gpu is not None:    print( Use GPU: {} for training . format(args. gpu))    # rank分两种，一种是全局rank, 可以理解为所有进程的编号。用于进程间通信。    # 一种是local rank, 每个节点上的进程的编号。用于本地设备分配。    # 以下rank都指全局rank    if args. distributed:    if args. dist_url ==  env://  and args. rank == -1:      args. rank = int(os. environ[ RANK ])    if args. multiprocessing_distributed:      # For multiprocessing distributed training,             # rank needs to be the global rank among all the processes            # 让每个进程都获得一个唯一的编号            # rank在作为参数输入的时候写的是节点编号      args. rank = args. rank * ngpus_per_node + gpu        # dist_backend默认的是nccl，一个多gpu卡通信框架    dist. init_process_group(backend=args. dist_backend, init_method=args. dist_url,                world_size=args. world_size, rank=args. rank)        # 设置栅栏，同步进程    torch. distributed. barrier()  # create model    # arch或a指模型的名称，默认是resnet50   print( =&gt; creating model '{}' . format(args. arch))  # 如果模型名称是以vit开头  if args. arch. startswith('vit'):    model = moco. builder. MoCo_ViT(            # 这里留意一下偏函数的应用，args. stop_grad_conv1默认是False      partial(vits. __dict__[args. arch], stop_grad_conv1=args. stop_grad_conv1),            # 默认分别是256，4096，1      args. moco_dim, args. moco_mlp_dim, args. moco_t)    else:    model = moco. builder. MoCo_ResNet(            # 这里zero_init_residual会使残差分支中最后一个BatchNorm2d的初始值为0            partial(torchvision_models. __dict__[args. arch], zero_init_residual=True),             args. moco_dim, args. moco_mlp_dim, args. moco_t)    上面这段代码主要是设置一些关键的参数，以及加载模型，接下来看第二部分代码main_worker[2]:   # infer learning rate before changing batch size  args. lr = args. lr * args. batch_size / 256  if not torch. cuda. is_available():    print('using CPU, this will be slow')  elif args. distributed:    # apply SyncBN，要理解SyncBN可以仔细看一下参考[1]    # 简单理解SyncBN就是多卡模式的Batch Normalization (BN)    # 把是或者继承了torch. nn. modules. batchnorm. _BatchNorm的BN全部替换成SyncBN    # 所以在写model时，BN必须用torch. nn. modules. batchnorm. _BatchNorm来实现    # 否则就得自己写一个SyncBN    model = torch. nn. SyncBatchNorm. convert_sync_batchnorm(model)    # For multiprocessing distributed, DistributedDataParallel constructor    # should always set the single device scope, otherwise,    # DistributedDataParallel will use all available devices.     if args. gpu is not None:      torch. cuda. set_device(args. gpu)      model. cuda(args. gpu)      # When using a single GPU per process and per      # DistributedDataParallel, we need to divide the batch size      # ourselves based on the total number of GPUs we have      # 把batch分一分      args. batch_size = int(args. batch_size / args. world_size)      # 用来导入数据的进程数量，必须大于0，默认是32，这里为什么这么计算，不是很懂。      args. workers = int((args. workers + ngpus_per_node - 1) / ngpus_per_node)      # 构造DDP模型      model = torch. nn. parallel. DistributedDataParallel(model, device_ids=[args. gpu])    else:      model. cuda()      # DistributedDataParallel will divide and allocate batch_size to all      # available GPUs if device_ids are not set      model = torch. nn. parallel. DistributedDataParallel(model)  elif args. gpu is not None:    torch. cuda. set_device(args. gpu)    model = model. cuda(args. gpu)    # comment out the following line for debugging    raise NotImplementedError( Only DistributedDataParallel is supported.  )  else:    # AllGather/rank implementation in this code only supports DistributedDataParallel.     raise NotImplementedError( Only DistributedDataParallel is supported.  )  print(model) # print model after SyncBatchNorm  # optimizer默认是lars，resnet50用的是lars  # weight_decay默认是1e-6，momentum默认是0. 9  if args. optimizer == 'lars':    optimizer = moco. optimizer. LARS(model. parameters(), args. lr,                    weight_decay=args. weight_decay,                    momentum=args. momentum)    # vit用adamw    # weight_decay需要被设定为0. 1  elif args. optimizer == 'adamw':    optimizer = torch. optim. AdamW(model. parameters(), args. lr,                                weight_decay=args. weight_decay)    依旧是一些关键参数，以及把并行训练的一些配置，main_worker[3]:   # 在训练最开始之前实例化一个GradScaler对象    # 自动混合精度的详细内容请参考[2]    scaler = torch. cuda. amp. GradScaler()    # 如果是第一进程，使用预设名称建立实体，具体请参考[3]  summary_writer = SummaryWriter() if args. rank == 0 else None  # optionally resume from a checkpoint    # resume默认为空    # 如果需要训练已经训练了一段时间的模型，可以通过resume输入模型地址  if args. resume:    if os. path. isfile(args. resume):      print( =&gt; loading checkpoint '{}' . format(args. resume))      if args. gpu is None:        checkpoint = torch. load(args. resume)      else:        # Map model to be loaded to specified single gpu.         loc = 'cuda:{}'. format(args. gpu)        checkpoint = torch. load(args. resume, map_location=loc)      args. start_epoch = checkpoint['epoch']      model. load_state_dict(checkpoint['state_dict'])      optimizer. load_state_dict(checkpoint['optimizer'])      scaler. load_state_dict(checkpoint['scaler'])      print( =&gt; loaded checkpoint '{}' (epoch {})          . format(args. resume, checkpoint['epoch']))    else:      print( =&gt; no checkpoint found at '{}' . format(args. resume))    # 在cuDNN中选择卷积算法，以加快训练速度，具体见参考[4]  cudnn. benchmark = True  # Data loading code    # 产生训练数据目录    traindir = os. path. join(args. data, 'train')    一些关键的设置，比如自动混合精度，归一化，以及是否是接续之前的训练，训练模型。main_worker[4]:     # transforms. Normalize公式input[channel] = (input[channel] - mean[channel]) / std[channel]  # Normalize() 函数的作用是将数据转换为标准正太分布，使模型更容易收敛。  # mean=[0. 485, 0. 456, 0. 406], std=[0. 229, 0. 224, 0. 225]   # 是从 ImageNet 数据集的数百万张图片中随机抽样计算得到的。  normalize = transforms. Normalize(mean=[0. 485, 0. 456, 0. 406],                   std=[0. 229, 0. 224, 0. 225])  # follow BYOL's augmentation recipe: https://arxiv. org/abs/2006. 07733  # 数据增强  augmentation1 = [    # 将给定图像随机裁剪为不同的大小和宽高比，然后缩放所裁剪得到的图像为制定的大小    # 即先随机采集，然后对裁剪得到的图像缩放为同一大小，默认scale=(0. 08, 1. 0)    transforms. RandomResizedCrop(224, scale=(args. crop_min, 1. )),    # 以0. 8的概率执行此动作，    transforms. RandomApply([      # 改变图像的属性：亮度（brightness）、对比度（contrast）、饱和度（saturation）和色调（hue)      # 怎么变的呢，将数字带入[max(0, 1 - offset), 1 + offset]获得区间      # 比如亮度位置上是0. 4，则得到区间[0. 6，1. 4]      # 那么新图的亮度会是这个区间的一个随机数      transforms. ColorJitter(0. 4, 0. 4, 0. 2, 0. 1) # not strengthened    ], p=0. 8),    # 依概率p将图片转换为灰度图    transforms. RandomGrayscale(p=0. 2),        # 产生在[0. 1，2]区间的随机数，对图片进行高斯模糊        # 调用的函数是ImageFilter. GaussianBlur    transforms. RandomApply([moco. loader. GaussianBlur([. 1, 2. ])], p=1. 0),        # 依据概率p对PIL图片进行水平翻转 参数：p默认值为0. 5    transforms. RandomHorizontalFlip(),        # 将PIL Image或者 ndarray 转换为tensor，并且归一化至[0-1] 注意事项：归一化至[0-1]是直接除以255，若自己的ndarray数据尺度有变化，则需要自行修改。    transforms. ToTensor(),        # 之前定义的归一化方式，归一化后，数值所在的区间大概是（-1，1）    normalize  ]  augmentation2 = [    transforms. RandomResizedCrop(224, scale=(args. crop_min, 1. )),    transforms. RandomApply([      transforms. ColorJitter(0. 4, 0. 4, 0. 2, 0. 1) # not strengthened    ], p=0. 8),    transforms. RandomGrayscale(p=0. 2),        # 高斯模糊的概率和augmentation1不一样    transforms. RandomApply([moco. loader. GaussianBlur([. 1, 2. ])], p=0. 1),    # 以0. 2的概率做Solarize        # 默认阈值是128，大于128的像素值，转换成二进制，然后做01反转        # 即0变1，1变0，得到新的像素值。    transforms. RandomApply([moco. loader. Solarize()], p=0. 2),    transforms. RandomHorizontalFlip(),    transforms. ToTensor(),    normalize    ]    这部分主要是关于数据增强的设定，至于为什么这么做数据增强，作者有把参考的文章列在代码注释里，感兴趣的可以去看一下。接下来是这个函数的最后一个部分main_worker[5]:     # 在指定目录下做载入数据    # ImageFolder(root,transform=None,target_transform=None,loader=default_loader)    # transform：对图片进行预处理的操作（函数），原始图片作为输入，返回一个转换后的图片。    # 对图片类别进行预处理的操作，输入为 target，输出对其的转换。如果不传该参数，即对 target 不做任何转换。    # loader 默认操作是读取PIL image对象。    # 这里是模型很关键的一步，将每一幅图做两种不同的transform，得到两张图  train_dataset = datasets. ImageFolder(    traindir,        # 输入im返回是[im1,im2]，分别按augmentation1和augmentation2做了变换    moco. loader. TwoCropsTransform(transforms. Compose(augmentation1),                    transforms. Compose(augmentation2)))  if args. distributed:        # 把数据划分成num_gpu份，不同的GPU拿自己那一份    train_sampler = torch. utils. data. distributed. DistributedSampler(train_dataset)  else:    train_sampler = None    # sampler和shuffle是互斥的，有sampler，shuffle就可以是None了    # num_workers要大于0，0代表用主进程导入数据，大于0表示用num_workers个进程导入数据    # pin_memory: if True, the data loader will copy tensors into CUDA pinned memory before returning them    # 主机中的内存，有两种存在方式，一是锁页，二是不锁页，     # 锁页内存存放的内容在任何情况下都不会与主机的虚拟内存进行交换（注：虚拟内存就是硬盘），    # 而不锁页内存在主机内存不足时，数据会存放在虚拟内存中。    # 显卡中的显存全部是锁页内存,当计算机的内存充足的时候，可以设置pin_memory=True。    # 意味着生成的Tensor数据存放在锁页内存中，这样内存中的Tensor转移到GPU的显存会更快。     # 当系统卡住，或者交换内存使用过多的时候，设置pin_memory=False。    # 因为pin_memory与电脑硬件性能有关，pin_memory默认为False。    # drop_last: set to ``True`` to drop the last incomplete batch     train_loader = torch. utils. data. DataLoader(    train_dataset, batch_size=args. batch_size, shuffle=(train_sampler is None),    num_workers=args. workers, pin_memory=True, sampler=train_sampler, drop_last=True)  for epoch in range(args. start_epoch, args. epochs):    if args. distributed:            # 加了这行，每次gpu才会拿到不同组合的batch            # 换句话说，加上这行，每个epoch才会被shuffle      train_sampler. set_epoch(epoch)        # train for one epoch，这个接下来讲    train(train_loader, model, optimizer, scaler, summary_writer, epoch, args)        #主进程存储训练过程中的模型，防止因为一些事情中断训练后，需要从头再来。    if not args. multiprocessing_distributed or (args. multiprocessing_distributed        and args. rank == 0): # only the first GPU saves checkpoint      save_checkpoint({        'epoch': epoch + 1,        'arch': args. arch,        'state_dict': model. state_dict(),        'optimizer' : optimizer. state_dict(),        'scaler': scaler. state_dict(),            # 这个代码里没有做is_best判断            # 通常这个参数是用来保存训练过程中测试的效果最好的模型，防止过拟合。            }, is_best=False, filename='checkpoint_%04d. pth. tar' % epoch)  if args. rank == 0:    summary_writer. close()这部分代码主要内容：一、准备数据，二、训练，三、主进程保存节点模型。     接下来看train这个函数： def train(train_loader, model, optimizer, scaler, summary_writer, epoch, args):    # AverageMeter是作者自己定义的函数，用来计算循环中，一些过程的平均消耗  batch_time = AverageMeter('Time', ':6. 3f')  data_time = AverageMeter('Data', ':6. 3f')  learning_rates = AverageMeter('LR', ':. 4e')  losses = AverageMeter('Loss', ':. 4e')  progress = ProgressMeter(    len(train_loader),    [batch_time, data_time, learning_rates, losses],    prefix= Epoch: [{}] . format(epoch))  # switch to train mode  model. train()  end = time. time()  iters_per_epoch = len(train_loader)  moco_m = args. moco_m  for i, (images, _) in enumerate(train_loader):    # measure data loading time        # 计算在循环中，载入数据平均消耗的时间。        data_time. update(time. time() - end)    # adjust learning rate and momentum coefficient per iteration    # adjust_learning_rate是作者自己定义的函数        # learning rate先由小变大，再由大变小。具体可以参考代码    lr = adjust_learning_rate(optimizer, epoch + i / iters_per_epoch, args)        # 更新在循环中，使用的lr的平均值    learning_rates. update(lr)    if args. moco_m_cos:            #同样是作者自己定义的函数，moco_m则是由本来就很接近1（0. 99），变得越来越接近1。      moco_m = adjust_moco_momentum(epoch + i / iters_per_epoch, args)    if args. gpu is not None:      # 如果pin_memory=True的话，将数据放入GPU的时候，      # 也应该把non_blocking打开，这样就只把数据放入GPU而不取出，访问时间会大大减少。      images[0] = images[0]. cuda(args. gpu, non_blocking=True)      images[1] = images[1]. cuda(args. gpu, non_blocking=True)    # compute output    with torch. cuda. amp. autocast(True):            # moco_m: moco momentum of updating momentum encoder            # model下一篇说      loss = model(images[0], images[1], moco_m)        # loss平均值更新    losses. update(loss. item(), images[0]. size(0))    if args. rank == 0:      summary_writer. add_scalar( loss , loss. item(), epoch * iters_per_epoch + i)    # compute gradient and do SGD step        # optimizer. zero_grad()意思是把梯度置零,        # 也就是把loss关于weight的导数变成0.     optimizer. zero_grad()    # Scales loss. 为了梯度放大. scaler原理可以参考[2]    scaler. scale(loss). backward()        # scaler. step() 首先把梯度的值unscale回来.     # 如果梯度的值不是 infs 或者 NaNs, 那么调用optimizer. step()来更新权重,    # 否则，忽略step调用，从而保证权重不更新（不被破坏）    scaler. step(optimizer)    # 准备着，看是否要增大scaler scaler. update()    # measure elapsed time    batch_time. update(time. time() - end)    end = time. time()        # print_ferq默认是10    if i % args. print_freq == 0:            progress. display(i)    这里就是Moco v3训练的主要代码，有些太过细枝末节又容易懂的就没有放进来。     代码主要特点和主要内容有：		- 多卡，每张卡有多个gpu，每个进程管理一个gpu		- 用了自动混合精度机制    不同的数据增强的配置   当然还有很多细节的设计，比如lr和moco_m参数的调整，数据存入gpu的方式等等，都是非常值得深入学习的，这篇只是浏览，具体原理可以参考我给的一些参考链接，自己搜索更有用的资料，或者去研读pytorch源代码。      下一篇讲Moco v3的模型代码。喜欢或者觉得有用就关注吧，可以的话，右下角点个“在看”吧。     参考： [1] 996黄金一代，[原创][深度][PyTorch] DDP系列第三篇：实战与技巧，知乎，2020 [2] Gemfield，PyTorch的自动混合精度（AMP），知乎，2020 [3] dexter，TensorBoardX 介紹 (在 PyTorch 中使用 Tensorboard），知乎，2018 [4] xiaopl，torch. backends. cudnn. benchmark ?!，知乎，2019 "
    }, {
    "id": 29,
    "url": "http://localhost:4000/GiantMIDI-Piano-%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A8%E5%8F%91%E5%B8%83%E7%9A%84%E5%A4%A7%E5%9E%8B%E5%8F%A4%E5%85%B8%E9%92%A2%E7%90%B4%E4%B9%90MIDI%E6%95%B0%E6%8D%AE%E9%9B%86/",
    "title": "GiantMIDI-Piano：字节跳动发布的大型古典钢琴乐MIDI数据集",
    "body": "2021/10/11 -     机器学习或深度学习的研究常常会用到一些公开数据集，这里呢，为大家介绍一个公开的古典钢琴乐的MIDI数据集。     这是字节跳动（ByteDance）的项目，项目地址： https://github. com/bytedance/GiantMIDI-Piano    首先说一下MIDI是什么，可以简单理解为数字乐谱。     进行步骤：    从IMSLP（International Music Score Library Project）抓取作曲家的名字和他的曲目名（最终获得18067个作曲家的名字和143701个曲子名称，并记录每个作曲家的名字，曲子，出生日期，死亡日期，国籍。有些作曲家国籍，出生死亡日期是缺失的     在Youtube上根据作曲家的名字和曲名爬曲子，下载Youtube返回的第一顺位的曲子。这里需要注意的是返回的结果里的字和我们搜索的关键字或者关键字集合未必完全匹配，但仍有可能是正确的结果，搜索结果匹配度计算方式如下（Jaccard Similarity，X是搜索关键字的集合，Y是搜索结果的关键字的集合，J表示相似度，越大越好，这个任务里阈值是0. 6）：  最后找到了9709个作曲家的60724个曲子。  这样下载的曲子，有些是单纯的钢琴曲，但有些不是，怎么从里面择出钢琴曲呢？他们训练了一个CNN网络，把曲子切成一秒一秒的片段，分别转换成梅尔频谱（Log Mel Spectrograms）输入，输出是是钢琴曲的概率，再取平均值，最后的结果如果大于0. 5，则判断该曲子为钢琴曲。最后得到2786个作曲家的10854个钢琴曲。 通过他们之前设计的一个系统high-resolution piano transcription system（https://github. com/bytedance/piano_transcription），将收集的钢琴曲都转换成琴谱，转谱内容包括音高（pitch），琴键的触发（onset）、抬起（offset）、和力度（velocity），以及踏板的触发（onset）和抬起（offset）。    接下来看一些统计数据：      按国家统计了数据集中作曲家的数量。     数据集中88个琴键被使用的情况。     数据集中作曲数量Top 100的作曲家最常使用的琴键。     这里展示了来自三个不同地方的作曲家的琴键的使用情况。     六个作曲家使用不同调的频率的统计。     音差分布的统计，音差即后一个音调减去前一个音调的数值。     不同三和弦的使用频次。     不同四和弦的使用频次。     最后看一下，如何评估这个转谱系统的，他们手工标注了数据集中100个钢琴曲，发现其中有89个确实是钢琴曲，也就是说分辨一个曲子是不是钢琴曲的系统的精度是89%。     再用HMM（Hidden Markov Model）这样一个对齐工具来对齐由系统生成的MIDI和手工生成的MIDI内容。那么转谱错误率计算公式如下：      S（substitution）指替代错误，I（insertion）指插入错误，D（Deletion）指删除错误的数量。N为音符总数。     对于MAESTRO这个数据集，错误的来源分两种，一种是performance error（钢琴家演奏的时候弹错了）和system error（对齐系统的错）：    而GiantMIDI还有一个transcription error（翻译错误）： Relative error的计算方式如下： Relative error会近似于transcription error。因为由系统和演奏造成的错误值大概率是相近的。 文章中评估了52首同时出现在这两个数据集中的曲子。得到的结果如下: Relative difference越小，说明曲转谱系统越好。     这篇就到这里了，简单介绍了一下数据集的产生，如果大家对转谱系统感兴趣，可以去看他们团队的另外一篇文章：  &lt;div align=center&gt;&lt;/div&gt; 他们公开的数据集包括搜集到的钢琴曲mp3，转谱的结果，以及相关的代码和模型。对古典音乐感兴趣的小伙伴可以下载听听。 ​  最后还是那句话，觉得有用就关注我吧。右下角点个“在看”再走哇。 参考： [1] 机器之心，万余首钢琴作品、一千多个小时，字节跳动发布全球最大钢琴MIDI数据集，2020 "
    }, {
    "id": 30,
    "url": "http://localhost:4000/Stanford-CS224N%E7%AC%94%E8%AE%B0-%E8%AF%8D%E5%90%91%E9%87%8F%E5%92%8C%E8%AF%8D%E4%B9%89-Word-Vectors-and-Word-Senses/",
    "title": "Stanford CS224N笔记：词向量和词义（Word Vectors and Word Senses）",
    "body": "2021/09/06 -     想系统学一下NLP，所以再开个新坑，如果有天我累死了，一定不要奇怪，都是自找的     一不小心写得小作文了，文章里不会所有概念都仔细说，但尽量会给参考链接，会对视频课程做一些延申。     给你们翻译一下这张图：     A：好吧，我可以不在乎！     B：我觉得你在说，你无法不在乎， 你说你可以不在乎，表示你至少是有一点在乎的。     A：我不知道。     A：我们是在虚空中漂流的复杂得难以置信的大脑，在一片黑暗中盲目地抛出文字，徒劳地想要通过这种方式和别人产生联系。（有没有觉得有点莎士比亚）     A：每一个关于措辞，拼写，语气，时间的选择都包含了无尽的信号，意义，和潜台词。而每个人都会用自己的方式来理解。语言不是一个规范的系统，语言是瑰丽又混沌的。     A：你永远无法明确知道任何一段文字对任何一个人的真正意义。     A：你可以做的，是尝试更好的揣测自己的语言会对别人造成的影响，那样，你才能能组词措句让他们感受到你想让他们感受的。其他都是毫无意义的。     A：我想，你告诉我你怎么理解我的话，是想让我觉得不那么孤单。如果是这样，感谢你。这对我来说意义非凡。     A：但如果你只是在用一些心理学的技巧在分析我说的话，来显示你知道很多。     A：那我想我不必在意。     这幅图是教授用来开题的，语言中带有很多不确定性，甚至说话的人都有可能受潜意识驱动，并不清楚自己真正想要表达的，而我们每个人对每句话可能都有不同的理解，语言是个瑰丽，混沌，模糊的系统，是一抹精致的灰。而不是逻辑严密，规整清晰的。《小王子》里也有一句很类似的话：”…. . 你先坐在草地上，离我稍微远些，就像这样。我从眼角里瞅着你，你什么也不要说。话语是误解的根源。但是，每天，你都可以坐得离我稍微近一些……”     在这里延申一个概念，图灵测试，图灵测试是英国计算机科学家图灵于1950年提出的，注意到“智能”这一概念难以确切定义，他提出了著名的图灵测试：如果一台机器能够与人类展开对话（通过电传设备）而不被辨别出其机器身份，那么称这台机器具有智能。因此，我们也可以这样理解图灵测试，它定义“智能”为掌握了语言，包括理解和应用。     要理解语言，首先我们要学字，词，计算机也一样。对于我们而言，每个单词，每个字都是有意思的，那么计算机要怎么也能让每个字在它那里有个意思呢？换句话说，我们要怎么将每个词的意思转换成计算机能读的形态。这样说可能还是觉得模糊，我们来看看一些已经有的一些解决方案来理解一下：     1. WordNet     WordNet的使用方法和一些基本概念可以参考[2]，这里简单说一下他的基本概念，英文中一个单词可以有很多词性，比如一个单词，可以是名词（noun），形容词（adj），副词（adv），动词（verb）。WordNet用下面这种结构来表达词义：     比如说dog既可以做名词用，又可以做动词用，那么dog就会被分为名词和动词两类，而这两类里又有不同的词义，dog做名词用时，可以有不同的涵义，而每个含义我们都给它一个符号表示。所以一个单词的一种具体含义就用“单词. 词性. 词义序号”这种方式来表达，比如：dog. n. 01表示dog作为名词时的一种含义。下面这段代码就不难理解了：       from nltk. corpus import wordnet as wn#将所有词性缩写映射到词性名词上。poses = { 'n':'noun', 'v':'verb', 's':'adj (s)', 'a':'adj', 'r':'adv'}#找出good这个词的每个词义 for synset in wn. synsets( good ):  #打印出每个有这个词义的单词  print( {}: {} . format(poses[synset. pos()], ,  . join([l. name() for l in synset. lemmas()])))运行发现，good可以有很多种含义，而大多数含义不是只有good才有，大多数含义都可以用其他单词表达： 再看一个WordNet的例子： from nltk. corpus import wordnet as wn#panda可以有两种意思，一种是giant_panda. n. 01指大熊猫#一种是lesser_panda. n. 01指小熊猫，#panda. n. 01和giant_panda. n. 01是一个意思#这里panda得到的是giant_panda. n. 01这个值panda = wn. synset( panda. n. 01 )#求上位词，比如panda. n. 01的上位词是procyonid. n. 01hyper = lambda s: s. hypernyms()#循环找自己的上位词 list(panda. closure(hyper))panda = wn. synset( panda. n. 01 )得到的结果如下，可以打印出大熊猫一层一层往上所属的类，可以发现WordNet是树状的，知识性的，明确的，规整的：  那理解了WordNet整理语言的方式，那么我们来看一下它的缺点：    我们开始提过，语言是混沌的，模糊的，不是规范的，规整的，但是WordNet的这种组织方式，非常规整，导致它并不能展现词与词之间的微妙差别，比如在WordNet里proficient和good可以表达同样的意思，但如果是同样的场景和上下文，用proficient和good其实是有细微差别的，而WordNet是无法表现这种差别的。     另外就是，这种结构是人为规范的，需要人的介入，时时更新，整理出新的词和词义。     因为是人整理的，所以这种结构是主观的，是出于人对语言的理解。     需要人力。     无法计算词与词之间的相似性，或者差别。  有这么多缺陷，所以啊，得想个新法子，我们看下一个。     2. One-Hot         每个单词都用一个向量来表示，每个单词在向量里都有一个专属的位置，比如说at这个单词，那么就用[0,0,1,0,…,0]这个向量来表示，即第三个位置表示at，所以第三个位置值为1，其他位置的值都是0，之所以这个方法名字叫one-hot，就是因为向量里只有一个位置的值是1，其他位置的值都为0。     但这个方法的缺陷也尤为明显：  单词量还是挺大的，比如如果我们有10000个单词，那就得用10000个长度为10000的向量来表示这个系统。而且大多数位置还是0，特别稀疏，非常浪费。 另外，就是所有的向量在数学层面来说都是正交的，这样的方式无法表达词的意思，也无法计算词与词之间的相似度。    3. Word2Vec     “You shall know a word by the company it keeps” (J. R. Firth 1957: 11)     中国有句古话，物以类聚，人以群分，上面这句话的意思就是，你想知道一个词是什么意思吗？那看看出现在它周边的单词就知道了。这个想法是1957年由一个语言学家提出来的。     根据这个想法，产生的一个概念叫分布语义： Distributional semantics: A word’s meaning is given by the words that frequently appear close-by. 一个词的意思是由周边经常出现的词决定的。 比如我们要表示banking这个词，我们就用围绕着banking的上下文，前前后后出现的单词来表示。     这个想法似乎有点古怪，不太能想通，也显得不太可靠，应该不管用吧，但事实是按照这个想法设计的算法，产生的词义，能很好的表达单词的意思。超乎预料的管用。    我们先不管算法，看看最后的结果可以是怎么样的： 比如banking这个词，就可以是一个长度为8的向量（这个长度可以根据单词量，样本量和精确度需求来决定，这里只是举个例子），每个维度应当都表达了一层意思，但这个意思不是人为去定义的，而在每个维度上的值就组成了banking的整体意思。     我们可以把一个向量想象成一个高维度的空间里的一个位置，比如说一个二维的向量，就可以表示一个平面上的点，一个三维的向量，则可以表示我们生活的这个三维空间里的一个点，一个四维的向量，立体空间再加上时间这个维度就是四维的。那么如果我们用一个长度为8的向量去表示一个词，则可以理解成，在一个高维度（八维）的空间里，banking这个词有个位置，它所在的位置即是它的意思。而在这个八个维度的空间里，每个在训练样本里的单词，都会有一个位置。比如像下图一样：     上面只是一个意思意思的示意图，因为超过两维的，咱画不出来。     Word2Vec就是运用这个概念的算法，那我们来看算法具体的思路： 翻译如下：  我们有一个语料库，里面有很多文章或者对话，总之是语言。- 每个单词我们都用一个向量来表示（实际算法一般是两个向量，一个中心词向量，一个周边词向量），每个单词的向量维度是一致的，刚开始的时候，向量里的值是随机赋予的随机数。 遍历每篇文章里的每个位置，经过那个位置的时候，在那个位置上的词为中心词c，周边的词是o。 利用c和o的相似度来计算给出c能得到o的概率，即c发生的条件下，o发生的概率P(o|c)，或者反过来P(c|o)也行。 调整词向量里的值来提高P(o|c)或P(c|o)。 循环重复之前的三个步骤，直到P(o|c)或者P(c|o)无法再提高。举个例子如下图： 比如中心词为‘into’的时候，而临近词的区域设定为2的时候，就需要计算当into出现时，那些在区间内出现的词会出现概率。 计算完后，再以下一个单词为中心词，重复上面的过程。  条件概率的计算是一个softmax的方程（具体可以看Softmax）： vc是中心词向量，uo是周边词向量，V是所有单词的集合，uw是V中的单词。 这个问题的极大似然函数： 𝜃指所有可以调整的参数，这个任务里指所有词向量里的所有值，这里每个词有两个词向量，一个u（作周边词的时候用），一个v（作中心词的时候用）： T指所有位置的集合，经过一个位置时，位置上的词为中心词wt，周边的词为wt+j，m限制了周边词的范围，L(𝜃)求的是在参数为𝜃的情况下，遍历所有的文本位置，求每个中心词和周边词的条件概率，再把它们乘起来。得到的条件概率越大，L(𝜃)越大。而我们要做的是调整𝜃的值，使得L(𝜃)获得最大值。 损失函数的定义： 将最大似然函数调整一下，就可以得到通常会用的损失函数，最小化J(𝜃)和最大化L(𝜃)本质上是在做同一件事。 那如何根据损失函数来调整𝜃呢，求导，做梯度下降： 上面的公式是示意公式，实际上是对单个参数分别求导，做梯度下降： 过程如下图所示： 每次对𝜃做细微的调整，直到𝜃接近自己的最优值，即可以使L(𝜃)最小的值。上图中Cost即L(𝜃)。关于梯度下降的具体操作和方法细节，之后的笔记会做介绍。 最后我们来看看，怎么用别人训练好的Word2Vec做一些事情： import numpy as np# Get the interactive Tools for Matplotlib%matplotlib notebookimport matplotlib. pyplot as pltplt. style. use('ggplot')from sklearn. decomposition import PCAfrom gensim. test. utils import datapath, get_tmpfilefrom gensim. models import KeyedVectorsfrom gensim. scripts. glove2word2vec import glove2word2vecgensim是一个工具包，可以让我们导入训练好的模型参数，斯坦福有一个GloVe项目（https://nlp. stanford. edu/projects/glove/），在里面可以下载训练好的词向量： #将下载的词向量文件放在合适的目录，并读入glove_file = datapath('D:/glove. 6B. 100d. txt')word2vec_glove_file = get_tmpfile( glove. 6B. 100d. word2vec. txt )glove2word2vec(glove_file, word2vec_glove_file)model = KeyedVectors. load_word2vec_format(word2vec_glove_file)接下来，我们就可以做一些事情了，比如找近义词： &gt;&gt;model. most_similar('obama')[('barack', 0. 937216579914093), ('bush', 0. 927285373210907), ('clinton', 0. 896000325679779), ('mccain', 0. 8875633478164673), ('gore', 0. 8000321388244629), ('hillary', 0. 7933662533760071), ('dole', 0. 7851964831352234), ('rodham', 0. 7518897652626038), ('romney', 0. 7488929629325867), ('kerry', 0. 7472624182701111)]比如找反义词： &gt;&gt;model. most_similar(negative='banana')[('shunichi', 0. 49618104100227356), ('ieronymos', 0. 4736502170562744), ('pengrowth', 0. 4668096601963043), ('höss', 0. 4636845588684082), ('damaskinos', 0. 4617849290370941), ('yadin', 0. 4617374837398529), ('hundertwasser', 0. 4588957726955414), ('ncpa', 0. 4577339291572571), ('maccormac', 0. 4566109776496887), ('rothfeld', 0. 4523947238922119)]比如找对应的词： # king-woman 近似于 queen-man &gt;&gt;model. most_similar(positive=['woman', 'king'], negative=['man'])[0]('queen', 0. 7698540687561035)&gt;&gt;model. most_similar(positive=['australia', 'japanese'], negative=['japan'])[0]('australian', 0. 8923497796058655)&gt;&gt;model. most_similar(positive=['france', 'beer'], negative=['australia'])[0]('champagne', 0. 6480063796043396)&gt;&gt;model. most_similar(positive=['reagan', 'clinton'], negative=['obama'])[0]('nixon', 0. 7844685316085815)&gt;&gt;model. most_similar(positive=['bad', 'fantastic'], negative=['good'])[0]('terrible', 0. 7074226140975952)# 找出非我族类&gt;&gt;model. doesnt_match( breakfast cereal dinner lunch . split())'cereal'可以看出来，Word2Vec使每个词都有了一个可以让计算机计算的独一无二的意思。而这个意思是非常非常接近人的理解的。而数值的复杂性和不可解释性也恰恰符合了语言混沌的特点。     那么这到底是不是最优解呢？我想这还需要时间来回答。但很显然，现在自然语言处理离强人工智能还有很远的距离。但毫无疑问，对于这个答案的探索，对人类本身的意义也是非常的。     第一课笔记结束啦，觉得有用就关注吧，右下角的‘在看’不要忘记点哦。  参考： [1] http://web. stanford. edu/class/cs224n/ [2] TinaSmile，手把手教你NLTK WordNet使用方法，IT人，https://iter01. com/521234. html "
    }, {
    "id": 31,
    "url": "http://localhost:4000/%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0Visual-Transformers(ViT)%E7%9A%84%E8%AE%AD%E7%BB%83%E7%BB%8F%E9%AA%8C(Moco-v3)-%E8%AE%BA%E6%96%87%E8%A7%A3%E6%9E%90/",
    "title": "自监督学习Visual Transformers(ViT)的训练经验(Moco v3) -- 论文解析",
    "body": "2021/09/01 -     之前讲过一篇自监督学习：自监督学习（Self Supervised Learning），里面有提到几种把图像转成通用的embedding的方式，有CPC, SimCLR, 还有Moco。今天来详细说一下Moco v3，主要讲下面这篇文章：An Empirical Study of Training Self-Supervised Visual Transformers。     这篇文章主要讲的是自监督ViT的训练技巧。之后一定会讲这篇文章的源代码哦，心动就关注吧。     CV和NLP在预训练时有两点不同：  NLP选择的通常是masked auto-encoder，而CV最近比较倾向孪生网络（Siamese networks） NLP用的主干结构是self-attentional Transformers，而CV的主干通常是deep residual networks（ResNets）    这篇文章，主干结构用的是self-attentional Transformers，网络框架是基于Siamese networks（Moco和一些其他框架）。网络属于Vision Transformers (ViT)这个范畴。ViT属于一种新的网络结构，因此关于这种网络的训练技巧比较少，这篇文章主要就从一些很基础的方面，比如batch size，learning rate，optimizer这些方面来讨论，这种网络要怎么训练。实验反馈的结果是，Transformers搭配Contrastive Learning会比搭配auto-encoding效果好。下图中iGPT是Transformers搭配auto-encoding的模型，可以发现Moco v3优势是很大的。另外也可以发现，ViT越大，准确率越高，这表示ViT还持续有潜力。另外，ViT-Large在某些案例里可以直接打败监督学习。 ViT也可以打败ResNets和Contrastive Learning的搭配，下面这张是MoCo各个版本搭配ResNets训练的结果：     作者发现，训练过程中的不稳定情况会略微降低这个网络最后的精确度，大概1%-3%。而通过一些鸡贼的操作，可以减轻这个问题带来的影响，从而提高精确度。我们就来看看，究竟是个什么问题，他们又是怎么解决的。     我们看一下Moco v3架构的伪代码： # f_q: encoder: backbone + proj mlp + pred mlp# f_k: momentum encoder: backbone + proj mlp# m: momentum coefficient# tau: temperaturefor x in loader: # load a minibatch x with N samples  x1, x2 = aug(x), aug(x) # 将同一个样本随机进行不同的数据增强处理  q1, q2 = f_q(x1), f_q(x2) # 用encoder分别编码  k1, k2 = f_k(x1), f_k(x2) # 再 momentum encoder分别编码  loss = ctr(q1, k2) + ctr(q2, k1) # 交叉计算loss再求 loss. backward()  update(f_q) # 更新encoder的参数  f_k = m*f_k + (1-m)*f_q # 根据更新的encoder和之前的momentum encoder得到新的momentum encoder# contrastive lossdef ctr(q, k):  logits = mm(q, k. t()) # [N, N] 对角线上的出自同一幅图 labels = range(N) # positives are in diagonal  loss = CrossEntropyLoss(logits/tau, labels) return 2 * tau * loss     损失函数在伪代码里是ctr，也可以看上图的公式，q和k+是同一幅图得到的embedding，q和k-则是不同图得到的embedding，q是encoder得到的，k是由momentum encoder得到的，容易理解的是，这样的损失函数，就是要让qk+的值尽量大，qk-的值尽量小，而momentum的设计目的我们之后再说。关于Cross Entropy Loss可以看Cross Entropy Loss。     这里说一下encoder（f_q）和momentum encoder（f_k）的网络结构，encoder包含的是一个主干网络结构，即ViT，或者ResNet，另外还包含一个projection head和一个prediction head，而momentum encoder则只有projection head和主干网络，momentum encoder在获取encoder参数来调整自己参数的时候，不会用到prediction head。 接下来我们来看一下各种训练参数怎么配置比较合适： 1.  Batch size     通常来说，ViT是很大的模型，配置比较大的batch size往往会取得更好效果，下面是配置了不同batch size的一些实验结果： 为了展示ViT训练存在的不稳定性问题，他们给了上面的实验结果，这里涉及到一个概念，kNN accuracy，这个具体怎么计算，我们解析代码的时候再说，这里我们就把它当成评量自监督模型的方式就好，越高，表示模型越好。实验者将batch size逐渐放大，从1024到6144，发现当batch size变大的时候，kNN accuracy出现了很大的震荡，这种震荡是局部突然下降（作者推测是训练突然重启了，即跳出了原本的局部最优点，重新梯度下降，训练虽然没有偏离，但准确性决定于新的局部点），batch size越大，这种现象越严重（为了更好的捕捉这种震荡，训练的时候他们每十个batch就测一次kNN accuracy）。 2.  Learning rate     首先设定了一个base learning rate: lr，然后再根据batch size调整，调整方式是：lr*BatchSize/256。     和之前调整batch size一样，learning rate也出现了局部突然下降又快速回升的现象，learning rate越大，这种现象越明显。另外也可以看出来，不是越稳定就能获得越好的结果。 3.  Optimizer     文章里提到了三种Optimizer，一种AdamW（常用来做ViT训练），一种LARS（在自监督学习经常用来匹配batch size很大的训练），最后一种LAMB（AdamW-counterpart of LARS）。实验在不同lr情况下对LAMB进行了测试，发现当lr=5e-4的时候，取得的效果会好于用AdamW的，但是看图我们会发现LAMB对lr的取值非常敏感。虽然曲线平滑，没有用AdamW时候突然下降的情况，但是训练中途会下降很长一段时间，作者们猜测是用LAMB会有梯度不可靠的情况。因此，这篇文章最终选择的Optimizer还是AdamW。 4. 如何提高训练过程中的稳定性     接下来，我们看看怎么解决精确度突然下降的问题，首先当然要找到产生这个现象的原因： 他们去观察了网络每一层在训练过程中的梯度，发现是梯度的突增导致了精确度的陡降，还发现，总是第一层最先开始突增，大概几十个batch之后，最后一层的梯度也出现了同样的现象。这样不难推测，不稳定性极有可能是最前面的patch projection layer引发的，然后实验者就尝试在训练过程中冻结patch projection layer的参数，即这一层在最开始参数随机配置后就不再做梯度下降，调皮点说，就是既然你不好好学习，那我们就暂时不让你学习啦。开心不？     答案是开心的。     不难发现，冻住projection layer效果是明显的，准确率陡降的现象消失了，而且最终获得的准确率也提高了，甚至当lr增大的时候，这种提高还尤为明显。     另外，他们发现这种不稳定性不止出现在MoCo里，SimCLR和BYOL里也有类似的问题。 同样的，将patch projection layer冻住，不让它学习，可以大大提高SimCLR和BYOL的稳定度，且可以最终提高模型的准确率。此外，还有SwAV，可以用这个方法使得SwAV可以使用更大的lr来训练模型，进而获得更高的精度。所以这个方法几乎可以用于所有的这一类自监督学习模型的训练。     另外，他们还尝试用BatchNorm（BN），WeightNorm（WN），甚至对patch projection进行梯度裁剪（gradient clip），即设定一个阈值，当梯度超过这个阈值的时候，就让梯度等于这个阈值。实验发现，BN和WN是无用的，梯度裁剪是有用的，但是要设定好阈值，总的来说，还是干脆不让学习最有效。     这个方法虽然有效地缓和了训练过程中的不稳定性的问题，但是本质上，冻结patch projection来克制不稳定性是一种回避问题的方法，当lr太大时，依然还是会出现不稳定的情况。第一层应当不是造成不稳定的本质原因，只是它不是Transformer的部分，好控制，并不算找到了解决不稳定性的根本办法。对于这个问题，应该要有更好的解决办法才对（小本本记下来，潜在课题）。     到这里，文章的前四小节本啤应该是讲清楚了，之后的细节会穿插在代码里面说。     觉得有用，右下角帮啤酒点个在看吧 "
    }, {
    "id": 32,
    "url": "http://localhost:4000/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C%E4%B9%8BReference%E6%95%B4%E7%90%86%E8%A7%84%E8%8C%83/",
    "title": "论文写作之Reference整理规范",
    "body": "2021/08/29 -     咱写论文，每次都要整个Reference，每个期刊会议，规矩都会稍稍不一样，好在他们都会发指引文档。     这里就整理点基本的规范，统一的规则。每次可以照着找找自己的茬。这里不讲bib，latex用法，假设我们文章已经写好了，做最后的检查的时候，关于Reference我们要做哪些事情。有错漏欢迎指正补充哦。     这个规范主要参考的是《IEEE REFERENCE GUIDE》，不是这个领域的可能不适用。  一、作者或编辑     首先不管什么类型的参考，一般都是作者打头，开头得是作者或者发布文档的机构，比如： 作者只有一个人，姓(Last Name)完整保留，名字则只保留每个单词首字母，大写，打点表示省略。 如果有两个作者用and链接： 如果是大于等于三个作者，逗号分割，最后用and连接： 有些论文，作者众多，那么有些期刊会议有要求最多只能显示几个作者，超过一定数量，只写主要作者，其他的用et al. 表示省略（et al. 斜体），有些则会要求尽可能显示所有作者，除非你找不到人家的名字： et al. 表示作者不止列出的这一个。     另外我们还会在名字后面看到eds. ，表示的是前面的名字是编辑的名字，在参考[1]中，以Eds. 形式出现，e大写，如果只有一个编辑，Ed. 即可，小写ed. 通常指代书的版本： 当然也会有些文档，手册，标准，是没有作者的，那就用文档手册标准的名字开头： 二、文章名称     通常作者名称列完后，紧跟其后的是文章名称，用双引号引起来： 如果是引用一本书，那么双引号里的可以是章节的名称（“Title of chapter in the book,” in Title of His Published Book），上图就是这样的，那如果没有章节名称就直接跟书名(Title of His Published Book)就好： 书名得是斜体。         一样得是斜体的情况有：    [Conference Proceedings] “Title of paper,” in Abbreviated Name of Conf.     [Conference Proceedings Online] “Title of paper,” in Abbreviated Name of Conf.     [Online Dataset Reference] Title, Source, Date     [Handbooks] Name of Manual/Handbook, x ed. , Abbrev. Name of Co. ,     [Manuals/Software(Manual (Online))] J. K. Author (or Abbrev. Name of Co. , City of Co. Abbrev. State, Country).  Name of Manual/Handbook     [Manuals/Software(Manual (Print))] Name of Manual/Handbook, x ed. , Abbrev. Name of Co. , City of Co. , Abbrev. State, Country     [Manuals/Software(Software)] Title of Software. (version or year), Publisher Name.     [Online Video] Video Owner/Creator, Location (if available). Title of Video: In Initial Caps.     [Periodicals] J. K. Author, “Name of paper,” Abbrev. Title of Periodical     [Periodicals(Periodical in Other Language)] J. K. Author, “Name of paper,” (in Language), Abbrev. Title of Periodical     [Periodicals(Virtual Journal)] Name(s) of Ed(s). , “Title of Issue,” in Title of Journal,     [Standards] Title of Standard, Standard number, Corporate author, location, date.     [U. S. Government Documents] Legislative body. Number of Congress, Session. (year, month day). Number of bill or resolution, Title.      不是斜体的情况有：     [Conference Paper] “Title of paper,” presented at the Abbreviated Name of Conf.       [Conference Paper Online] J. K. Author. (Date). Title. Presented at Abbreviated Conf.    这个值得注意的是Author和Title中间会有一个Date，另外不像其他，每个子项是用逗号分隔的，这个每个子项是用句点分隔的。     [Course] Name of University. (Year). Title of course  这个格式有点像上面。     [Coursepack] J. K. Instructor. Title of coursepack. (Semester). Title of course.     [Datasets] Author.  “Title. ” (Date, Year). Distributed by Publisher/Distributor     [Lectures] J. K. Author. (Year). Title of lecture     [Patent] J. K. Author, “Title of patent,” Country Patent xxx,     [Patent Online] Name of the invention, by inventor’s name.     [Reports] J. K. Author, “Title of report,” Abbrev. Name of Co. , City of Co. , Abbrev. State, Country,     [Report Online] J. K. Author, “Title of report,” Company, City, State, Country,     [M. Theses (B. S. , M. S. ) and Dissertations (Ph. D. )] J. K. Author, “Title of thesis,” M. S. thesis/Ph. D. dissertation, Abbrev. Dept. , Abbrev. Univ. , City of Univ. , Abbrev. State, year.     [Unpublished] J. K. Author, “Title of paper,” unpublished.      这里记一下Conference Proceedings和Conference Paper的区别： ” The Conference Proceedings reference type is best used for unpublished proceedings. Articles that are published as part of the comprehensive conference proceedings should be entered as Conference Paper references. ”     这里的理解是：已经出版的Proceedings是Paper，没有出版的是Proceedings。这里我自己是不太会分辨的，一般找到的citation是proceedings就当proceedings，是paper就当paper。     另外再记一个关于缩写的问题，Reference哪些部分要缩写呢：    [Book] Abbrev. of Publisher     [Conferences and Conference Proceedings] Conference的名称要缩写，这个是重点，不缩写可能会挨骂，而且最好省略大多数的冠词和介词，这个不省略可能不会被骂，可能(经验不丰富，没有参考价值)，举个例子Proceedings of the 1996 Robotics and Automation Conference 可以被缩写成 Proc. 1996 Robotics and Automation Conf.  看到Proc就要条件反射地知道要斜体，不然也可能要挨骂。Conference名称中出现下列单词时，按下图进行缩写(下图的缩写请一定遵守)：     更多缩写方式见参考[1]的附录A。     州名缩写：Abbrev. State     机构名缩写：Abbrev. Name of Co.     月份缩写：Abbrev. Month，月份缩写通常显示前三个字母，比如Jul.     期刊名缩写：Abbrev. Title of Periodical     大学院系名和大学名缩写：Abbrev. Dept. , Abbrev. Univ.  常见符号的意思：  et al.  指多个作者 Eds. 指多个编辑 Ed. 指一个编辑 ed. 书的版本 ch. 章节 sec. 段落 pp. 从哪页到哪页 p. 如果只有一页 vol. 哪一卷 no.  序号 Art. no.  期刊的文章号码 doi: 数位识别码，有doi的话把doi加到http://dx. doi. org/后面，就可以下载这个文章，这个通常也是放在索引的最后，doi和online有一个就行，都没有也没关系。doi前面一般是逗号隔开。三、Latex bib 参考文献类型：  article: magazine （指刊登各种文章、小说、诗歌、评议的杂志。） 或 journal （本义为“日报”，现在可指定期发生的周刊、月刊、季刊等或指学术团体出版的刊物、杂志或学报）上的文章 book: 出版的书 booklet: 已经发布的，但是没有出版商或机构的作品 conference: conference proceedings 里的文章 inbook: 引用书的一部分 (section, chapter, 等等) incollection: 书中带独立标题的章节 inproceedings: conference proceedings 里的文章 manual: 技术文档 masterthesis: 硕士论文 phdthesis: 博士论文 proceedings: conference proceedings 里的文章 techreport: 机构发布的报告 unpublished: 还没有正式出版的文章，或书 misc: 不属于以上任何一类最后一些Tips：  年份是不可缺少的，每样东西都有发布的年份。时间是很重要的信息。 如果有网上的资源链接，可以用[Online]. Available: http://www. web. com这种方式显示在最后。有时候可以没有[Online]。参考[1]还附有一些有用的附录：     A. Reference的缩写指引，哪个单词该用什么样的缩写都列在了这个表格。     B.  出版商名单（出版商名称中括号和括号里的部分是不用显示在Reference里的）     C. 非英文期刊的缩写 参考： [1] IEEE REFERENCE GUIDE，2018 [2] Preparation of Papers for IEEE Signal Processing Letters (5-page limit)，2015 "
    }, {
    "id": 33,
    "url": "http://localhost:4000/%E7%BE%8E%E8%82%A1%E8%B5%84%E6%96%99%E6%94%B6%E9%9B%86(Python)/",
    "title": "美股资料收集(Python)",
    "body": "2021/08/02 -  声明：  这篇笔记和参考均不构成任何投资建议哦。   本来这应该是一篇读书笔记，要翻译整理课程‘Nick McCullum, Algorithmic Trading Using Python, Nick McCullum，freeCodeCamp. org, 2021’，但后来写成了美股数据收集。这篇会比较适合有编程基础的读者，有什么不准确的地方，欢迎路过的都是大佬的大佬们指正。     首先算法交易（Algorithmic Trading）的意思就是用计算机，产生投资决策。     来看几家精于此道的公司[1]：  Renaissance Technologies: $165B in AUM (assets under managerment) AQR Capital Management: $61B in AUM Citadel Securities: $32B in AUM    算法交易的步骤：    数据收集     生成策略     策略测试     投入使用    这篇只涉及数据收集      参考[1]课程作者分享了一个列举了众多API的链接： https://github. com/public-apis/public-apis    我们需要注意的是这个链接里Finance目录下的IEX Cloud，视频课程中的实验数据是从这个接口调取的。简单说一下IEX Cloud这个工具，首先，它是个付费工具，个人级一个月9刀，描述说是各种金融相关的数据都能从它那儿调取到。另外课程里没有用付费通道，用的是测试通道。而这篇译文里会从雅虎金融拉取数据，是真实免费数据。     视频课程[1]完成了三个项目：  Equal-Weight S&amp;P 500 Screener (选大公司)  Quantitative Momentum Screener （选最近涨得多的） Quantitative Value Screener (选性价比高的)     三个项目的代码链接： https://github. com/nickmccullum/algorithmic-trading-python    这里就不详细说这三个项目了，视频是非常适合无编程基础的人看的，有编程基础的直接看github上的代码就能懂。 数据收集       首先我们看几个抓取股票清单的方式和方法。     比如我们要抓取S&amp;P 500的股票清单。     测试有效的有以下两种，第一种[2]： import requestsimport pandas as pdurl = 'https://www. slickcharts. com/sp500'headers = { User-Agent  : 'Mozilla/5. 0 (Windows NT 10. 0; Win64; x64) AppleWebKit/537. 36 (KHTML, like Gecko) Chrome/83. 0. 4103. 116 Safari/537. 36'}request = requests. get(url, headers = headers)data = pd. read_html(request. text)[0]stk_list = data. Symbolsp500 = set(list(stk_list. values))    第二种，维基百科[3]： import osimport bs4 as bsimport pickleimport requestsresp = requests. get('http://en. wikipedia. org/wiki/List_of_S%26P_500_companies')soup = bs. BeautifulSoup(resp. text, 'lxml')table = soup. find('table', {'class': 'wikitable sortable'})tickers = []for row in table. findAll('tr')[1:]:  ticker = row. findAll('td')[0]. text. split( \n )[0]    tickers. append(ticker) sp500 = set(tickers)    测试了一下，这两种方式取得的结果是一致的。     另外推荐一个雅虎金融上的screener功能，使用方法可以看[2]，这里贴一段用python抓取近一日交易量最多的100只美股的代码： import pandas as pdimport requestsurl = 'https://finance. yahoo. com/screener/predefined/most_actives?count=100&amp;offset=0'headers = { User-Agent  : 'Mozilla/5. 0 (Windows NT 10. 0; Win64; x64) AppleWebKit/537. 36 (KHTML, like Gecko) Chrome/83. 0. 4103. 116 Safari/537. 36'}request = requests. get(url, headers = headers)data = pd. read_html(request. text)[0]# 欄位『Symbol』就是股票代碼ma100 = list(data. Symbol. values)    接下来看一下怎么获取股票最近的交易信息，包括(Date, High, Low, Open, Close, Volume, Adj Close) [3]： import datetime as dtimport pandas_datareader. data as webstart = dt. datetime(2021, 7, 20)end = dt. datetime. now()# 获得单只股票最近信息填股票代码就行APPL_df = web. DataReader('AAPL', 'yahoo', start, end)# 想同时获取多只股票交易信息可以直接填list# 雅虎金融中股票代号中的'. '用'-'号取代sp500 = { item. replace('. ', '-') for item in sp500}sp500_df = web. DataReader(list(sp500), 'yahoo', start, end)    这里另外介绍一些好用的API[2]: pip install yfinance #Yahoo Finance python API 主要数据来源pip install fredapi #FRED python API，总体经济数据pip install pytrends #Google Trends python API 股票关键字搜索流行度    从雅虎金融上获取股票的一些基本资料[2]： import yfinance as yfstock = yf. Ticker('aapl')# 获取公司资料stock. info# 获取损益表（测试发现可以获取近四年的）stock. financials# 获取资产负债表stock. balance_sheet# 现金流表stock. cashflow# 取得价量资料+股利发放资料+股票分割资料# 测试APPL，最早可以获取到1980年12月12日的资料stock. history(period = 'max')         从FRED (Federal Reserve Economic Data) 取得总体经济状况的资料[2]： from fredapi import Fredimport requestsimport numpy as npimport pandas as pdimport datetime as dt# 注册账号：https://research. stlouisfed. org/docs/api/api_key. html# 然后找到 Request API key 获取API keyapi_key = '填入你的API'# 获得FRED资讯的大分类，比如：National Accounts - GDP (Eurostat)r = requests. get('https://api. stlouisfed. org/fred/releases?api_key='+api_key+'&amp;file_type=json', verify = True)full_releases = r. json()['releases']full_releases = pd. DataFrame. from_dict(full_releases)  # 寻找自己感兴趣的大分类# 比如我们要找的大分类是 National Accounts - GDP (Eurostat)# 通过下面代码，我们会知道National Accounts - GDP (Eurostat)的ID是267search_keywords = 'gdp'search_result = full_releases. name[full_releases. name. apply(lambda x: search_keywords in x. lower())]# 接下来看大分类下面有哪些小分类# 第一个参数是大分类的ID，limit限制的是小分类的数量，排序按popularity，降序排序series_df = fred. search_by_release(267, limit = 10, order_by = 'popularity', sort_order = 'desc')# 假如我们感兴趣的是英国国内生产总值:Real Gross Domestic Product for United Kingdom# 对应编号为：'CLVMNACSCAB1GQUK'，确定起止时间，即可获得数据。fred. get_series('CLVMNACSCAB1GQUK', observation_start = '2000-01-01', observation_end = dt. datetime. today())         从Google Trends获取关键字搜索量资料： from pytrends. request import TrendReqimport pandas as pdimport numpy as npimport datetime as dt# 首先指定时区，国内访问需要额，大家懂的# 不用代理 pytrends = TrendReq(hl='en-US', tz=360)# 用代理 (没跑通，跑通的大佬教下我)pytrends = TrendReq(hl='en-US', tz=360, timeout=(10,25), proxies=['https://34. 203. 233. 13:80',], retries=100, backoff_factor=0. 1, requests_args={'verify':False})    以上没跑通，跑通的大佬教下我。     用其他方法调通了一个，代码如下，有点粗糙，具体原理和稍微细致一点的代码及解说请参考[4]： # 时区参数设置，测试了AAPL在en-US, 360和zh-CN, -480两种配置下的结果，是一致的hl= 'zh-CN' # en-UStz = '-480' # 360# 设置想拉取数据的时间区间period = '2020-12-31 2021-05-22'# 设置想确定搜索热度的关键字keyword =  AAPL headers = {'user-agent': 'Mozilla/5. 0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/537. 36 (KHTML, like Gecko) Chrome/80. 0. 3987. 163 Safari/537. 36', 	'x-client-data': 'CIu2yQEIo7bJAQjEtskBCKmdygEIy67KAQjQr8oBCLywygEIl7XKAQjttcoBCI66ygEYx7fKAQ==',	'referer': 'https://trends. google. com/trends/explore?date=today%201-m&amp;q=bitcoin,blockchain,eth',	'cookie': '__utmc=10102256; __utma=10102256. 31392724. 1583402727. 1586332529. 1586398363. 11; __utmz=10102256. 1586398363. 11. 11. utmcsr=shimo. im|utmccn=(referral)|utmcmd=referral|utmcct=/docs/qxW86VTXr8DK6HJX; __utmt=1; __utmb=10102256. 9. 9. 1586398779015; ANID=AHWqTUlRutPWkqC3UpC_-5XoYk6zqoDW3RQX5ePFhLykky73kQ0BpL32ATvqV3O0; CONSENT=WP. 284bc1; NID=202=xLozp9-VAAGa2d3d9-cqyqmRjW9nu1zmK0j50IM4pdzJ6wpWTO_Z49JN8W0s1OJ8bySeirh7pSMew1WdqRF890iJLX4HQwwvVkRZ7zwsBDxzeHIx8MOWf27jF0mVCxktZX6OmMmSA0txa0zyJ_AJ3i9gmtEdLeopK5BO3X0LWRA; 1P_JAR=2020-4-9-2'}# 获取token的链接url1 = 'https://trends. google. com/trends/api/explore?hl={}&amp;tz={}&amp;req={{ comparisonItem :[{{ keyword : {} , geo :  , time : {} }}], category :0, property :  }}&amp;tz={} '. format(hl,tz,keyword,period,tz)r = requests. get(url1, headers=headers,timeout=15)data = json. loads(r. text[5:])req = data['widgets'][0]['request']token = data['widgets'][0]['token']# 获取数据的链接url2 = 'https://trends. google. com/trends/api/widgetdata/multiline?hl={}&amp;tz={}&amp;req={}&amp;token={}&amp;tz={}'. format(hl, tz, req, token, tz)r = requests. get(url2)# 最后的结果会以一张表显示if r. status_code == 200:  data = pd. DataFrame. from_dict(json. loads(r. text. encode(). decode('unicode_escape')[6:])['default']['timelineData'])         觉得有用就点个在看哦，点得多的话同类型文章会接着写哦。     另，文章中有任何表述不恰当的地方，欢迎指正。 参考： [1].  Nick McCullum, Algorithmic Trading Using Python, Nick McCullum，freeCodeCamp. org, 2021 [2]. AI StockBoy, 用 Python 打造自己的股市資料庫 — 美股篇, Medium, 2019 [3].  万能的小草，Python在Finance上的应用6 ：获取是S&amp;P 500的成分股股票数据，腾讯云，2020 [4]. 编程学习笔记，批量爬取Google Trends的日频数据，实现EXCEL实时存储，CSDN，2020 "
    }, {
    "id": 34,
    "url": "http://localhost:4000/UC-Berkeley%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-Self-Supervised-Learning/",
    "title": "自监督学习（Self Supervised Learning）",
    "body": "2021/05/15 -     今天来说一说自监督学习，根据Yann LeCun的报告，这是一类可以预测未来，回溯过去，补缺查漏的算法，是不是很有吸引力。     首先我们来了解一下这位科学家 Yann LeCun，看人先看脸，上图：      实打实的牛人，博士期间，发明了反向传播算法的原型，在贝尔实验室的时候发明了卷积神经网络。     监督学习和强化学习的有效性就不重复了，说一下局限性，监督学习需要大量标注好的数据，成本是巨大的。强化学习目前需要和世界进行大量的互动，但是研究者发现，婴儿并不是这么学习的，婴儿在初期只能通过观察学习，而单单通过观察，便可以构建很多基础能力。         上图是一个婴儿的月份和他的能力的对照，可以发现，七八个月后他才能抓爬，而在这之前他已经掌握了一系列复杂的能力。     那如果机器只依靠观察来学习呢？AlphaStar在学习如何打星际争霸这个游戏的时候，只是观察，也就是说输入只有游戏的视频数据，模型用了至少200年的数据达到了攻克99. 8%活跃玩家的成绩。     为什么机器需要这么多数据，但人类却不需要？         这个，额，人类自己也不是很清楚。         那么，今天就让我们一起来看看，也许会让我们更接近答案，更接近强人工智能的一类算法，自监督学习（Self Supervised Learning）。     什么是自监督学习，来看看Yann LeCun的描述吧：     是一种可以通过数据的一部分来预测其他部分的方法，这当中包括，预测未来，回溯过去，补缺查漏，补齐空间等等。         Yann LeCun对自监督学习的定位是这样的：     如果把机器学习能获取的信息比喻成一块蛋糕，那么强化学习相当于是这个蛋糕上的樱桃，监督学习相当于这个蛋糕里的冰块，自监督学习相当于这个蛋糕的主体。这是很高的评价和期望了。     接下来看一些自监督学习的案例：     1. 去噪     在这个案例中，输入是有噪声的图片，经过一个编码器，将图片转化成存储了这张图片有用信息的特征向量，再通过一个解码器，即可得到除去噪声后的结果。     这样一个应用除了去噪的功能外，图中间的特征向量，可以用来完成其他任务，比如判断图片里面有没有小狗之类的。而去噪这样的一个过程，比起直接将原图编码解码，能得到更好的特征向量，即特征向量中的信息能更好地表达原图的有用信息。     2. 补缺     一张图，抠掉中间区域，让模型预测抠掉的部分。     和去噪网络的区别是，特征向量分了两种出来，一个是编码特征向量，一个是解码特征向量，这样设计的理由应该是考虑到要预测的图片和输入的图片信息是不一致的。     在这个应用中，损失函数的选择对结果的影响很大，下图是不同损失函数产生的结果：     最后Joint Loss应该是L2 Loss和Adversarial Loss组合产生的Loss，具体可能要去参考Pathak et al在2016年发表的论文。     3. 着色     得到一张黑白图片，计算机可以给它自动上色吗？     4. 拼图      这有点像让机器人学习玩人类的拼图游戏，把一张图片切割成9份，然后打乱了输入模型，让模型输出这9张小图在原图中的位置。     5. 旋转判断     判断一张图是否是旋转过的图片，以及旋转了多少度。          6.  视频着色     给一张彩色帧，然后根据这个彩色帧，给一整个视频上色。     以上都是一些图像相关的自监督学习，接下来说一些自然语言处理相关的自监督学习。     1. Word2Vec     图像处理里，很多自监督学习是把图片转成向量，在自然语言处理里，则是将词转换成向量，比较典型的CBOW和Skip Gram，都是利用词的位置关系来进行训练。这里举个例子（这里只做了概念性的一些解释）：     比如 “我爱你。”这句话，CBOW是给出“我”和“你”，让模型预测“爱”这个字，而Skip Gram和CBOW是镜像对照的，给你一个“爱”字，希望模型能预测出“我”和“你”。我们可以思考，为什么这种方式，能得到每个词的大意（用向量的方式表达了出来）。这样训练后，每个词都可以用一个向量来表示，比如，“爱”的最后的向量是[0. 9, 0. 7, 0. 5]，“恨”的向量是[0. 1，0. 7，0. 5]，有了这个计算机才能做计算，比如计算“爱”和“恨”的距离，[0. 8, 0, 0]，计算机才会知道，“爱”和“恨”，哪里相同，哪里不同。     再比如下面这幅图，左边是国家，右边是每个国家对应的首都，可以发现各个国家和自己首都之间的距离是差不多的。          接下来介绍三个比较新，且在机器视觉领域带来了比较大突破的自监督学习的算法，分别是CPC，MoCo，和SimCLR，其中CPC和MoCo都有v2版本，且都可以跨领域应用起来。     1.  CPC (Contrastive Predictive Coding)     现在有一段语音，给了前半段，怎么预测后半段？     CPC在训练的时候，会把用来预测的语音段和希望预测出来的语音段用同样的编码器编码，分别得到特征向量，再在训练过程中通过调整编码器，使得两边得到的特征向量最大可能的一致。可以理解为，训练一个编码器，它能提取当下信息中可以用来预测未来的信息。     训练的时候会有正样本和负样本，真正的未来是正样本，其他片段都是负样本，负样本可以来自于同一段语音，也可以来自不同语音，这可以根据下层任务来做调整，训练的基本目标就是拉近与正样本的距离，拉大和负样本的距离。     CPC可以有很多变形，比如结合时间序列模型来预测，再比如可以根据未来预测过去等等。     而CPC在分辨说话人等下层任务的表现上，只比监督学习差了一点点。         接下来看CPC在视觉领域的做法，其实就是把时间这个维度换成了空间。        切割图片，按50%的重叠，切割出来的效果如下图：     切完了，用encoder将每个分块都转换成特征向量，再由上三层的特征向量去预测下三层。     CPC同样可以用于自然语言处理和强化学习，这里就不细说了。     CPC后来又做了改进，有了CPC v2，CPC v2的效能就很好了，下图中，红线是只用监督学习的效能，蓝线是先用CPC v2，再用监督学习的效能，可以发现，当你只有少量标注数据的时候，CPC v2可以为你的分类器的效能带来非常大的提升。     2. MoCo (Momentum Contrast)      MoCo和CPC在网络架构上的最大不同是，MoCo用了一个Momentum encoder，即编码比较项的时候，用的是不同的编码器，且在训练过程中，这个编码器是根据编码query项的编码器调整的。调整方式如下图所示：     另外，还增加了一个记忆区，存储负样本的编码，这样可以让query和更多的负样本进行对比，不用受batch size的限制，这部分由下面代码可以看出。     3. SimCLR     Sim是Simple的意思，这个架构就比较简单了，去掉了query的概念，一个样本分别做不同的随机改变，经过编码器编码得到特征向量，再通过网络得到用来对比的特征向量，目标就是要让出自同一个样本的对比向量尽量一致。     这里值得一提的是，SimCLR的效果比MoCo要好，但MoCo的团队很快就给出了回应，改进了MoCo，于是有了MoCo v2，效果又超过了SimCLR，看来大家都是倔强的王者。         今天就到这里啦，觉得有用，就帮小的点个在看再走吧，大吉大利，恭喜发财！ 注：所有图片，公式，理论，论点都来自参考文献，本文仅是整理笔记。 另：很多知识点理解可能不到位或者有偏差错漏，还请指正，讨论，谅解。 参考： [1] Yann LeCun, Self Supervised Learning, ICLR, 2020 [2] Thalles Silva, Self-Supervised Learning and the Quest for Reducing Labeled Data in Deep Learning, toward data science, 2020 [3] Lecture 7 Self-Supervised Learning，UC Berkeley Spring 2020 CS294-158，Deep Unsupervised Learning "
    }, {
    "id": 35,
    "url": "http://localhost:4000/MIT%E5%9B%A0%E6%9E%9C%E8%BF%B7%E4%BD%A0%E8%AF%BE%E7%AC%94%E8%AE%B0-%E5%9B%A0%E6%9E%9C%E5%BD%92%E7%BA%B3%E5%92%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BDomain-Adaptation/",
    "title": "MIT因果迷你课笔记 — 因果归纳和机器学习之Domain Adaptation",
    "body": "2021/03/20 - 系列首篇：MIT因果迷你课笔记 —— 相关和因果 上篇：MIT因果迷你课笔记 — 因果归纳和机器学习之强化学习     这是这门课最后一部分的内容，因果归纳和机器学习。     总共分四个部分，这是第四部分，讲因果归纳和Domain Adaptation。     先理解 Multi-task Learning （MTL）和 Transfer Learning 的概念。     （引用自参考[2]） Multi-task Learning （MTL），只专注于单个模型可能会忽略一些相关任务中可能提升目标任务的潜在信息，通过进行一定程度的共享不同任务之间的参数，可能会使原任务泛化更好。广义的讲，只要loss有多个就算MTL。     （引用自参考[3]）Transfer Learning的初衷是节省人工标注样本的时间，让模型可以通过已有的标记数据（source domain data）向未标注数据（target domain data）迁移。从而训练出适用于target domain的模型。举个例： 上图是某行人检测任务数据集当中的4张图片，假设前两张正对着摄像机的行人作为训练集，后两张背对着的行人图片作为测试集，结果该模型的测试评分会很差，因为训练时没有考虑到摄像机观察角度引起的问题，类似的在图像识别领域有很多因素会降低识别率（例如光照，背景等）。那能否用一些未标记的图片（类似图3，4这样的图），增强我们的行人检测模型，让它不仅可以识别正对着的行人，还可以识别背对着的行人？这就是迁移学习要干的事。     由上面的概念可知，MTL的训练数据来自不同的域（domain），而应用的场景可能只是训练数据中的其中一个域。TL的训练数据来自不同的域，而应用的场景是训练数据中没有的域。     现在有一个假设：     这个假设叫做Coverage Shift Assumption（注：因为视频和课件中均没有出现文字版的名字，不知道这三个单词我打对了没）。这个假设是说对于所有域，以X为条件的Y的分布是一致的，如上式所示。      首先说明，这只是一个假设，不是完美的。另外，从因果的角度来检讨，也有些不那么自然的地方。因为会影响Y的是Y的因，而显然X中不是所有特征都是Y的因。     那如何根据上面的假设传递知识：     上面公式中1,2,3,4,5这些可以看成是不同domain的编号，也就是说有些域和Y之间的关系是已知的（比如有打过标签的训练数据，训练过），但有些域和Y之间的关系是不知道的（比如有些域的数据没有打过标签，没有参与训练模型），那如何从已知的Y和一些域的关系去推导Y和另外一些域的关系？        上图是授课老师展示的一个方案，这里就不细说了，因为他没细说，我也没明白。     另外一个原因是Domain Adaptation还在概念的阶段，不是一个成熟的课题，所以这里的主要任务是抛出问题，引发思考，感兴趣的可以深入了解一下。     到这里，这门因果课就结束啦，下面是授课老师推荐的一些相关的书，可供参考：     最后再回头说一说进入这个领域的初衷（改自课程老师在该课程最后十分钟说的一段话），我想对这个领域感兴趣的大多数人，都是人工智能领域的学者，现在深度学习大热，所有问题都开始用端到端的方式解决了，也取得了很好的效果。但是我们仔细想一下，知道这其中是有问题的，问题在于真正的智能，比如我们人类，解决问题不都是端到端的，人类擅长总结，概括，类比，推理，归纳因果，甚至进行想象，反事实等，我们下围棋不一定能赢过机器人，但我们下围棋的策略，是根据游戏规则，自身经验，推理而来的，再比如人类归纳因果也不总是对的，但我们的行为背后总有动机在支撑。这是现在的人工智能无法做到的。虽然目前因果归纳并没有在人工智能领域取得什么好的成果，但仍有很多学者和科学家坚信，在人工智能领域加上因果归纳，也许会让我们更接近强人工智能。     完结，撒花，希望所有的时间都没有白费，期待下一段旅程，觉得有用的话，右下角帮忙点个‘在看’哦！恭喜发财，感激不尽。 系列首篇：MIT因果迷你课笔记 —— 相关和因果 上篇：MIT因果迷你课笔记 — 因果归纳和机器学习之强化学习 记：公式上上下下有不匹配的地方，因为截自不同的参考文献，所以写法会有出入，打公式太累了，我放过了自己。 声明：所有图片均来自参考，没有原创图片，公式和定理。 参考： [1] Jonas Peters, University of Copenhagen, Mini-course on Causality, Laboratory for Information &amp; Decision Systems (LIDS) and Models, Inference &amp; Algorithms of the Broad Institute, MIT, 2017 [2] Anticoder,  Multi-task Learning(Review)多任务学习概述，2019 [3] Xf Mao，什么是迁移学习 (Transfer Learning)？这个领域历史发展前景如何？，2017 "
    }, {
    "id": 36,
    "url": "http://localhost:4000/MIT%E5%9B%A0%E6%9E%9C%E8%BF%B7%E4%BD%A0%E8%AF%BE%E7%AC%94%E8%AE%B0-%E5%9B%A0%E6%9E%9C%E5%BD%92%E7%BA%B3%E5%92%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/",
    "title": "MIT因果迷你课笔记 — 因果归纳和机器学习之强化学习",
    "body": "2021/03/19 - 系列首篇：MIT因果迷你课笔记 —— 相关和因果 上篇：MIT因果迷你课笔记 — 因果归纳和机器学习之half-sibling regression     这是这门课最后一部分的内容，因果归纳和机器学习。     总共分四个部分，这是第三部分，讲因果归纳和强化学习。     这部分有三个案例：     一：肾结石案例     这个案例之前提到过，先回顾一下：     肾结石的两个治疗方案：     肾结石有两个治疗方案，A和B，如果看总的治愈率会发现，B方案明显好于A方案。但是肾结石的石头又可分为大小两种情况，从这两种情况的治愈率来看，A方案又明显好于B方案，造成这种结果的原因是，治愈率不但和治疗方案有关，还和结石大小有关，A方案的治愈率虽然高，但主要用在了治愈率较低的大结石，而B方案则主要用在了治愈率较高的小结石，因此，简单地比较总的治愈率，对A方案来说是不公平的。治愈率，结石大小，和治疗方案的因果关系可以用下图表示：     公式化上图，得：      P(r,t,s)可以按上面公式展开，其中p(r|t,s) = R = g(S,T,NR)，p(t|s) = T = f(S,NT)，现在想对治疗方式进行干预，那分布该如何变化：     也就是说，如果想干预治疗方式，那么改变的是治疗方式的分布函数，如果想干预治愈，那么改变的是治愈的分布函数。可以用这种方式去求解治疗方式的最佳选择。     二：Blackjack     这是一个扑克牌游戏，又名21点，源于法国，现已流传到世界各地。规则如下：     该游戏由 2 到 6 个人玩，使用除大小王之外的 52 张牌，游戏者的目标（Goal）是使手中的牌的点数之和不超过 21 且尽量大。     每张牌都有点数， 2 到 10 的牌的点数就是其牌面的数字；J 、 Q 、 K 的点数是 10；A 有两种算法， 1 或者 11 ，如果 A 算为 11 时总和大于 21 ，则 A 算为 1 。例如 （ A, 8 ） 是 19 点，（ A, 7, J ） 则为 18 点。     首次发牌（Dealing）每人2张牌。庄家以顺时钟方向向众玩家家派发一张暗牌（即不被揭开的牌），然后向自己派发一张暗牌，接着庄家会以顺时钟方向向众玩家派发一张明牌（即被揭开的牌），之后又向自己派发一张明牌. 　当众人手上各拥一张暗牌和一张明牌，庄家就以顺时钟方向逐位向玩家询问是否再要牌（Hit）（以明牌方式派发）。在要牌的过程中，如果玩家所有的牌加起来超过21点，玩家就输了（俗称爆煲Bust），游戏也就结束了。该玩家的注码归庄家。     如果玩家无爆煲，庄家询问完所有玩家后，就必须揭开自己手上的暗牌。若庄家总点数少于17点，就必须继续要牌；如果庄家爆煲，便向没有爆煲的玩家，赔出该玩家所投注的同等的注码。如果庄家无爆煲且大于等于17点，那么庄家与玩家比较点数决胜负，大的赢（例牌除外）。一样点数为平手，各自取回赌注。     停牌（STAND）：不再拿牌。在任何情况下，玩家可选择停止要牌。     分牌（SPLIT）：玩家再下一注与原赌注相等的赌金，并将前两张牌分为两副单独的牌。这两张牌的点数必须相同（即一对8、一对K 或一对Q，某些玩法中两张10点的牌如一张10一张Q时也可分牌）。但分牌后的黑杰克，只能作普通21点计算，其赔率只是1 赔1 。     双倍下注（DOUBLE或DOUBLE DOWN）：玩家在拿到前两张牌之后，可以再下一注与原赌注相等的赌金（如果觉得少可以加倍），然后只能再拿一张牌。如果拿到黑杰克，则不许双倍下注。（部分玩法中拿到3张或3张以上也可选择双倍下注，但同样只能再拿一张牌）     **例牌（BlackJack**）：如果玩家手中的一张暗牌和一张明牌分别是一张A牌（可作11点）和一张十点牌（K、Q、J、10），这副牌叫做二十一点，例牌（BlackJack）。该玩家可向庄家报到，庄家须向该玩家赔上1倍（有些，1. 5倍）注码。 举例：     图中最左边的玩家有两手，庄家点数是17，这个玩家的两手分别是16和21，毫无疑问21这手赢了庄家，中间超过21点，爆煲，输了，(J，K)加起来20，赢了，最右边16，平了。     那么，如何取得这个游戏的最佳策略呢？     首先，采用随机策略，进行一些游戏，取得一些样本数据p = p(X，Y，Z)。X：当前的状态，比如玩家手里的牌或所有玩家手里的牌，桌面上出现过的牌，庄家总共用了几副牌等等，Y：对当前状况采取的策略。l = l（X，Y，Z）是收益方程，p策略，即知道了现在的状况后，为了争取最大收益或最小损失，选择的策略，如拿牌，停牌，分牌等，p则是用样本优化后我们求的策略。Epl表示在这种策略下的期望收益或损失。       期望收益的计算可以如下：     这里的每一步推导就不展开说了，我们想得到的是p(y|x)，已知的是p(y|x)，我们希望用p(y|x)替换p(y|x)后，Ep*l的值能变大。     训练是一个循环往复的过程，刚开始的策略是随机策略，然后进行一些游戏后，用已有的样本进行策略优化，将优化的策略替换原本的策略，再进行一些游戏，进行这些游戏的时候，一部分策略用的是之前优化后的策略，但依然会保留随机性，即有一部分决策是随机的选择。再用新得的样本优化现用的策略，将优化后的策略替换原本的策略，如此反复，直到期望收益不再会因为这个过程提高。     这个游戏的因果结构如下（简单的）：     最后的输赢取决于你的策略，桌面上的明牌和暗牌。也可以在因果图中补充一些信息，比如，就这个游戏而言，输赢和牌的花色没有关系。     下图是增强算法在优化这个游戏策略上的表现：     接下来看最后一个例子：     三：广告推荐     当我们使用搜索引擎搜索一些东西的时候，或者做一些其他和网络互动的事情时，经常会被推送一些广告，如下图：     可以通过研究推送广告过程中的因果关系来优化模型： 用户的意图会决定用户的操作，并产生一些数据，而通过分析用户的数据，我们来决定给他推送什么样的广告，以及推送多少广告，而这些推送又会一定程度上决定用户的行为。     user intention指用户意图，user data指和用户相关的数据，main line reserve指保留多少及什么样的空间给广告，#ads in main line推荐的广告数量，click是用户的点击。这只是一个简单的广告推荐的因果图，广告推荐的因果图可以更复杂，比如加上广告的内容，类别等等。这样的推荐系统，也可以用强化学习的方式来做，保留一点随机性，可以发现更多的可能。我们最终想要的是更多的点击，而不是把广告推送出去。     下面对广告系统做了一点简单的实验：     （0，0）位置表征的是这个广告系统的初始状态，增加或减少广告区域，可以发现，减少广告区域可以增加广告的点击量。蓝色区域表示的是区间，因测试数据并不充分，因此只能表达出调整广告区域后点击量所在的区间。     由因果图可以发现，广告区域决定了广告的数量，而广告数量才是决定点击量的直接因素。那么如果直接用广告数量来进行实验，实验结果如何：     趋势还是那个趋势，区间大大缩小了。因此，准确地使用因果关系来分析问题，效果可能会更好。 系列首篇：MIT因果迷你课笔记 —— 相关和因果 上篇：MIT因果迷你课笔记 — 因果归纳和机器学习之half-sibling regression 记：公式上上下下有不匹配的地方，因为截自不同的参考文献，所以写法会有出入，打公式太累了，我放过了自己。 声明：所有图片均来自参考，没有原创图片，公式和定理。 参考： [1] Jonas Peters, University of Copenhagen, Mini-course on Causality, Laboratory for Information &amp; Decision Systems (LIDS) and Models, Inference &amp; Algorithms of the Broad Institute, MIT, 2017 [2] cplwsy,  21点Blackjack玩法，百度经验，2017 "
    }, {
    "id": 37,
    "url": "http://localhost:4000/MIT%E5%9B%A0%E6%9E%9C%E8%BF%B7%E4%BD%A0%E8%AF%BE%E7%AC%94%E8%AE%B0-%E5%9B%A0%E6%9E%9C%E5%BD%92%E7%BA%B3%E5%92%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8Bhalf-sibling-regression/",
    "title": "MIT因果迷你课笔记 — 因果归纳和机器学习之half-sibling regression",
    "body": "2021/03/18 - 系列首篇：MIT因果迷你课笔记 —— 相关和因果 上篇：MIT因果迷你课笔记 — 因果归纳和机器学习之半监督学习     这是这门课最后一部分的内容，因果归纳和机器学习。     总共分四个部分，这是第二部分，讲因果归纳和half-sibling regression。     先考虑一个关于系外行星的研究。我们有一个开普勒望远镜，以某个角度在观察宇宙，任务是发现系外行星。     该望远镜的观察范围是3000光年。         那么理想状况下，如何判断一个恒星周围有行星在绕行呢？观察这个恒星的光的强度，如果如上图所示，间隔固定时间就有一段时间光的强度降低了一些，那么可以认为，有一个行星在绕这个恒星运行。     但是，理想归理想，现实很骨感，现实中观察到的数据充满了噪声，无法用上面那种简单的逻辑来分辨出行星的踪迹。     接下来，我们分析一下这个案例的信息构成。     我们没有观察到的信息，由我们需要的恒星系统的光强及系统噪声组成。我们观察到的信息，由对该恒星系统的观察和对宇宙其他部分的观察组成。对该恒星系统的观察由该恒星系统的光强和系统噪声组成，其他观察由系统噪声组成。     这些信息又呈什么关系呢，因为系统噪音和true signal是来自不同星体的信息，且间隔很远，相互之间的影响可以忽略不计，因此true signal和系统噪音是相互独立的。因此观察到的信息中measurement和other measurements中的共同部分一定是系统噪音。     将上图公式化，得下图：     我们要求的是Q，怎么得到呢？移除Y中所有可以被X解释的部分。      上图中展示了三种方法，第一种就是找到X和f(N)之间的映射关系，计算出f(N)，解出Q，这无疑是最理想的一种方式。第二种，假设X中只有少量噪声。第三种，假设有多个观察，即多个X。     具体去噪方法可以参考最后一幅图中的参考文献，这里就不仔细展开了，之后的几幅图展示了一下实验效果：     横轴是时间，中间断裂的部分是仪器的观测机制造成的，纵轴是观测数据，第一幅图是原始数据，第二幅图是去噪后的结果，可以发现，是可以明确发现行星的踪迹的。下图也一样：      上面这幅图就不做具体解释了，感兴趣可以看图中参考文献。    系列首篇：MIT因果迷你课笔记 —— 相关和因果 上篇：MIT因果迷你课笔记 — 因果归纳和机器学习之半监督学习 记：公式上上下下有不匹配的地方，因为截自不同的参考文献，所以写法会有出入，打公式太累了，我放过了自己。 声明：所有图片均来自参考，没有原创图片，公式和定理。 参考： [1] Jonas Peters, University of Copenhagen, Mini-course on Causality, Laboratory for Information &amp; Decision Systems (LIDS) and Models, Inference &amp; Algorithms of the Broad Institute, MIT, 2017 "
    }, {
    "id": 38,
    "url": "http://localhost:4000/MIT%E5%9B%A0%E6%9E%9C%E8%BF%B7%E4%BD%A0%E8%AF%BE%E7%AC%94%E8%AE%B0-%E5%9B%A0%E6%9E%9C%E5%BD%92%E7%BA%B3%E5%92%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/",
    "title": "MIT因果迷你课笔记 — 因果归纳和机器学习之半监督学习",
    "body": "2021/03/17 - 系列首篇：MIT因果迷你课笔记 —— 相关和因果 上篇：MIT因果迷你课笔记 —— 基于不变性的因果预测（invariant causal prediction）     这是这门课最后一部分的内容，因果归纳和机器学习。     总共分四个部分，这是第一部分，讲因果归纳和半监督学习。     关于半监督学习（semi-supervised learning），介绍的人很多，比如参考[2]，这里就不专门仔细介绍了，大概就是，在有标签数据+无标签数据混合成的训练数据中使用的机器学习算法吧[2]。     现在我们需要回顾一下MIT因果迷你课笔记 —— 发现因果关系1中RCCP，d-separation和Makov概念。     我们认为一个图是Markov的，如果图中的变量满足：     这种时候，如果一个变量的因的值都知道了，那么它应该是独立于其他知道因的变量的：     我们可以想象一下变量x1和x2的方程式，应该是有因和噪声组成的，如果因知道了，那么就只剩下噪声了，当然就是互相独立的。     接下来看一个特殊案例：     知道了因的果和因是相互独立的。     再来看半监督学习，半监督学习感兴趣的是P(Y|X)，即给你X（比如图片），你判断Y（比如图片里有一只狮子）发生的概率是多少。而半监督学习除了通过标注的数据训练模型，还想通过未标注的数据优化模型。如果把图片是因，Y是果，根据上面的理论，P(X)和P(Y|X)相互独立，更多的未标注的图片P(X)并不能为P(Y|X)带来更多的信息量。         因此，根据上面的理论，半监督学习对于X和Y存在严格的因果关系的任务是无效的。接下来看一个实验：     横轴标记了不同的数据集，纵轴则标记了使用未标注的优化模型后，模型的表现，0表示模型准确度没有因为未标注的数据参与优化而变，负值表示准确度反而降低了，正值表示准确度提高了。这里分别介绍一下Anticausal，Confounded，Causal，和Unclear的意思，Anticausal表示数据集中X至少有一个特征是Y的果，Confounded表示X中至少有一个特征和Y有共同的因，这两个要表达的意思是P(Y|X)和P(X)不相互独立，Causal表示，X中只有Y的因，没有Y的果，即P(Y|X)和P(X)相互独立，Unclear很好理解，就是什么因啊果啊的，分不清楚，不知道P(Y|X)和P(X)是相互独立呢还是相关。由上面的实验可知，半监督学习对于标记为Causal的数据集是无效的，但对于很多标记为Anticausal和Confounded的数据集是有效的。     所以在做半监督学习之前，不妨先研究一下数据集的因果关系吧。     上图是数据独立性判断的一些机制，和判断因果有共通之处，感兴趣的具体可以参考2012年发表在ICML上的论文On causal and anticausal learning，这里就不详细说了。 系列首篇：MIT因果迷你课笔记 —— 相关和因果 上篇：MIT因果迷你课笔记 —— 基于不变性的因果预测（invariant causal prediction） 记：公式上上下下有不匹配的地方，因为截自不同的参考文献，所以写法会有出入，打公式太累了，我放过了自己。 声明：所有图片均来自参考，没有原创图片，公式和定理。 参考： [1] Jonas Peters, University of Copenhagen, Mini-course on Causality, Laboratory for Information &amp; Decision Systems (LIDS) and Models, Inference &amp; Algorithms of the Broad Institute, MIT, 2017 [2] 糯米稻谷，半监督深度学习小结，知乎，2018 [3] Schölkopf et al. , On causal and anticausal learning, ICML, 2012 "
    }, {
    "id": 39,
    "url": "http://localhost:4000/MIT%E5%9B%A0%E6%9E%9C%E8%BF%B7%E4%BD%A0%E8%AF%BE%E7%AC%94%E8%AE%B0-%E5%9F%BA%E4%BA%8E%E4%B8%8D%E5%8F%98%E6%80%A7%E7%9A%84%E5%9B%A0%E6%9E%9C%E9%A2%84%E6%B5%8B-invariant-causal-prediction/",
    "title": "MIT因果迷你课笔记 —— 基于不变性的因果预测（invariant causal prediction）",
    "body": "2021/03/14 - 系列首篇：MIT因果迷你课笔记 —— 相关和因果 上篇：MIT因果迷你课笔记 —— 因果归纳模型的评估方式（SHD和SID）     这篇是发现因果关系的最后一篇，也是这里要提的最后一个发现因果关系的方法。     之前讲过两种归纳因果关系的方法，第一种是基于独立关系的判断方法：         MIT因果迷你课笔记 —— 发现因果关系1     第二种是基于分布方程中，噪音的独立性的方法：         MIT因果迷你课笔记 —— 发现因果关系2(restricted structural causal model)     今天要说的是第三种：基于不变性的因果预测（invariant causal prediction）     如上图所示，假设我们现在有一个因果系统，不知道因果关系，但在不同环境下观测到了一系列数据，左边蓝色数据是在环境1中观测到的，右边红色数据是在环境2中观测到的。环境和环境之间的区别在于做了不同的干预。     现在我们想要知道Y的因是哪些。     首先我们用线性模型拟合：     Pr(&gt;|t|)是P-value，P值估计系数不显著的可能性，有较大P值的变量是可以从模型中移除的候选变量。由上述实验可知，线性拟合后，Y和X1，X2，X3，都有很小P值，无法根据这个实验找到Y的因。     那用这篇介绍的Invariant Causal Prediction (ICP)来实验呢：     ExpInd长度和Y一致，但其中数据表示的是环境编号，1指环境1，2值环境2，即用于ICP这个方法的数据，包含了不同环境下的数据。而得到的结果表明，X3是Y的因。     接下来我们来说说ICP基于的原理：     不变性原理：     如果Y的结构方程不变，那么无论如何进行干预，只要不干预Y，P(Y|PAY)保持不变。这个很好理解，Y的值由Y的因和噪音决定，那么如果确定了因的值，Y的分布也是确定的，且与图中其他变量是无关的。     这里列一些典型的干预：      1. 使X1只受特定噪音影响     2. 改变影响X4的噪音：     3. 对多个变量作出干预，如下： &lt;div align=center&gt;&lt;/div&gt;        这些干预都不能改变P(Y|PAY)。     当我们取得了不同干预（不同环境）下的数据后，对于任意环境下的数据，都有一致的P(Y|PAY)，前提是这些环境中不包括干预了Y的环境：     以上就是不变性原理。     那么怎么根据这个原理，来找到Y的PAY呢？     这才是我们这篇的主要任务。          方式是，穷举S，然后检测这个S是否会使得P(Y|S)在不同环境下一致。如果某个S使得P(Y|S)在不同环境下一致，那么S有可能就是PAY。当出现多个集合符合要求，即满足不变性原理时，取交集：     α指显著性水平。     举个例子，列举不同的S集合及不变性验证结果如下：     打勾的是符合不变性原则的，打叉的是不符合不变性原则的，上例中，{3,5}，{1，3，6}，{3，8}都有可能是PAY，取交集后得到{3}，则{3}有很高的概率是属于PAY(S*)的。     这种因果判断的方法，和进行不同干预的数量，即我们所能拥有的环境的数量，以及干预的效果关联很大，如果想让结果尽量的准确，得到的交集不是空集，且尽可能地接近PAY，那么增加环境的数量，保证干预的有效性以及准确性会非常重要。     干预也可以被看成是一个变量，如下图所示：     将干预看成是变量E，变量E影响了X2的值。     用ICP验证，很容易得到X2和X4是Y的因：     但如果X2和Y之间的关系是非线性的，但调用解决线性关系的代码，方程或软件包，就可能无法得到正确答案：     另一种会失败的情况是，干预了Y：     这里引入一个新的概念，hidden variables，隐藏变量，如下图中的X4，他确然会影响X2和Y的值，是X2和Y的因，但我们并不知道它的存在，或者无法对它进行干预，那么就会出现下面的情况。     得到的Y的因不是X2，而是X1，即X2的因。从某种程度上来说，也算是归因成功，因为X1是X2的因，因此X1其实是Y的间接因。所以这种情况下的也能在一定程度上完成归因的任务。     接下来来看看这个方法如何用在真实案例中：     以上是一个基因实验，总共有6170个基因，有160个数据观察点，也可以理解为有160个样本，图中横坐标显示的是基因5954的活性，纵坐标显示的是基因4710的活性，第二张图中显示，如果降低一个样本（红色点表示）中的基因5954的活性，那么基因4710的活性也会降低，由此可推断，基因5954是基因4710的因。     上图中中间这幅图，表示的是在既不干预基因5954，又不干预基因4710活性的前提下，对实验环境进行不同干预后，基因5954和基因4710的活性表达。在这个实验中总共有1479个不同的基因删除动作（干预实验）。      上面三个干预实验中，头两个实验能很明显的判断出，被干预的那个基因是观察基因的因。而第三个实验中，表达的是降低基因3672的活性并不能使基因1475的活性降低，因此不能确定是1475的因。     那么不变性方法如何应用到上述实验中的因果推断中?     上述实验中总共有六千多个基因，如果两两配对，那么有6170×6169个组合，如果再算上方向问题，即A有可能是B的因，B也有可能是A的因，那么需要做6170×6169～6170×6169×2个干预实验，才能理清楚这个实验中，所有基因的因果关系。     因此，可以用不变性方法打辅助啊。     首先将干预中可以明确指出因果关系的干预排除，比如我们想找基因4710的因，那么我们先将实验中对5954的干预实验排除，通过其他的干预实验和不变性方法，看看是否能推断出5954是4710的因。在这里，不做任何干预的环境视为环境1，做了干预（Y和Y的因的干预实验）的视为环境2，应用不变性方法，找出潜在因果关系。     接下来，我们来比较一下不变性原理和其他方法在上述实验中找潜在因果关系的效能。     横坐标表示的是通过不变性原理，推理出来的可能存在因果关系的基因对的数量，比如通过不变性原理，发现了25组基因可能存在因果关系，按可能性由高到低排个序，选出最有可能的一组，去验证，发现果然是的，所以折线图第一个点的坐标落在了（1, 1），接着验证下一组，发现也是对的，则坐标继续落在了（2,2），如果接下来一组验证不对，那么坐标落在（3,2），完美的情况，当如PERFECT那条线，随机的选择验证的基因组，则结果就如灰色条所示。     由上述实验可知，不变性原理对因果推断是非常有用且有效的。大大提高了因果推断的效率。     之后会将因果归纳和机器学习相关联，感兴趣的不妨关注吧。     如果觉得有用，一定帮老夫在右下角点个“在看”哦。 系列首篇：MIT因果迷你课笔记 —— 相关和因果 上篇：MIT因果迷你课笔记 —— 因果归纳模型的评估方式（SHD和SID） 记：公式上上下下有不匹配的地方，因为截自不同的参考文献，所以写法会有出入，打公式太累了，我放过了自己。 声明：所有图片均来自参考，没有原创图片，公式和定理。 参考： [1] Jonas Peters, University of Copenhagen, Mini-course on Causality, Laboratory for Information &amp; Decision Systems (LIDS) and Models, Inference &amp; Algorithms of the Broad Institute, MIT, 2017 [2] Jonas Peters, Peter Bühlmann and Nicolai Meinshausen, Causal inference using invariant prediction: identification and confidence intervals, 2015 [3] Nicolai Meinshausen, Package ‘InvariantCausalPrediction’, 2019 "
    }, {
    "id": 40,
    "url": "http://localhost:4000/%E4%BB%A5%E5%9B%BE%E7%89%87%E4%B8%BA%E7%9B%AE%E6%A0%87%E7%9A%84%E8%A7%86%E8%A7%89%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/",
    "title": "以图片为目标的视觉强化学习",
    "body": "2021/03/11 -     今天说一下发表于NIPS 2018的一篇文章，以图片为目标的视觉强化学习。     为了更直观的了解一下这篇论文解决的问题，可以看一下这个链接里的视频demo: https://sites. google. com/site/visualrlwithimaginedgoals    这里简单介绍下Variational Autoencoders（VAE），变体自动编码器，VAE是一种典型的生成神经网络，功能类似Generative Adversarial Networks (GAN)，VAE包含两个部分，一个是编码器qφ，将图片转换成向量，还有一个是解码器p[这里下标符号打不出来，注明一下]，但VAE不同于通常的编解码器： 通常的编解码器，是将输入转换成向量，再由向量解码，对比解码出来的结果和原始输入，可以判断这个编解码器的好坏。而为了让VAE有生成网络的能力，则是将输入转换成向量的概率分布，再从分布中采样，最后解码，具体原理，请参考参考[1]，有机会专门介绍一下GAN和VAE。这篇论文中训练VAE的损失函数如下： L中头两个参数分别指解码器和编码器的参数，DKL指KL散度，即相对熵，计算的是两个分布之间的距离。编码器和解码器是一起联合在一起训练的。     在这个研究中，目标域G和状态域S是一致的。即，所有可能的状态都应该能达到才对。     首先通过随机执行一些策略，观察结果，来训练一个VAE将状态图和目标图转换成向量，z=e(st), zg=e(g)。     在训练好VAE之后，训练目标导向的Q-function Q(z,a,zg)和对应的策略，并再对VAE进行优化。算法如下： 其中2指VAE的损失函数，1指Q(s,a,g)的损失函数，用的是Bellman error： 参考： [1] Joseph Rocca, Baptiste Rocca, Understanding Variational Autoencoders （VAEs), torwards data science, 2019 "
    }, {
    "id": 41,
    "url": "http://localhost:4000/%E7%94%A8%E4%BA%8E%E7%BB%86%E5%88%86%E7%B1%BB%E7%9A%84API-Net-Attentive-Pairwise-Interaction-Network/",
    "title": "用于细分类的API-Net（Attentive Pairwise Interaction Network）",
    "body": "2021/02/23 -     在分类问题中，细分类是一个很有挑战性的问题，鸟和人的区别很大，但是不同品种的鸟之间，区别就不是很大了，具有高度混淆性，即使是人，有时候也很难分辨。另外分类越细，每个类别的数据量就会越少。     下面这篇发表在AAAI2020的文章，提出了一个新的网络结构来解决这个问题。Attentive Pairwise Interaction Network (API-Net)，这个网络的名字包含了这个网络设计的三个要点：Attentive对应的是注意力机制，Pairwise表明训练用的是成对训练的方式，Interaction对应的是交互机制。     因为是老问题了，就直接看网络结构吧：     网络结构就如上图，首先将数据两两成组，两张图分别通过一个CNN网络得到特征向量，x1和x2，再将x1和x2拼接在一起，通过一个多层感知器（Multilayer Perceptron，MLP）得到共同向量xm：     我们期望xm标记了x1和x2差距大的位置。打个比方，如果x1，x2向量第一个位置标记的是鸟爪的颜色，如果两只鸟鸟爪的颜色很不一样，那么我们期望xm第一个位置的值接近1，如果两只鸟鸟爪的颜色一致，那么我们期望xm第一个位置的值接近0。当然每个位置代表的特征不是事先定义好的，这里只是举个例子，具象化解释一下。             接下来分别计算两个特征向量的注意向量g1和g2：     g1被期望表达的是x1和x2不同的地方，且这个不同的地方是x1的主要特征，而g2表达的是x2与x1不同的地方，且这个不同的地方是x2的主要特征。比如说向量的一个位置表达的是爪子是否是黄色，x1在这个位置值高，x2在这个位置值低，那么我们就期望g1在这个位置的值会去接近1，g2在这个位置的值会去接近0。那么如果x1和x2在这个位置的值都高和都低呢？那么就期望g1和g2在这个位置的值都低。     接下来说Pairwise Interaction，硬要翻译的话，两两交互吧：     self可以理解为强化自己的重点，other可以理解为强化别人的重点。softmax做最后的分类归纳。     最后的损失函数： &lt;div align=center&gt;&lt;/div&gt;         这部分用的是交叉熵(Cross Entropy Loss)，表达的是两个分布之间的距离，也可以理解为预测的结果和真实的结果之间的距离。     这部分用的是Score Ranking Regularization，这部分想要表达的是，强化了自己的重点的向量得到的分类结果，应该要比强化了别人的重点得到的分类结果要更准确。     这篇就说到这里，如果觉得还不错，帮忙右下角点个“在看”再走哇。 声明：所有公式和图片还有原理均来自论文本身。 "
    }, {
    "id": 42,
    "url": "http://localhost:4000/MIT%E5%9B%A0%E6%9E%9C%E8%BF%B7%E4%BD%A0%E8%AF%BE%E7%AC%94%E8%AE%B0-%E5%9B%A0%E6%9E%9C%E5%BD%92%E7%BA%B3%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0%E6%96%B9%E5%BC%8F-SHD%E5%92%8CSID/",
    "title": "MIT因果迷你课笔记 —— 因果归纳模型的评估方式（SHD和SID）",
    "body": "2021/02/07 - 系列首篇：MIT因果迷你课笔记 —— 相关和因果 上篇：MIT因果迷你课笔记 —— 发现因果关系3(多变量) 如何评估因果模型？     如何评估你的因果归纳模型？这个问题转换一下可以是如何比较两个因果关系图，再数学一点，就是如何定义两个图的距离。     这和因果关系要应用的场景，领域，以及该因果关系的特质都有关系，实际上是一个非常复杂又多变的问题。     下面讨论两种可以应用于一般场景的方法：     1.  Structural Hamming Distance (SHD)     PDAG指的是部分有向无环图，即不一定所有边都是有向的。P指的是由p个变量的所有有向无环图构成的图域。那么任意两个图的SHD是指两个图不一致的边的数量。#在这里是数量的意思。任意两个变量之间的边有三种：i和j之间没有任何关联，i-&gt;j，和j-&gt;i。     比如下面三张图：     就有：     2.  Structural Intervention Distance (SID)     这个概念有点难理解，首先得理解分布的马可夫性，这里只列一下定义，详细可以参考MIT因果迷你课笔记 —— 发现因果关系1，分布的马可夫性有三种定义方式：     这是基于D-separation，d-connection的定义，要理解可能要详细看一下D-separation的概念。为什么用Markov呢，Markov形容d-separation为，已知的当下分开了过去和未来，让未来不再依赖过去。     我们认为分布P基于图G是Markov的，如果Xi和Xj在图G中被S d-separated能推导出Xi⊥Xj|S。这里我们用⊥代替图中的独立符号，因为图中的独立符号在这里打不出来。      另外，马可夫状态还有其他两种定义方式：     其中Y是一组变量的集合，PA(X)指X的所有的父变量的集合，ND(X)指除了X变量和其后裔变量之外的所有变量的集合。下面这种是基于分解的定义方式：     这几种定义方式是等价的。          干预分布(intervention distribution)     当分布具有马可夫性时，下面这个公式表现的是对Xi的值进行干预后的概率分布函数，do表示干预这个动作：     那么干预对单个结点的影响又如何表达：       如果Y是X的父节点，那么干预X对Y不会造成任何影响，而如果Y不是X的父节点，那么干预X之后，Y的分布则会因X的值还有X的父节点调整。     举个例子，在图b中，干预y2的值，会对y3造成什么影响呢？y3不是y2的父节点，计算如下：     根据推导可知，在a图和b图中，干预y2对y3的影响是一致的。而如果你仔细计算每一个干预分布会发现，图G和图H1所有的干预分布都是一致的，但是图G和H2的干预分布有8个不一致：     显而易见，SID统计的是两个图干预分布不一致的个数。     SID完整的定义如下：     首先定义了correctly and falsely inferred intervention distributions，干预分布的一致性和不一致。再根据falsely inferred intervention distributions的定义定义了SID。      为什么设计SID这么一个度量标准，这和因果推断的这个课题本身的特质是有关系的，有些因果错了并不会对整体推断带来大的影响，但有些会，如果有具体的场景，可能又要结合具体的场景设计评估方式，详细可以参考[2]。另外定义SID的方式有很多种，还有一种基于valid adjustment set的方式： 下面定义了判断valid adjustment sets的方式：     用valid adjustment sets定义SID的概念如下：     DE对照PA，指的是子节点。     下面做了一些实验，设定结点的数量，随机生成一些有向无环图组，计算它们的SHD和SID值，可以发现，图和图之间的SHD和SID值区别是很大的，也就是说SHD和SID是完全不同的评量标准。       下面两幅图，分别用SID和SHD评估了一些因果推断的方法，可以发现，有些在SHD标准下优于随机推断的方式，在SID标准下和随机推断相比，毫无优势。      觉得有帮助到你的话，右下角帮忙点个“在看”哇。 系列首篇：MIT因果迷你课笔记 —— 相关和因果 上篇：MIT因果迷你课笔记 —— 发现因果关系3(多变量) 记：公式上上下下有不匹配的地方，因为截自不同的参考文献，所以写法会有出入，打公式太累了，我放过了自己。 声明：所有图片均来自参考，没有原创图片，公式和定理。 参考： [1] Jonas Peters, University of Copenhagen, Mini-course on Causality, Laboratory for Information &amp; Decision Systems (LIDS) and Models, Inference &amp; Algorithms of the Broad Institute, MIT, 2017 [2] Jonas Peters, Peter Buhlmann, Structural Intervention Distance for Evaluating Causal Graphs, Neural Computation, 2015 "
    }, {
    "id": 43,
    "url": "http://localhost:4000/MIT%E5%9B%A0%E6%9E%9C%E8%BF%B7%E4%BD%A0%E8%AF%BE%E7%AC%94%E8%AE%B0-%E5%8F%91%E7%8E%B0%E5%9B%A0%E6%9E%9C%E5%85%B3%E7%B3%BB3(%E5%A4%9A%E5%8F%98%E9%87%8F)/",
    "title": "MIT因果迷你课笔记 —— 发现因果关系3(多变量)",
    "body": "2021/02/04 - 系列首篇：MIT因果迷你课笔记 —— 相关和因果 上篇：MIT因果迷你课笔记 —— 发现因果关系2(restricted structural causal model) 这节继续讲 restricted structural causal model，会从两个变量的因果归纳，延伸到多个变量。     多个变量的问题通常可以转换成两个变量的问题来解决，怎么转换？     假设你现在有三个变量，它们三个的关系可能会是下面两种：     一种是左边这种，x是y和z的因，z是y的因。另一种，x是y和z的因，y是z的因。那怎么转换成两个变量的因果判断呢，固定x的值即可，赋予x一个固定值，那么x到y和z的因果关系就被切断了。接下来的问题就变成了判断z是y的因还是y是z。              我们可以通过联合分布推出有向无环因果图吗？答案是不能。但如果噪声呈联合独立高斯分布，且函数都是非线性的话，那么就可以从左边的分布式推出右边的因果图。至于这其中的道理，概括着讲就是，如果满足非线性且高斯分布的条件，那么因和噪音会相互独立的，但果和噪音不相互独立，可以根据这一点来判断因果。具体请看上一篇MIT因果迷你课笔记 —— 发现因果关系2(restricted structural causal model)。     分类概括，有下面的公式可以参考，第一种structural causal model，不能从分布函数推导出因果图，但是additive noise model和causal additive model可以，linear Gaussian model又不行。PAi指结点i的父节点。     现有additive noise model，一种是X-&gt;Y的分布，分布函数所有的可能性用下图中红色的区域表达，一种是Y-&gt;X的分布，所有的可能性用绿色的区域表达。如果函数是线性的，那么因果关系既可以是X-&gt;Y，也可以是Y-&gt;X，交线表达的是线性情况。但如果函数非线性，则不会有相交的部分。     假如说X和Y的真实分布是P，而我们实验得到的分布是P’，如何根据P’来判断是X-&gt;Y，还是Y-&gt;X，可以分别计算P’到两个分布函数空间的距离，离哪个空间近，就认为属于哪个空间。     计算两个分布之间的距离可以用KL散度，即相对熵，具体的定义和解释可以见Cross Entropy Loss，而这里要做的是找到最近的分布函数，即最小化相对熵或者KL散度（最小化交叉熵（或KL散度[相对熵]）和极大似然估计是等价的，关于这个，这里不做详细说明，具体原理见参考[2]）。上图表达的是两个变量的分布域，而现实中，可能会有多个变量，多个变量可以有多种可能的因果，每一种因果都会有一个分布域。因此可用下面的方法找到最有可能的因果关系：     上式中residuals是残差，即观察值与估计值（拟合值）之间的差。用一句话说明就是：遍历每一种可能的因果关系，基于每种因果关系，拟合出分布函数，再计算拟合出的分布与真实分布之间的距离，最后选择那个与真实分布距离最小的分布函数基于的因果关系。     用空间感强一点的方式解释是，每个因果关系都有一个分布域，将真实分布投射到每个分布域上，再计算真实分布与投射点的距离，选出最小距离的投射，对应的因果关系就是这个方法的结果。     下面有一段R语言代码，演示了如果判断两个变量间的因果关系，仅供参考： library(dHSIC)library(mgcv)set. seed(1)# 模拟出数据，x是因，y是果# 现实中数据是实验观察所得，而观察者可能并不知道真实的因果关系x&lt;-rnorm(200)y&lt;-x^3+rnorm(200)# 假设x-&gt;y，拟合出分布函数，广义加性模型gam在R语言中用于拟合非线性关系。modelforw &lt;- gam(y ~ s(x))# 假设y-&gt;x，拟合出分布函数modelbackw &lt;- gam(x ~ s(y))# 独立性测试，modelforw的残差与输入值的独立性更大# p-value为假设检验，p值越小，拒绝原假设的理由越充分dhsic. test(modelforw$residuals, x)$p. value# 0. 771228771228771dhsic. test(modelbackw$residuals, y)$p. value# 0. 00699300699300699# 似然估计，很明显modelforw离真实分布距离更近-log(var(x)) - log(var(modelforw$residuals))# 0. 142006280126033-log(var(y)) - log(var(modelbackw$residuals))# -1. 01401349632063     根据上面的代码，会发现归纳两个变量之间的因果关系似乎还挺简单的，那么这个方法有什么问题呢？     1. 当变量增多，如果只有一个变量，没有因果关系会存在，如果有两个变量，那么有两种可能的因果关系，如果有三个变量呢？有25种可能的因果关系（有向无环图）。如果有20个呢？答案是无数个，看下图：     关于这个问题，有些研究提出了一些算法解决，这里就不细说了：  边越多，拟合出来的效果会越好，似然估计值会越大，但是有些边是无用的。需要一些其他方法来剔除掉多余的边。这里也不展开了。    另外一个点，就是你需要多少数据来归纳因果关系。这时候，可能就需要计算正确的分布和错误的分布之间的距离，如果它们距离很近，那么很明显，因果关系就比较难归纳。     怎么计算呢？不同情况，方式不一样，下面举例了一种情况，从下面公式可以看出，当噪音强的时候，两种分布就会离的比较近，我们就可能需要更多样本来归纳因果关系： 系列首篇：MIT因果迷你课笔记 —— 相关和因果 上篇：MIT因果迷你课笔记 —— 发现因果关系2(restricted structural causal model) 声明：所有图片均来自参考，没有原创图片，公式和定理。 参考： [1] Jonas Peters, University of Copenhagen, Mini-course on Causality, Laboratory for Information &amp; Decision Systems (LIDS) and Models, Inference &amp; Algorithms of the Broad Institute, MIT, 2017 [2]轻墨，极大似然估计与最小化交叉熵损失或者KL散度为什么等价？https://zhuanlan. zhihu. com/p/84764177 "
    }, {
    "id": 44,
    "url": "http://localhost:4000/%E5%85%B3%E7%B3%BB%E5%BD%92%E7%BA%B3%E5%81%8F%E7%BD%AE-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%92%8C%E5%9B%BE%E7%BD%91%E7%BB%9C/",
    "title": "关系归纳偏置，深度学习，和图网络",
    "body": "2020/11/23 -     开个新坑。     DeepMind，Google Brain，MIT，University of Edinburgh，这阵容，不能不看。     这篇略有些不同，讨论的不是一个方法，一个技术，一个验证，而是关于人工智能发展方向，或者说我们应该着重的研究和探索的方向。我们人类究竟能不能获得强人工智能，又如何才能获得强人工智能？这篇论文当然也没有答案，只是提供了一些想法。那就让我们来看一看都写了些啥。     以下是自己整理的一些关键点，没有逐句翻译，感兴趣的可以结合原文来看。 In particular, generalizing beyond one’s experiences—a hallmark of human intelligence from infancy—remains a formidable challenge for modern AI.     大意就是，我们离强人工智能其实还挺远的。     然后这篇文章的立场： We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective.     结构化的表达和计算，也许是实现强人工智能的关键。     这篇文章干了啥？ We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning.     三个关键单词，graph networks，relational reasoning，combinatorial generalization。这篇文章就是在讨论，graph networks 在relational reasoning和combinatorial generalization这两块的作用。至于这三个都是啥？咱且慢慢说。      首先说combinatorial generalization，强行翻译就叫她组合生成吧。 A key signature of human intelligence is the ability to make “infinite use of finite means” (Humboldt, 1836; Chomsky, 1965)     人类智慧的关键点，能用有限创造无限。比如我们的语言，有限的单词，无限的话语；比如1900的钢琴曲，有限的琴键，无限的音乐。下面引述一段“1900”的一段的台词： Take a piano. The keys begin，the keys end. You know there are eighty-eight of them，nobody can tell you any different. They are not infinite. You are infinite. And on these keys the music that you can make is infinite. I like that. That I can live by.     所以可能，无限生于有限，而永恒藏于瞬间。     这里第一个概念就出来了，人类用单词构造语句，用音符谱乐曲的能力，就叫做combinatorial generalization，这里翻译成组合生成。     那人这种能力又是怎么来的？ We represent complex systems as compositions of entities and their interactions 1 (Navon, 1977; McClelland and Rumelhart, 1981; Plaut et al. , 1996; Marcus, 2001; Goodwin and Johnson-Laird, 2005; Kemp and Tenenbaum, 2008)     我们人类的认知系统可以将现实中获得的信息概括成实体或概念，并将这些实体和概念关联起来。另外，实体和概念还会被分为不同层级，比如你在家庭里面，而家庭又在小区里面，小区又在城市里，城市在国家里。我们擅长用熟悉的技能和规则解决新的问题。例如你第一次规划一个行程，那么这个任务其实可以分解成很多小的你熟悉的问题，比如怎么去一个地方，怎么订酒店，在哪儿吃饭，几点到几点干什么。我们擅长类比，关联，推理，从获得的经验中抽取实体和关系，在遇到新的问题的时候，进行类比，关联，来确定新的实体或概念之间的关系，进而推理获得新的问题的解决方法。     其实一开始，受限于收集数据和计算的成本，人工智能领域比较倾向结构化的方法来解决问题。而如今，随着收集数据和计算的成本降低，大家又都偏向端到端的解决问题的方式，比如深度学习，在各个领域获得了巨大进展。但深度学习明显不是什么都能做到的，比如对语言和场景的理解，结构化数据的推理，训练场景之外的迁移学习，还有从少量经验中学习的能力（比如一个小孩子，可以通过几张照片来认识一个新的生物，并不需要一大堆样本）。     重申立场： We suggest that a key path forward for modern AI is to commit to combinatorial generalization as a top priority, and we advocate for integrative approaches to realize this goal.     这篇文章认为，组合生成才是实现人工智能的关键，而要实现这个目标，需要结合各种技术，包括深度学习，结构化方法，等。     接下来，Relational Reasoning，关系推理     我们定义结构，由已知的模块相互关联组成的产物。比较典型的如知识图谱，再有因果图谱也是。那么一个结构化的表达包含哪些元素呢？     1. 实体（entity)，有特征或属性的元素或概念，比如特朗普，白宫，宪法，白桦树。 ​  2. 关系（relation），实体和实体之间的属性，比如特朗普和白宫的关系，特朗普是白宫的前任主人？除了从属关系，还可以是很多其他关系，比如谁比谁高，谁比谁的销量好，谁喜欢谁，等等。     3. 规则（rule），可以想成一个函数，输入是实体，关系，通过这个函数将输入的实体和关系映射到其他实体或关系，比如判断一个实体是不是重的，判断一个实体是不是比另一个实体重。     结构化表达，即捕捉结构的组成（比如元素的排布），而结构化计算，将元素和组成作为一个整体，进行操作。关系推理，使用构成实体和关系的规则操控实体和关系的结构化表达。这里我晕了，并没有理解，咱就过吧。     Relational inductive biases 关系归纳偏置 ​	先说归纳偏置     学习是一个通过观察和和世界交互来获取和理解知识的一个过程。它包括搜索一个最佳或更优的解决问题，解释数据，或者获得报酬的策略。但是好的策略和方法往往不止一种。归纳偏置则是使得学习算法去偏向一种或一个方法，策略或解释。在机器学习中，归纳偏置非常常见，举个例子，贝叶斯网络中的归纳偏置通常是通过先验函数的选择和参数化来表达的。其他情况下，用来避免过度拟合的正则项（regularization term），还有网络结构的设计，都能达到归纳偏置的效果。     用大白话说一下，就是，我们知道，解决这个问题呢，有很多很多种可能，但是我们的大脑根据经验，某个特性，或者定理，觉得可以朝这个方向试试，于是我们通过调整网络结构，选择先验函数等方法，使得算法在搜索解决问题的方法的时候有了偏好，这就是归纳偏置，是以牺牲灵活性为代价的，通常是为了更好更快，用更少数据找到好的策略，如果偏置策略选的不恰当，可能反而会有不好的效果。     这里用关系归纳偏置泛指归纳偏置，在学习过程中在实体之间的关系和相互作用上施加的约束。在深度学习中，各种非关系归纳偏置有，非线性激活函数，weight decay，dropout，batch and layer normalization，data augmentation，training curricula，和optimization algorithm。     下面这个表格分别列出了全连接网络，卷积网络，RNN，还有图网络的实体和关系的定义，以及关系归纳偏置的特点。      首先我们看全连接网络：     实体是网络中的单元，即上图中的方框，而关系是每一层的方框和下一层所有的方框都相连，规则则是权重和偏置项还有连接的函数决定的。没有信息的重用和隔离，因此该种网络关系归纳偏置是很弱的。     看CNN：     这里的实体依然是每个单元，但关系相较于全连接网络，稀疏了很多。这里是用局部性和平移不变性，实现了关系归纳偏置。局部性是说，我们要提取的某个实体的特征其实只和它周边的实体有关，和距离很远的实体关系不大或无关，平移不变性是说有些规则可以在不同的位置上的实体达到相同的作用。     接下来看RNN：     实体是每一步中的输入和隐藏状态，并将当前隐藏状态和上一层隐藏状态，还有当前输入的马可夫依赖性作为关系。而规则则是利用之前的隐藏状态还有当前的输入，来获得新的隐藏状态的参数和方法。规则在每一个时间步骤上都是一样的。因此，RNN是用时间不变性来产生偏差的，即，有些状态不是一天达到的，而变迁的规则每天是一样的。另外，RNN同时也利用马可夫结构对序列中的局部性产生偏置。     从上面几个案例来看，深度学习运用了很多关系归纳偏置的方法。虽然如此，并没有一个默认的，或者说统一的深度学习组件可以在任意关系结构上运行。     我们来看一下集合（sets）这个结构，集合里的元素是没有顺序的。有这样一个问题，我们需要算出一系列星球组成的星系的质心，我们得到了星球的集合，集合里的每个星球有各种各样的属性，现在我们需要训练一个深度神经网络，来计算质心，那么我们要怎么排布输入？如果用全连接网络，那么我们的输入，星球必然有一个排布，如果训练的时候，只有一部分排布作为了训练数据，那么测试的时候，换个训练时网络没见过的排布输入，那么结果可能完全不一样。而其实这个问题是和星球的输入顺序或者说排布没有任何关系的。另外，一个集合不同排布的数量是极高的，n个星球，不同的排布可以有n！个。解决这个问题的方法之一是，将每个星球转换成特征向量，再取均值，再用全连接网络计算质心。但也并不是所有问题都能这样解决。     除置换不变性之外，还有其他问题，比如预测星系中某个星球在一段时间后的位置，在这个问题上，上面取均值的方法就不奏效了，因为在这个问题上，其他星球对这个星球都是有作用的。     对于上面两个问题，一个是实体之间没有关系，另一个是实体之间两两关系，而自然界中的实体之间的关系通常是介于两者之间。因此，我们原本解决问题的深度神经网络的方式是有明显的缺陷的。     图（Graph），通常来说，是一个支持任意关系结构的表达。而基于图的计算，相较于CNN或RNN，有非常强的关系归纳偏置。     最后一个概念，图网络（Graph Network），也是这篇文章提出的一个结构，以下简称GN。     其实这也不是一个新领域，至少有十多年历史了。     GN框架的主要单元是GN模块，输入是图，输出是图。如下图所示：     圆为实体，边为关系，u是全局属性，它们都可以用特征向量来表达：     因此，一个图可以被定义为一个三元组，u这种全局属性在现实生活中，可以是类似重力场之类的属性。     那么，一个GN模块，是怎么将一个图更新到一个新图呢？每个GN模块有三个更新函数：     写成算法是下图这样：     画成图是下图这样：     其中，蓝色表示正在更新的元素或关系，而黑色表示与这次更新相关的元素或关系。     这篇文章的前因后果大概就是如此，如果想了解更多，可以去看论文原文，以及源代码： https://github. com/deepmind/graph_nets    如果有机会，本公众号也许会讲这篇文章GN模块的具体代码和设计细节，所以，关注吧。 其他参考： [1] 海上钢琴师 "
    }, {
    "id": 45,
    "url": "http://localhost:4000/Kaggle%E6%AF%94%E8%B5%9B%E7%B3%BB%E5%88%97-Conway's-Reverse-Game-of-Life-2020/",
    "title": "Kaggle比赛系列：Conway's Reverse Game of Life 2020",
    "body": "2020/11/10 -      Conway是一位数学家，John Horton Conway，活跃于有限群论，结论，数论，组合博弈论和编码论。 他还为休闲数学的许多分支做出了贡献，最著名的是1970年，细胞自动机（cellular automaton）的发明，即生命游戏（The Game of Life）。即这个比赛需要攻克的游戏。Conway在利物浦出生，长大，在剑桥大学度过了职业生涯的前半段，后移居美国，在普林斯顿大学担任约翰·冯·诺依曼教授职位。     2020年4月11日，他享年82岁，死于COVID-19并发症。     这个比赛的发起者是Kaggle。六年前发起过，今年做了一些小的修改后再次发起，重新发起，应该是为了纪念这位在疫情中逝世数学家。     这是一款零人游戏，意味着其演变取决于其初始状态，无需进一步输入。趣味在于创建初始状态，然后观察其演变。该游戏是图灵完备（Turing complete）的，可以模拟通用构造函数或任何其他图灵机。     生命游戏的宇宙是一个无限的二维正交方格，每个方格都处于两种可能的状态，即活着或死了。每个方格都有八个相邻的方格，称之为邻居。然后，在每个时间步上，都会完成如下转换：     1. Underpopulation：任何具有少于两个活邻居的活细胞都会死亡，就好像人口不足一样。   2. Stasis: 任何有两个或三个活邻居的活细胞都可以存活到下一代。     3.   Reproduction: 具有正好三个活邻居的任何死细胞都变成活细胞，就像通过繁殖一样。     4. Overpopulation: 任何具有三个以上活邻居的活细胞都会死亡，就好像人口过多一样。     自从被发明，生命游戏就吸引了人们的极大兴趣，因为它可以以惊人的方式演变。即使初始状态没有经过设计，也能演变成有组织有设计的状态。对于计算机科学家，物理学家，生物学家，生物化学家，经济学家，数学家，哲学家，生成科学家以及其他人来说，执行非常简单的规则而出现复杂模式的这种现象非常有趣。它可以用来传达某种违反直觉的观念，即在没有设计师的情况下，“设计”和“组织”可以自发出现。哲学家和认知科学家丹尼尔·丹尼特（Daniel Dennett）广泛使用了生命游戏的宇宙的类似物，来说明复杂的哲学构造（比如意识或自由意志）可能是从简单的确定的物理定律演化而来的。     生命游戏会出现许多不同类型的模式，这些模式根据其行为进行分类，常见的包括：     静物，一代又一代不变，如下图：     振荡器，经过有限的世代后返回初始状态：     宇宙飞船，在整个网格中转换：     更多细节可以去看参考[1]。     而Kaggle这个比赛又与生命游戏略有不同，Conway’s Reverse Game of Life，这个比赛想知道的是，如果我们倒退时间，会发生什么。这个竞赛是一项实验，旨在研究机器学习（或其他方法）是否可以预测反向的生活游戏。 有序的结尾是否能用来预测混沌的开始？ Kaggle进行了许多不同开始的生命游戏，并提供参赛者最终的状态。要求参赛者预测每个最终状态的初始状态。     1. 数据构成     50,000个游戏样本作为训练数据，50,000个作为测试数据，任务：预测起始状态。理论上，生命游戏的宇宙是无穷大的，但现实中的游戏很难满足这个条件，在比赛中，生命游戏的宇宙为25x25的空间，总共625个单元。如果需要，参赛者可以自己生成更多的游戏样本作为训练样本。     训练文件的变量：  id - 游戏编号 delta - 启动状态到停止状态经过的步数 start_0 - 启动状态第一行第一列的值 start_1 - 启动状态第一行第二列的值 … stop_0 - 停止状态第一行第一列的值 …    参赛者对于测试集的预测应该是停止状态在给定步数之前的状态，即启动状态。训练用的测试集和训练集是这样生成的：  以1%-99％的随机密度填充空间中的单元。   然后经历五步，生成启动状态。   然后启动状态开始经历一定步数到达停止状态。经历的步数是随机产生的，在1-5之间。如果停止状态是空的，即所有细胞都死了，那这个游戏样本不会被加入训练集和测试集常见问题    为什么需要热身步骤？从最初的随机状态到下一个状态的演变可能会非常“非线性”且神奇。比如，如果初始状态大多数细胞都还活着，那么一步之后它们可能都死了。所以五个步骤的预热可以使细胞平静下来，更接近生命演变的方式。    生命游戏随着时间的流逝会丢失信息，什么原因？正确，这是一个多到一的问题（不同启动状态可能产生相同停止状态）。但在短步骤内，这应该是一个小问题。这项比赛中倒退的最大步长是5，就是希望退化不会成为问题。    我可以预测任何有效状态吗？任何能以指定步数达到结束状态的开始状态都将获得最佳分数。     2. 模型评估     计算误差用的是平均绝对误差（mean absolute error）     用参赛者给出的启动状态经历指定步数得到终止状态，再计算这个终止状态和被预测启动状态的终止状态的误差。     3. 时间     比赛截至:  November 30, 2020     4. 实现 ​   解题思路一：基因算法（Genetic Algorithm）     参考[5]，该作者公布的算法，成绩是0. 06189，可以在LeaderBoard上排到13，作者本人最好的成绩是0. 05861，排到了12名。目前LeaderBoard上最好的成绩是0. 00939，他用的什么算法呢？     基因算法概念上可以这样理解，有一群袋鼠，一开始随机在可能的解上均匀分布，然而，在劣势解上的袋鼠会被慢慢地消灭，而在优势解上的袋鼠会不断繁衍，这样，最后袋鼠们会慢慢聚拢到一个一个局部最优解上，然后从这些局部最优解中选择最优的作为最终解。基因算法并不能保证一定能得到最优解。     解题思路二：Z3约束 (Z3 Constraint Satisfaction)         参考[6]，分数是0. 0857。Z3约束是微软开发的一套约束求解器，简单的可以理解为解方程的神器。很强大，可以了解看看。该作者还开源了另外一种方法，比这个方法成绩稍微低一点。     这世上，有人赢了，有人输了，有人已经死去了，有人还好好地活着，有人想预测未来，去触摸世界的尽头，也有人，想回溯过去，看看世界它本来的样子。     可谁说，不是一回事呢？ 参考：     [1] https://en. wikipedia. org/wiki/Conway’s_Game_of_Life     [2] https://www. kaggle. com/c/conways-reverse-game-of-life-2020     [3] http://jakevdp. github. io/blog/2013/08/07/conways-game-of-life/     [4] https://en. wikipedia. org/wiki/John_Horton_Conway     [5] Game of Life - Genetic Algorithm (Spanish), Desareca, 2020, https://www. kaggle. com/desareca/game-of-life-genetic-algorithm-spanish     [6] Game of Life - Z3 Constraint Satisfaction, James McGuigan, 2020, https://www. kaggle. com/jamesmcguigan/game-of-life-z3-constraint-satisfaction     [7] 番茄鸡蛋炒饭被抢注啦，[算法]超详细的遗传算法(Genetic Algorithm)解析，CSDN，2018 "
    }, {
    "id": 46,
    "url": "http://localhost:4000/ArcFace-CosFace-SphereFace-%E4%B8%89%E7%A7%8D%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E7%AE%97%E6%B3%95%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0(Loss-Function)%E7%9A%84%E8%AE%BE%E8%AE%A1/",
    "title": "ArcFace，CosFace，SphereFace，三种人脸识别算法的损失函数(Loss Function)的设计",
    "body": "2020/11/09 -     最近看了点人脸识别算法，发现ArcFace，CosFace，SphereFace的损失函数（Loss Function）设计得非常有意思，且设计理念都是相似的，因此今天就记一篇损失函数的设计。     ArcFace，出自英国帝国理工。文章全名是：ArcFace: Additive Angular Margin Loss for Deep Face Recognition。     这篇文章有公开源代码，B站有作者对论文的讲解，很仔细，网上甚至能搜到翻译好的论文，大家都可以去看。 CosFace，出自腾讯，文章全名是：CosFace: Large Margin Cosine Loss for Deep Face Recognition。          SphereFace，出自美国和中国的三所大学，文章全名是：SphereFace: Deep Hypersphere Embedding for Face Recognition。     文章先介绍Softmax，再介绍Softmax Loss，最后介绍ArcFace，嫌啰嗦可以直接看第三部分。     1. Softmax     ArcFace, CosFace, SphereFace的损失函数都是从Softmax-Loss演化来的，而Softmax-Loss是由Softmax和交叉熵(cross-entropy loss)而来。先看Softmax，这个概念参考[1]解释的非常仔细，这里只做简述，先上公式： 这公式是啥子意思呢？先看一个应用：     假如我们现在有一个分类任务，如果模型足够理想，输入一张猫图，输出[1, 0]，输入一张狗图，输出[0, 1]。通常这种任务，前面会是一个深度卷积神经网络，最后会有一个全连接层，经过这个全连接层会得到图的特征向量(embedding)，我自己喜欢管embedding叫特征向量。上图中最后得到的特征向量是[1. 2, 0. 3]，再经过softmax： 得到了[0. 71, 0. 29]，我们可以这样理解最后这个结果，这张图是猫的概率是0. 71，是狗的概率是0. 29，它们两加起来是1，不管softmax的输入向量为何，输出向量里的值相加一定是1，得到的结果可以理解为图在各个类上的概率分布，向量的长度即类别（class）的数量。Softmax主要用在多分类任务上。    2. Softmax Loss 和 Cross Entropy Loss     交叉熵度量的是两个概率分布的差异。     要理解交叉熵，有很多概念需要理解，一个一个来。     信息量，一个事件发生的概率越大，携带的信息量越小，发生的概率越小，这个事件携带的信息量越大。比如太阳从东边升起，这个事件如果发生了，我们可以从这个事件中获得的信息是几乎没有的。但是，如果哪天太阳从西边升起了，那么我们从这个事件中获得的信息量是极大的，一定发生了什么，或者即将发生什么，才造成了这个事件发生。     假设X是一个离散型随机变量，概率分布函数为：     则定义X = x0的事件为：     若p(x0)为0，也就是事件x0是不可能发生的事件，但是它却发生了，那么这个事件的信息量是无穷大的，I(x0)的值是无穷大的，如果p(x0)为1，也就是事件x0是一定会发生的事件，那么这个事件的发生是不带信息量的，I(x0)的值是0。     信息熵，则是信息量的期望值：     事件的概率不均，信息熵较小，若各个事件发生的概率一样，信息熵较大。 ​	相对熵(relative entropy)，又称之为KL散度(Kullback-Leibler (KL) divergence)，公式：     相对熵的目标是：计算用P描述目标问题，比Q描述目标问题能获得的信息增量。     如果分布P和分布Q是一样的，那么相对熵是0，如果不一样，相对熵大于0，越大，表示两种分布之间的差距越大。     在机器学习的项目中，通常P表示真实的分布，即需要训练模型达到的分布，Q是现在的模型预测的分布。     交叉熵(Cross entropy)，将相对熵公式变形：     前半部分是信息熵的负值，后半部分则是交叉熵，所以交叉熵的公式是：     因为P的信息熵是一定的，那么其实是可以省略这部分计算的，交叉熵和相对熵的意义是一样的。只是最后计算出的值，区间不一样。     Cross-Entropy Loss 和 Softmax Loss     毫无疑问，交叉熵可以用作损失函数，且比起MSE，MAE，要优秀不少， … using the cross-entropy error function instead of the sum-of-squares for a classification problem leads to faster training as well as improved generalization. — Page 235, Pattern Recognition and Machine Learning, 2006.      结合上面猫狗分类的案例，假如有一张猫图输入，P是[1, 0], Q是[0. 71, 0. 29]，交叉熵的计算为：     H(P, Q) = – (P(cat) * log(Q(cat)) + P(dog) * log(Q(dog)))     值得注意的是，在很多多分类问题中，不论有多少类，P不论有多少个元素，都只有一个为1，其他都为0，所以交叉熵的计算可以化简为，也就是说如果P(cat)为1，那么交叉熵的结果和Q(dog)，Q(car)，Q(any other)是无关的：     H(P, Q) = –  log(Q(cat))     因此，如果Q(cat)是用Softmax Function计算出来的，那么H(P, Q)计算得到的就是该样本在该模型下的Softmax Loss。     Softmax Loss的完整公式如下：     N是样本数量，n是class的数量，特征向量的长度为d，Wj是W的第j列，和b一起是获得特征向量的全连接层，W是d*n的矩阵，bj的长度是n。log后面则是用Softmax Function计算出的‘Q(cat)’。     因此，其实本来没有什么Softmax Loss的概念，这个公式是在多分类任务中使用Softmax Function+Cross Entropy loss产生的。 ​	3. SphereFace， CosFace和ArcFace     以上都是前情提要，因为SphereFace, CosFace和ArcFace的损失函数都是由Softmax Loss而来。     首先，令：     再做如下转换：     令：     再令||xi||的值为s，则之前的Softmax Loss可转换为如下表达：     这样转换，其实还是Softmax Loss只是换了个写法，接下来看本文介绍的三个方法分别是怎么定义的损失函数：     SphereFace:     CosFace:     ArcFace:     将上面三个公式融合一下还可以得到一个新的损失函数：     接下来看为什么做上述这些变化，是有意义的。其实这样变换的思路并不新奇，逻辑回归-&gt;SVM，Triplet-Loss的设计都用的是这样的设计理念。     即：         类内聚敛(Intra-class compactness)         类间分离(Inter-class discrepancy)     用大白话说一下就是，训练的时候我们就告诉模型，啊，这两个类呢，你得给我们分出来，而且呢，还得按一定距离分出来，再白话一点就是，不但要分出来，还得分得够开。那么怎么能够让模型把两个类分得够开呢？这里我们简单说一下Triplet Loss，这个损失函数常用于人脸识别。     比方说，你现在有以下一个任务：训练一个模型，认出吴恩达老师。因为人脸的类别多，每个类的数据不多，你采用Pairwise的训练方式，用的损失函数是Triplet Loss，即每组样本有三张图，分别是Anchor Image，Positive Image, 和Negative Image。 &lt;div align=center&gt;&lt;/div&gt;    长成Anchor Image这样的就是吴恩达老师，现在又分别有一张Positive Image，Negative Image，通过训练，希望模型可以缩小Anchor和Positive的距离，增大Anchor和Negative的距离，用更直观的图表现如下： 而训练的时候计算Triplet Loss如下： 上面公式分三部分，第一部分是Anchor和Positve的距离，第二部分是Anchor和Negative的距离，我们希望模型计算出的第一个距离小于第二个距离，但不是小就行了，还得小到一定量才行，公式中的第三部分的偏置值则告诉模型，要小多少，loss才能为零或者小于零。     这个理念搁在角向量里，则可以表达为： 即，Anchor的特征向量和Positve的特征向量的夹角得小于Anchor和Negative的夹角，而且得小m那么大。      假如是一个八分类的问题，a图是Softmax Loss的训练目标，而b图是ArcFace的训练目标，两个之间的不同不言而喻。     也就是说，我们不仅希望模型在角向量里可以将不同类区别开来，还可以按一定距离区别开来。SphereFace和CosFace设计理念也是一样的，至于它们三个的区别，看下图：     很多方法的设计理念是一致的，但变化是无穷的，精益求精。     就好像钢琴就那么一排键盘，却能弹出数不尽的曲子，而且，总会有更令人心动的曲子出现。     喜欢的话，关注，分享，点赞，点在看，赞赏吧。 除了文中的列出的三篇论文，还参考了： [1] Thomas Wood，Softmax Function Definition, DeepAI [2] Jason Brownlee，A Gentle Introduction to Cross-Entropy for Machine Learning，2019 [3] 史丹利复合田，一文搞懂交叉熵在机器学习中的使用，透彻理解交叉熵背后的直觉，CSDN，2018 [4] Daniel Godoy, Understanding binary cross-entropy / log loss: a visual explanation, Towards Data Science, 2018 [5] Softmax, Chaos-gravity, 2020 [6] Cross Entropy Loss, Chaos-gravity, 2020 [7] Susmith Reddy, Intuition of Triplet Loss, 2019 "
    }, {
    "id": 47,
    "url": "http://localhost:4000/Kaggle%E6%AF%94%E8%B5%9B%E7%B3%BB%E5%88%97-Two-Sigma-Using-News-to-Predict-Stock-Movements/",
    "title": "Kaggle比赛系列：[Two Sigma] Using News to Predict Stock Movements",
    "body": "2020/10/20 -     不想学习的时候，不如想想怎么发财     这是一个和实现财富自由很接近的Kaggle比赛，通过新闻预测股价。     发起的公司叫Two Sigma，看公司图标：     这个名字，这个logo，很数学，强烈怀疑公司创始人是学数学出身的，一查，MIT的，好家伙。公司网址：https://www. twosigma. com/。     该比赛的数据不提供下载，也不允许参赛的人散布，可见数据收集也对实验结果有着决定性的作用(这好像是废话)，也具有非常强的专业性。当年我完全有时间参与这个比赛，但是我在干什么呢？我啥也没干，追悔莫及，遗憾程度不亚于至尊宝错过了紫霞仙子。     该比赛的数据由两个部分组成：     市场数据(Market data)由Intrinio提供。这家公司应该是一间小公司，Real-Time Financial Market Data Provider …，进去主页就能看到我最爱的斯坦福的标志，肯定是一间厉害的小公司，公司网址：https://intrinio. com/。     新闻数据(News data)由Thomson Reuters提供。汤森路透是一家大型跨国传媒集团，总部在加拿大多伦多。平时不看新闻的我，都感觉这个名字很耳熟。     时间线：     训练数据包含2007年到2017年十年的数据，评分分两个阶段，第一阶段评分用的是2017到2018年年中的数据，参赛者可以一边优化模型，一边看自己模型的优劣，之后在2019年年初锁定参赛者代码，也就是不能再改模型了，开始用参赛者代码进行真正的预测。     比赛已经结束了，第一名是一个来自上海的选手，参加了很多比赛，其他比赛成绩都很一般，只有这个比赛第一名。     一战成名啊。     数据结构：     1. 市场数据(market data):     市场数据包含的是美国股票的子集吧，介绍里用的是US-listed instruments。市场数据里包含的股票每天都是变动的，变动的规则取决于它们的交易量和其他可用的信息。这表示在这十几年的数据里，有些股票可能进来了又离开了，离开后又回来了。因此有些股票会存在空白期，然而并不表示空白期内这些股票没有产生市场数据，只是不在表内。     这组市场数据包含在不同时间跨度上计算出的收益。所有收益都具有以下属性：    收益总是按open-to-open（从一个交易日的开盘价到另一个交易日的开盘价）或close-to-open（从一个交易日的收盘价到另一个交易日的开盘价）计算。     收益要么是原始的，即数据没有按任何其他数据进行调整，比如大盘涨跌。要么是市场剩余的（market-residualized，Mktres），即排去了大盘涨跌造成的影响，计算该股票相对于大盘的涨跌。     可以在任意间隔内计算收益。这里提供的是1天和10天的间隔收益。     收益如果是向后计算出的，会打上‘Prev’的标签，即过去1天，十天的收益，如果是向前计算出的，会打上‘Next’的标签，即未来的收益。   市场数据的列信息如下:     time(ns, UTC) - 当前时间（每天记录的时间都是22:00 UTC）     assetCode(object) - 资产id     assetName(category) - 一组资产对应的名称，这个可能是空白的，如果这组股票没有相关的新闻数据。     universe(float64) - 表示今天是否对该股票进行评估。只有训练数据才有这个值。该值反馈的是今天可以交易的股票，不可以交易的股票不需要进行评分。     volume(float64) - 当日交易量     close(float64) - 收盘价（没有根据股息和拆股进行调整）     open(float64) - 开盘价 （没有根据股息和拆股进行调整）     returnsClosePrevRaw1(float64) - 意思见收益属性描述     returnsOpenPrevRaw1(float64) - 意思见收益属性描述     returnsClosePrevMktres1(float64) - 意思见收益属性描述     returnsOpenPrevMktres1(float64) - 意思见收益属性描述     returnsClosePrevRaw10(float64) - 意思见收益属性描述     returnsOpenPrevRaw10(float64) - 意思见收益属性描述     returnsClosePrevMktres10(float64) - 意思见收益属性描述     returnsOpenPrevMktres10(float64) - 意思见收益属性描述     returnsOpenNextMktres10(float64) - 十天，收益，向后，开盘价计算，不为空，为模型需要预测的值，即target。      2. 新闻数据(News data):     新闻数据包含了关于新闻文章的级别以及资产级别的信息，换句话说，这张表有意地没有规范化。    time(ns, UTC) -  数据在feed上可用的时间，精确到秒。有人可能不太了解feed的意思，可以这样理解，新闻开始被推送给用户的时间。     sourceTimestamp(ns, UTC) - 该新闻被创造出来的时间。    firstCreated(ns, UTC) - 该数据被创建的时间。    sourceId(object) - 该条数据的ID    headline(object) -  该条数据的标题    urgency(int8) - 新闻类型（1: 报警, 3: 文章）    takeSequence(int16) - 新闻序列号，从1开始。报警类新闻和文章类新闻有分开的序列号。    provider(category) - 提供新闻的组织机构（比如，路透社新闻RTRS，美国商业资讯BSW）    subjects(category) - 该新闻的主题代码，公司标志。主题代码描述的是该新闻的主题，可以涵盖资产类别，地域，事件，行业/部门和其他类型。     audiences(category) - 新闻属于哪个产品。通常不同新闻有不同受众。（比如，”M”指Money International News Service，”FB”指French General News Service）     bodySize(int32) - 该新闻当前版本正文大小    companyCount(int8) - 该新闻在主题字段明确列出的公司的数量。    headlineTag(object) - 新闻的汤森路透标题标签     marketCommentary(bool) - 新闻是否有讨论一般市场状况, 比如有类似”After the Bell”的总结。    sentenceCount(int16) - 新闻句子的数量。    wordCount(int32) - 单词和标点符号的总数。    assetCodes(category) - 新闻中提到的资产    assetName(category) - 资产名称    firstMentionSentence(int16) - 从标题开始，提到需要评分的资产的第一句（如：1：标题，2：新闻正文第一句，3：新闻正文第二句，0：没有提到该资产，则该新闻的标题和正文将用来做情感计算）。可以和句子总数结合起来用，用于计算第一次提到该资产的句子在文章中的相对位置。    relevance(float32) - 新闻项与资产的相关性。范围从0到1。如果标题中提到资产，则相关性设置为1。当项目为警报型（紧急度为1）时，相关性应改为使用firstMentionSentence进行度量。    sentimentClass(int8) - 该新闻对于某资产的主要情感类别。     sentimentNegative(float32) - 该新闻对于某资产的情感为负向的概率     sentimentNeutral(float32) - 该新闻对于某资产的情感为中性的概率         sentimentPositive(float32) - 该新闻对于某资产的情感为正向的概率     sentimentWordCount(int32) - 该新闻正文中与某资产相关的单词或标点符号的数量。它可以和wordCount结合起来，计算这篇文章有多大的比例在讨论这个资产。    noveltyCount12H(int16) - 该新闻内容对于某资产的12小时新颖度。它是和一系列包含该资产的新闻比较得来的。     noveltyCount24H(int16) - 24小时新颖度    noveltyCount3D(int16) - 3日新颖度    noveltyCount5D(int16) - 5日新颖度    noveltyCount7D(int16) - 7日新颖度    volumeCounts12H(int16) - 每个资产的12小时新闻量。    volumeCounts24H(int16) - 每个资产的24小时新闻量。    volumeCounts3D(int16) - 每个资产的3日新闻量。    volumeCounts5D(int16) - 每个资产的5日新闻量。    volumeCounts7D(int16) - 每个资产的7日新闻量。     模型评估： ​	输出:    &lt;div align=center&gt;&lt;/div&gt; 即模型预测的将来十天内某只股票的收益情况，如果模型觉得这只股票在未来十天的收益高于大盘很多，那么应该输出接近1的值，如果模型觉得这只股票在未来十天相对大盘会带来很多损失，那么应该输出接近-1的值，如果模型不确定这只股票在未来十天内的收益情况，则输出接近0的值。           在模型评估阶段，每天会做如下计算：     预测值后面跟的rti是指资产i在t这天算起，未来十天的收益，排除掉市场造成的影响。再后面的uti指资产i在t这天是否要被评估。若忽略uti，如果预测值和真实值，两个值正负不一样，则xt的值就小于0，正负一致就大于0。     预测的准确度计算如下：     xt求平均值后除以xt的标准差。如果标准差是0，那么分数为0。     虽然这个比赛已经结束了，但还是有很多值得学习和思考的地方，比如评估模型的时候，最后输出的结果其实是两个维度上的，一个资产维度，一个时间维度，为什么最后分数在资产维度上只是求和，而时间维度上却取平均后除以一个标准差，以及为什么那么设计市场数据的构成和新闻数据的构成等等。     这个比赛的介绍就到这里，之后可能还会更一期这个比赛的EDA代码及结果（根据Kaggle上网友分享的整理）和官方发布的一些代码。感兴趣就关注吧。 "
    }, {
    "id": 48,
    "url": "http://localhost:4000/MIT%E5%9B%A0%E6%9E%9C%E8%BF%B7%E4%BD%A0%E8%AF%BE%E7%AC%94%E8%AE%B0-%E5%8F%91%E7%8E%B0%E5%9B%A0%E6%9E%9C%E5%85%B3%E7%B3%BB2(restricted-structural-causal-model)/",
    "title": "MIT因果迷你课笔记 —— 发现因果关系2(restricted structural causal model)",
    "body": "2020/09/05 - 系列首篇：MIT因果迷你课笔记 —— 相关和因果 上篇：MIT因果迷你课笔记 —— 发现因果关系1     上篇中，我们提到了一些方法和假设，可以从数据分布中解析出因果关系，比如Causal Markov Condition和Faithfulness Assumption。但是通过数据分布P得到的d-separations的状况，可以让我们得到一组Markov Equivalence的因果图，也就是通过这种方法得到的因果图可能不是唯一的。那么接下来一个显而易见的问题是，如何从Markov Equivalence的因果图中得到最终的答案。如分布P是X⊥Y|Z **(vars: **X, Y,Z)，那么X，Y，Z之间的因果关系可以是X-&gt;Z-&gt;Y，X&lt;-Z&lt;-Y，或X&lt;-Z-&gt;Y，显然，在同一个现实场景中，这三种因果关系不能同时成立。那么究竟是哪一个呢？如何才能知道是哪一个呢？     除此之外，根据d-separation建立的Causal Markov Condition和Faithfulness Assumption还有另外一个局限，那就是，如果P中只有两个变量X，Y，那么这个建立在三个变量以上的推断因果关系的方法就不再适用。只有两个变量的时候，我们怎么判断是X-&gt;Y还是Y-&gt;X，这是我们接下来首先要讨论的问题。     首先，有个命题：     Prop：对于所有的分布P，如果P中只有两个变量X，Y，那么一定存在f和Ny，使得Y = f(X, Ny)，且Ny⊥X (这里用⊥顶替一下独立符号)。反过来也一样，一定存在g和Nx，使得X = g(Y, Nx)，且Nx⊥Y。     这个命题对于因果推断是不友好的，这表示，无论你有一个什么样的P，你都可以同时用Y-&gt;X和X-&gt;Y来解释它。另外f和g可能会非常复杂，那我们可以做的就是，可以去分析一些f和g比较简单的案例。     这里引入一个新的概念，restricted structural causal models，接下来，我们就讲restricted structural causal models。 Restricted Structural Causal Models 1. 假设X和Y之间是一种线性关系     先看一种极简的情况，X和Y是一种线性关系：     如果(X, Ny)不是联合高斯分布，那么就不能反推出X被Y线性影响着。也就是说不能通过上面的式子得到下面的式子。这个现象的理论基础是Darmois–Skitovich theorem。     举个例子，X, Ny服从均匀分布(uniform distribution)：     以X为自变量，Y为因变量，那么可以拟合出以下的关系图。     那么以Y为自变量，X为因变量呢，拟合出的线性关系则如下图所示：     但是并不能得到X = bY+Nx，且Nx⊥Y，按照上面的拟合结果，来看Nx是什么样的：     非常明显的是，Nx和Y不独立。     接下来插一个没什么用的概念：arrow of time **（老师的原话╮(╯3╰)╭），应该是从Darmois–Skitovich theorem**衍生出来的，没什么用的话，那应该就是挺有意思的，我们还是看一下。     我们怎么判断一个序列，是按时间流动的方向收集的，还是被人动过手脚，是逆着时间来的。     先来看一个时间序列模型：autoregressive–moving-average (ARMA) models：     t是当下，这个模型是根据过去的结果和噪声，来预测现在的模型。c是常量，c后面的是白噪声，再后面的是过去的结果和匹配的系数，再后面是过去的白噪声和匹配的系数。     这个定理说的是，如果时间序列可以由ARMA(p, q)模型预测，且白噪声是高斯分布的，那么时间序列是可逆的，也就是说时间序列倒过来，也能用一个ARMA(p, q)模型来准确预测。     这个定理在2014年用在了下面这个研究中，如果你把一个视频倒过来放，你能不能让计算机根据这个定理判断出来这个视频是倒过来放的，除了这个定理，这个研究还试了其他方法，有兴趣的可以看一下：      比如下面这个，可以根据之前的定理判断出来：     也可以用来判断GDP的变化是不是被动过手脚，在时间线上翻转了：     对这个研究也可以做一些延伸的思考，因果和时间是分不开的，果应该是发生在因之后，时间倒过来，也就是因果也倒过来了，如果机器可以用一个理论来区分时间的方向，那么表示这个理论很可能可以用来归纳因果关系。     上面说了一大堆，都在假设因果之间存在的是线性关系，但现实世界的因果通常都不是线性关系，看下面一些生活中常出现的只有X，Y的数据集：     上图中，是各式各样的拥有双变量的数据集的可视化呈现，是研究统计的，现实世界常出现的状况。上图中，所有小图，可能是横轴是因，也可能是纵轴是因。那么看下面42号子图，这个图稍微特别一些：     现在来判断一下，对于42号子图，横轴更有可能是因，还是纵轴更有可能是因。我想大多数人都会选横轴，因为横轴为因，同一值对应的是单一结果。若纵轴为因，同一值会对应一到两个结果，这种情况就复杂了。     2. 假设X和Y是非线性关系     对于非线性关系，不管噪音分布为何，都不能逆推。     举例，变量X和噪音都是高斯分布，并和Y呈以下关系：     画出关系图是：     那么以Y为自变量，X为因变量呢，拟合出的线性关系则如下图所示：     不能得到Nx⊥Y，按照上面的拟合结果，来看Nx是什么样的：      非常明显的是，Nx和Y不独立。        现在来看一些现实中的案例，高度和温度：     由常识，我们知道高度是温度的因，上面的数据可以做拟合，拟合的p-value是0. 024，p-value是假设检验里的概念，这里可以简单的理解为错误的拒绝了假设的概率，即拟合结果正确的概率。p-value forward指拿x作为自变量，p-value backward指拿Y作为自变量得到的p-value的值。可以发现，backward远低于forward，但它们又都非常低。       再看一个案例，咖啡和诺贝尔奖：     咖啡和诺贝尔奖的相关度非常高，如果我们将咖啡作为因变量，那么p-value非常低，如果将诺贝尔作为因变量，p-value会高一点。造成这个结果的原因可能有很多，比如模型不够好，比如我们假设噪声和自变量是独立的，这将模型大大的简化了。比如变量之间其实没有因果关系。有一件事情是很值得思考的，即p-value的值和因果关系的关系是什么？     接下来看本节最后一张图：     图中IGCI，LiNGaM，Additive Noise，PNL均是因果推断的模型，要理解这个这张图，首先得理解横纵坐标的定义，在参考[2]里找到了定义：     Decision rate 是判断了因果的数量/总量，Accuracy是判断对了的数量/总共判断的数量。那么为什么会有一个Decision rate，模型在做因果判断之前，会输出两个值，一个是X是Y的因的自信度，一个是Y是X的因的自信度。让它们做个减法再取绝对值：     通常会为模型设置一个阈值，绝对差值小于这个阈值的，模型不做判断，或者说不采纳模型的判断。因为有这个阈值，所以不是所有的X，Y都会采纳模型输出的因果关系，这样就有了Decision rate。     当然，这个阈值可以是按其他逻辑设计的，不一定非得是上面的逻辑。     由上图可以发现，现在有一些因果推断模型，还是有一定效果的，也许可以提升的空间，还非常非常的大。     下节继续讲restricted structural causal model，会从两个变量的因果归纳，延伸到多个变量，请继续关注吧！ 系列首篇：MIT因果迷你课笔记 —— 相关和因果 上篇：MIT因果迷你课笔记 —— 发现因果关系1 声明：所有图片均来自参考，没有原创图片，公式和定理。 参考： [1] Jonas Peters, University of Copenhagen, Mini-course on Causality, Laboratory for Information &amp; Decision Systems (LIDS) and Models, Inference &amp; Algorithms of the Broad Institute, MIT, 2017 [2] Jing Song, Satoshi Oyama, Masahito Kurihara, Tell cause from effect: models and evaluation, 2017 "
    }, {
    "id": 49,
    "url": "http://localhost:4000/%E6%8D%A2%E4%B8%AA%E6%80%9D%E8%B7%AF%E5%AE%9E%E7%8E%B0%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD_-%E5%9C%A8%E8%A7%86%E8%A7%89%E7%8E%AF%E5%A2%83%E4%B8%AD%E7%94%A8%E5%9B%A0%E6%9E%9C%E5%BD%92%E7%BA%B3%E5%AE%8C%E6%88%90%E7%9B%AE%E6%A0%87%E5%AF%BC%E5%90%91%E7%9A%84%E4%BB%BB%E5%8A%A1-%E7%AD%96%E7%95%A5%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/",
    "title": "换个思路实现人工智能: 在视觉环境中用因果归纳完成目标导向的任务——策略学习模型",
    "body": "2020/08/19 -      上篇：换个思路实现人工智能: 在视觉环境中用因果归纳完成目标导向的任务——因果归纳模型     系列第一篇：换个思路实现人工智能: 在视觉环境中用因果归纳完成目标导向的任务(上)     今天我们来说一说《CAUSAL INDUCTION FROM VISUAL OBSERVATIONS FOR GOAL DIRECTED TASKS》实验中策略学习模型的部分。这篇是这篇论文的终篇。终于从坑里爬出来啦。     官方源代码地址： https://github. com/StanfordVL/causal_induction    策略学习模型 (Learning Goal-Conditioned Policies) python3 learn_planner. py --horizon 7 --num 7 --fixed-goal 0 --structure one_to_one --method trajFi --seen 10 --images 1 --data-dir output/    先回顾一下各个参数的意思：             horizon是上图中H的值，num是开关或灯的数量，–fixed-goal是指学习是否是goal conditioned，如果–fixed-goal=0，那表示有多个目标，是goal-conditioned。structure是开关和灯的控制模式，有如下四类： 每一类都可以组合出非常多不同的因果关系，比如说One-to-One这种情况，如果有七组开关和灯，那么开关和灯之间可以有5040种不同的因果关系。在训练因果模型的过程中，给的训练数据会包含一些因果关系，最后需要测试训练好的agent在它从未见过的因果关系上的效果。seen这个参数用于设定参与因果归纳模型训练的因果关系的数量，在这篇论文中，seen这个值会被设为10，50，100和500来做实验。images这个参数如果设定为1，表示因果模型训练的输入是场景图片，而如果设为0，则因果模型的训练输入是灯的状态向量。data_dir，数据的存储路径。     method是这篇最重要的一个参数，在这篇，method分三大类，第一类是gt，gt是ground truth的意思，即参与策略模型训练的因果关系用的是真实的因果关系，而不是通过因果归纳模型得到的因果关系。第二类则有用因果归纳模型，和上篇的对应是，trajF为TCIN，trajFia是ICIN，trajFi为ICIN (No Attn)，即参与策略模型训练的因果关系是由因果模型推断出来的，而不是绝对正确的。第三类是trajlstm，是2019年在ICLR发表的一篇解决相同问题的方法《Causal Reasoning from Meta-reinforcement Learning》，用的是lstm网络，基本思路是先将所有开关操作一遍，并将场景图和操作输入网络，期望网络能归纳出因果关系，之后再输入场景图和目标图，得到策略。 memsize = 10000memory = {'state':[], 'graph':[], 'action':[]}    memory是训练过程中记录信息的，其中state记录的是当前场景图和目标场景图，graph记录的是因果向量，action记录的是接下来应该要执行的动作，这里要注意，可以把这里的记录的action看成是ground truth，而这个action并不一定会被策略选中，和下一步真正执行的action又是不同的。而memsize则用来控制memory的长度。     策略学习模型调用： if  args. method=='trajlstm':  pol = BCPolicyMemory(args. num, args. structure). cuda()else:  pol = BCPolicy(args. num, args. structure, True). cuda()optimizer = th. optim. Adam(pol. parameters(), lr=0. 0001)    trajlstm用BCPolicyMemory做策略学习模型，其他用BCPolicy做策略学习模型，这两个策略模型相似度高，可以一起看:     1. BCPolicy和BCPolicyMemory：     BCPloclicy要实现的是下面这个策略学习网络，结合了因果归纳和Attention机制，而BCPolicyMemory是一个基于LSTM的策略学习网络。  	1. 1 初始化​	两个网络初始化部分不相同，上图是BCPolicy的网络结构图，有attention机制，BCPolicyMemory没有： def __init__(self, num, structure, attention = False): #BCPolicydef __init__(self, num, structure): #BCPolicyMemory    定义了三个相同的卷积池化激励层和一个全连接层，这部分主要用来做图的encoding，将目标场景图和状态场景图转换成一个向量： self. encoder_conv = nn. Sequential(    nn. Conv2d(6, 8, kernel_size=3, stride=1, padding=1), # 32×32×6-&gt;32×32×8    nn. MaxPool2d(kernel_size=2, stride=2),  # 32×32×8-&gt;16×16×8  nn. ReLU(inplace=True),)self. encoder_conv2 = nn. Sequential(    nn. Conv2d(8, 16, kernel_size=3, stride=1, padding=1), # 16×16×8-&gt;16×16×16    nn. MaxPool2d(kernel_size=2, stride=2),  # 16×16×16-&gt;8×8×16  nn. ReLU(inplace=True),)self. encoder_conv3 = nn. Sequential(    nn. Conv2d(16, 32, kernel_size=3, stride=1, padding=1), # 8×8×16-&gt;8×8×32    nn. MaxPool2d(kernel_size=2, stride=2),  # 8×8×32-&gt;4×4×32  nn. ReLU(inplace=True),)self. fc1 = nn. Linear(4 * 4 * 32, 128) # 4×4×32-&gt;128    BCPolicy的其他部分，不全，只贴有部分： self. attlayer = nn. Linear(128, num) # 128-&gt;numself. softmax = nn. Softmax(dim=-1)   # 对最后那个维度做Softmaxif structure ==  masterswitch :     self. ins = self. num + 1         # masterswitch的gt向量比其他结构的gt向量长num,最后num个位置记录的是master switch的位置。else:    self. ins = self. numif not self. att:                    # 如果没有attention机制  if structure ==  masterswitch :    self. gfc1 = nn. Linear(num*num + num, 128) # num*(num+1)-&gt; 128  else:    self. gfc1 = nn. Linear(num*num, 128) # num*num-&gt;128else:    self. gfc1 = nn. Linear(self. num, 128) # num-&gt;128if self. structure ==  masterswitch :    self. fc2 = nn. Linear(256+args. num, 64) # 256+num -&gt;64else:  self. fc2 = nn. Linear(256, 64) # 256-&gt;64self. fc5 = nn. Linear(64, num) # 64 -&gt;numself. softmax = nn. Softmax(dim=-1) # 对最后一维做softmax    masterswitch的gt向量比其他结构的gt向量长num个，记录的是master switch的位置。     BCPolicyMemory其他部分： self. aenc = nn. Linear(num+1, 128) # num+1 -&gt; 128self. lstm = nn. LSTMCell(256, 256)  self. fc2 = nn. Linear(256, 64) # 256-&gt;64self. fc5 = nn. Linear(64, num) # 64-&gt;num    这段代码的核心是lstm的定义，用的是LSTMCell，这篇就不提LSTM的网络结构和特点了，只说一下，pytorch中除了LSTMCell，还有一个LSTM，可以直接构造多层LSTM。那LSTMCell不能直接构造多层结构，即一个Cell，第一个参数是feature的长度，第二个参数是记忆单元和隐藏单元hidden的长度。   1. 2 BCPolicy和BCPolicyMemory前向传播forward部分：     不同：函数入口 def forward(self, x, gr):  # BCPolicydef forward(self, x, a, hidden): # BCPolicyMemory    两个网络的输入x都是goal image和current image组合的32×32×6的tensor。gr是开关和灯的因果关系向量，当method是gt的时候，gr是真实的因果关系向量，是groud truth。而当method是trajF，trajFia，或trajFi时，gr则是因果归纳模型归纳出的因果关系向量。     当method是trajlstm时，用的策略模型是BCPolicyMemory，其中x是场景图，a是动作，hidden则是LSTM的记忆单元和隐藏单元。    相同：如网络结构图，goal image和current image组合的32×32×6的tensor` x都会经过一个Observation Encoder输出e3,再经过全连接层输出encoding: x = x. permute(0, 3, 1, 2). contiguous() # 维度换位，将通道维度移前e1 = self. encoder_conv(x)       # 32×32×6-&gt;16×16×8e2 = self. encoder_conv2(e1)     # 16×16×8-&gt;8×8×16e3 = self. encoder_conv3(e2)     # 8×8×16-&gt;4×4×32e3 = e3. view(e3. size(0), -1)    # 转成一维向量，前面是batchsize, 后面是向量长度encoding = self. relu(self. fc1(e3))  # 4×4×32-&gt;128    BCPolicy的其他部分： if self. att:  w = self. softmax(self. attlayer(encoding)) # 128-&gt;num, softmax    if self. structure ==  masterswitch : # 将master switch的因果关系与其他因果关系的分开    ms = gr. view((-1, self. ins, self. num))[:, -1, :]     gr = gr. view((-1, self. ins, self. num))[:, :-1, :]  else:    gr = gr. view((-1, self. ins, self. num))     gr_sel = th. bmm(gr, w. view(w. size(0), -1, 1)) # 因果关系矩阵×状态和目标向量 -&gt;num    gr_sel = gr_sel. squeeze(-1)   # 若最后一维长度为1,消去  g1 = self. relu(self. gfc1(gr_sel)) # num-&gt;128else:    g1 = self. relu(self. gfc1(gr))    # num*(num+1)-&gt;128 或者 num*num-&gt;128if self. structure ==  masterswitch :  eout = th. cat([g1, encoding, ms], 1) # 128 + 128 + numelse:    eout = th. cat([g1, encoding], 1)      # 128 + 128a = self. relu(self. fc2(eout))   # 256+num-&gt; 64 或者 256 -&gt; 64 a = self. fc5(a)  # 64 -&gt; numreturn a    如果有attention机制，首先会让状态场景图和目标场景图融合生成的encoding经过一个全连接层，生成因果关系图的attention向量，和因果关系相乘，得到需要重点关注的因果关系，然后再转换成因果向量，和encoding融合后通过两层全连接层得到最后的动作向量。 ​	归纳为公式，则是： s是状态场景图，g是目标场景图，E对应的就是网络结构图中的Observation Encoder，φi是全连阶层，α是attention向量，Ĉ是因果归纳模型预测的因果关系，e是selected edges，a是模型提议的策略。 BCPolicyMemory其他部分： ae = self. relu(self. aenc(a)) # num+1 -&gt; 128eout = th. cat([ae, encoding], 1)  # 128+128-&gt;256if hidden is None:  hidden = self. lstm(eout)else:  hidden = self. lstm(eout, hidden) # （256，256）a = self. relu(self. fc2(hidden[0])) # 256-&gt;64a = self. fc5(a)  # 64 -&gt; numreturn a, hidden    这里每次都会输入一个action向量，将其转换成128长的向量，然后再和128长的场景图向量拼接在一起组成一个256长的向量，最开始的时候hidden不做输入，之后就用上一次跑lstm产生的hidden来做输入，这里的hidden包含了lstm中的隐藏单元和记忆单元，接下来提取隐藏单元，通过两层全连接网络生成动作向量。 ​	2. 策略训练代码：     method有三大类，gt是一类，gt是ground truth的意思，也就是说参与策略模型训练的因果关系用的是真实的因果关系，而第二类method，trajF，trajFi，或trajFia，参与训练的因果关系则是因果模型计算得到的因果关系，不是真正的因果关系。第三类method是trajlstm，这个方法没有独立的因果归纳模型，但是其实有因果归纳的步骤。每一类都有一组训练代码，重合度很高，所以一起讲。     2. 1 初始化环境：     三类method都是相同的 l = LightEnv(args. horizon*2, args. num,  gt , args. structure, gc, filename=fname, seen = args. seen)l. keep_struct = False    和之前初始化环境不同的是horizon位置上的输入变成了2×args. horizon，决定了l. step()的最长步数。cond的输入变成了”gt”，l. _get_obs()函数输出的状态向量o会包含l. gt，即现在这个环境使用的因果向量，而l. _get_obs(images=True)只会返回当前因果和开关状态下的场景图。如果l. keep_struct = False，那么每次l. reset()都有可能会改变因果关系     当method是trajlstm是，memsize设定为了100。 memsize = 100    2. 2 策略训练训练：     相同的部分： for mep in range(100000):  l. train = True    l. reset()    imobs = l. _get_obs(images=True)    goalim = l. goalim    goal = l. goal    obs = np. zeros((args. num))    l. train为真，那么l. reset()的时候，会从训练模型可见的那些因果关系中选择一个因果关系来训练，seen为训练模型过程中可见的因果关系的数量，这部分详细内容请参考换个思路实现人工智能: 在视觉环境中用因果归纳完成目标导向的任务——生成数据。imobs获得的是l在reset之后获得的新的场景图。goalim是目标场景图，goal是灯的状态向量。     不同的部分：     当method是trajF，trajFi，或trajFia时： ## Predict Graphbuf = induction(args. structure,args. num, args. horizon, l, images=args. images)traj = buf. flatten()pred = predict(buf, FN, args. structure, args. num)l. state = np. zeros((args. num))     induction函数与因果模型训练的代码重合度很高，就不逐行解析了，这部分主要功能就是将每个开关都开一遍，然后记录下场景图和之后要执行的动作，存入buf中，而当结构是masterswitch的时候，稍微复杂了一些，先将所有开关开一遍并记录下场景图和动作，直到找到master switch为止，再将剩下的开关逐个开一遍，记录下场景图和之后要执行的动作。predict中输入的FN是之前训练好的因果归纳模型，输出则是因果关系向量。     当method是trajlstm时： memory = {'state':[], 'graph':[], 'action':[]}hidden = None## Get interction trajectorybuf = induction(args. structure,args. num, args. horizon, l, images=args. images)memory['graph']. append(buf)for w in range(buf. shape[0]):  states = buf[w, :32*32*3]. reshape(1, 32, 32, 3)  sgg = np. zeros_like(states)  states = np. concatenate([states, sgg], -1)  actions = buf[w, 32*32*3:]. reshape(1, -1)  act, hidden = pol(th. FloatTensor(states). cuda(), th. FloatTensor(actions). cuda(), hidden)l. state = np. zeros((args. num))    之前说过induction函数，即把所有的开关按顺序开一遍，记录场景图和之后的动作，在trajlstm这个方法中，直接将这样产生的buf存入memeory中的graph部分，意思很明显了，即buf里面有因果图。接下来就是跑lstmcell，将这个开关所有灯的行为依次输入lstm，states是当下的场景图，而actions是接下来要做的动作，输出也是接下来要做的动作。这部分的主要功能应该是让hidden中有灯和开关的因果关系。     每个episode: for k in range(args. horizon*2):    g = np. abs(goal - obs[:args. num])  # 目标向量-状态向量    st = np. concatenate([imobs, goalim], 2)  #32×32×6  sss = 1. 0*(np. dot(g, l. aj. T). T &gt; 0. 5)   if args. structure ==  masterswitch :        sss[l. ms] = 0    if sss. max() == 0:    #如果没有需要改变的开关，就结束该过程            break    action = np. argmax(sss)     #应该要执行的action  if args. structure ==  masterswitch :        if obs[:5]. max() == 0:    #里面的参数5似乎应该改成args. num            action = l. ms         #首先要打开masterswitch的开关    memory['state']. append(st)  memory['action']. append(action)    g是目标向量减去状态向量，表示要达到目标向量需要做出的改变。st则融合了当前场景图和目标场景图，在通道维度上融合，因此融合后得到的st是32×32×6的结构。g是需要作出改变的灯向量，sss则是需要做出改变的开关向量。如果是masterswitch的状态，一开始除了master switch之外没有开关再需要改变了，那么过程不需要再进行下去。如果有开关需要改变，则一开始要打开master switch，否则其他开关开了也没用。这里obs[:5]中的5似乎应该改成args. num。这边的action则是应该要被选择的action。     接下来一段代码，三种方法不同：     1. method == “gt”: memory['graph']. append(l. gt. flatten())if np. random. uniform() &amp;lt; 0. 3:  action = np. random. randint(args. num)else:  graph = np. expand_dims(l. gt. flatten(), 0)  act = pol(th. FloatTensor(np. expand_dims(st, 0)). cuda(), th. FloatTensor(graph). cuda())    action = act[0]. argmax()    # 策略选择的action    当method是gt时，memory[‘graph’]存的是真实的因果关系向量。st和graph经过pol函数产生action向量，然后再取最佳action。这边是策略选择的最后会被执行的action。     在强化学习模型训练过程中，不会总去执行策略模型当下觉得最优的动作，也会随机选择一些动作，以便最后能找到最优的策略。     2. method == “trajF”，”trajFi”，或”trajFia” memory['graph']. append(pred. flatten())## Random Noiseif np. random. uniform() &amp;lt; 0. 3:  action = np. random. randint(args. num)else:  graph = np. expand_dims(pred. flatten(), 0)  act = pol(th. FloatTensor(np. expand_dims(st, 0)). cuda(), th. FloatTensor(graph). cuda())  action = act[0]. argmax()    这个循环内因果关系并不会变，所以graph可以放在这个循环外生成，没必要每次都产生一遍。     3. method = “trajlstm”时： ## Policy Noiseif np. random. uniform() &amp;lt; 0. 3:   action = np. random. randint(args. num)else:  act, s_hidden = pol(th. FloatTensor(states). cuda(), th. FloatTensor(actions). cuda(), hidden)    action = act[0]. argmax()    这段代码应该是写错了，每次都输入states和actions，并没有用到每次l. step之后的状态，也没有用到goal image，有点不合逻辑。因此，正确的应该是要将states替换成st，而actions替换成都是0的向量，因为这里是要去预测下一步的动作。     接下来一段三种方法相同： obs, reward, done, info = l. step(action)imobs = l. _get_obs(images=True)if done:  break    在环境中执行action，返回obs（更新灯的状态向量+目标向量+因果向量），reward（目标和状态的欧几里德距离的负值），done（达到目标或者步数超过限制则结束），info（灯的状态向量是否和目标一致），imobs（更新状态场景图），如果达到目标，或者步数超过限制，则结束该过程。     接下来，不同的部分，当method是gt，trajF，trajFi，或trajFia时： if args. structure ==  masterswitch :  if sss[l. ms]:    st = np. concatenate([imobs, goalim], 2)    memory['state']. append(st)        #--------------下面代码是method不同，不同的部分-------------        # 当method是“gt”        memory['graph']. append(l. gt. flatten())         # 当method是 trajF ， trajFi ，或 trajFia 时        memory['graph']. append(pred. flatten())         #---------------------------------------------    memory['action']. append(l. ms)    obs, reward, done, info = l. step(l. ms)memory['state'] = memory['state'][-memsize:]memory['graph'] = memory['graph'][-memsize:]memory['action'] = memory['action'][-memsize:]for _ in range(1):  loss = train_bc(memory, pol, optimizer)    这段代码有两点很让人疑惑：     1. 首先是masterswitch的部分，按照代码的意思是，如果结构是masterswitch且master switch被打开了，那么则需要将主开关关上。          2. 不论因果结构是什么样的，在最后一个action之后的状态都没有被记录。可能是没有下一步动作的话，场景图记录下来也没有训练价值。     这部分有个函数train_bc： def train_bc(memory, policy, opt):  '''Train Imitation policy'''  if len(memory['state']) &amp;lt; 50:    return    opt. zero_grad() # 梯度清零，不清零的话可以写梯度累加的代码，适合GPU配置低，显存小的状况。  choices = np. random. choice(len(memory['state']), 32). astype(np. int32). tolist()  states = [memory['state'][c] for c in choices]  graphs = [memory['graph'][c] for c in choices]    actions = [memory['action'][c] for c in choices]  states = th. FloatTensor(states). cuda()  graphs = th. FloatTensor(graphs). cuda()  actions = th. LongTensor(actions). cuda()    # 模型觉得最优的动作  pred_acts = policy(states, graphs)  # loss = ((pred_acts - actions)**2). sum(1). mean()  celoss = nn. CrossEntropyLoss()  loss = celoss(pred_acts, actions)  l = loss. cpu(). detach(). numpy()  loss. backward()  opt. step()  return l    zero_grad()是用于梯度清零，不清零的话梯度就会累加，在一些实验中，batch size设置的大，显存会不够用，所以可以不清零梯度，多累加几个batch size，比如将batch size设置为100，而梯度累加八次再清零和将batch size直接设置为800，每次都梯度清零，效果是一样的。     在这个实验中，每次从memory中随机选择32个来计算梯度。     当method是”trajlstm”时： if args. structure ==  masterswitch :  if sss[l. ms]:    st = np. concatenate([imobs, goalim], 2)    memory['state']. append(st)    memory['action']. append(l. ms)    obs, reward, done, info = l. step(l. ms)if len(memory['state']) != 0:  trajs. append(memory)trajs = trajs[-memsize:]for _ in range(1):  loss = train_bclstm(trajs, pol, optimizer)    这里依然是，如果结构是masterswitch且主开关开着，就需要多走一步，将主开关关上。接下来看train_bclstm这个函数，trajs是最近的memsize个开关灯的过程，一开始会小于memsize个。pol是生成action的模型。下面贴了一部分代码，掐头去尾。 choices = np. random. choice(len(trajs), 4). astype(np. int32). tolist()for t in choices:  memory = trajs[t]  hidden = None  ## Feed interaction trajectory through policy with memory  buf = memory['graph'][0]  for w in range(buf. shape[0]):    states = buf[w, :32*32*3]. reshape(1, 32, 32, 3)    sgg = np. zeros_like(states)    states = np. concatenate([states, sgg], -1)    actions = buf[w, 32*32*3:]. reshape(1, -1)    num_acts = actions. shape    act, hidden = pol(th. FloatTensor(states). cuda(), th. FloatTensor(actions). cuda(), hidden)  states = np. array(memory['state'])  actions = np. array(memory['action'])  preds = []  for w in range(states. shape[0]):        a = np. zeros(num_acts)    pred_acts, hidden = pol(th. FloatTensor(states[w:w+1]). cuda(), th. FloatTensor(a). cuda(), hidden)    preds. append(pred_acts)  preds = th. cat(preds, 0)  loss = celoss(preds, th. LongTensor(actions). cuda())  totalloss += loss    前半段代码中的循环，应当是为了得到hidden的值，理想状况，我们期待这个值记录着接下来要做的动作，和灯和开关之间的因果关系。而接下来一个循环，则是要生成理想状况下应当产生的动作组和在该策略模型下产生的动作组的差异。损失函数用的是交叉熵。     接下来是最后一段代码，三个方法有不同的部分：   if mep % 1000 == 0:    print( Episode , mep,  Loss:  , loss )        # method是“gt”时：    trainsc = eval_bc(pol, l, True, args=args)          testsc = eval_bc(pol, l, False, args=args)              # 当method是 trajF ， trajFi ，或 trajFia 时：    trainsc = eval_bc(pol, l, True, f=FN, args=args)    testsc = eval_bc(pol, l, False, f=FN, args=args)    # 当method是 trajlstm 时：        trainsc = eval_bclstm(pol, l, True, args=args)        testsc = eval_bclstm(pol, l, False, args=args)  successes. append(l. _is_success(obs))print(np. mean(successes))    要是之前的代码都看懂了，eval_bc和eval_bclstm很容易懂，就不专门贴出来了，输入pol，用来计算action，而第三个参数输入True，表示用用于训练的因果关系来测试模型效果，输入False，表示用用于测试的因果关系来测试模型效果，用于测试的因果关系是训练模型过程中，模型没有接触过的因果关系。 3. 实验结果     这个图表现的是策略学习模型的实验结果，Memory指的是trajlstm，Oracle是gt，其他分别对应不同的因果归纳模型，毫无疑问，所有实验中，Oracle的效果是最好的，因为因果关系用的是ground truth。接下来就是用ICIN(Ours)了，即带attention的因果归纳模型。值得注意的是，这个实验结果用的是因果归纳模型和策略学习模型没有见过的因果关系来测试的。至于Memory和Memory(RL/Low Dim)的区别，在代码中没有体现出来，trajlstm只有一个版本。     另外，这是当switch的数量是5，训练时可见的因果关系为50时，网络有attention和没有attention的区别。 记：     1. 文章并没有把所有代码都贴上了，只贴了觉得有必要说明的，另外代码中有很多不必要的部分，可能是作者测试的时候遗留的，因此有些代码稍作了修改。          2. trajlstm的代码应该是写错了。                3. obs[:5]中的5似乎应该改成args. num。     上篇：换个思路实现人工智能: 在视觉环境中用因果归纳完成目标导向的任务——因果归纳模型     上上篇：换个思路实现人工智能: 在视觉环境中用因果归纳完成目标导向的任务——生成数据     系列第一篇：换个思路实现人工智能: 在视觉环境中用因果归纳完成目标导向的任务(上) 参考论文：  Suraj Nair, Yuke Zhu, Silvio Savarese, Li Fei-Fei, Stanford University, CAUSAL INDUCTION FROM VISUAL OBSERVATIONS FOR GOAL DIRECTED TASKS, 2019参考代码：  https://github. com/StanfordVL/causal_induction"
    }, {
    "id": 50,
    "url": "http://localhost:4000/Cross-Entropy-Loss/",
    "title": "Cross Entropy Loss",
    "body": "2020/08/03 -     交叉熵度量的是两个概率分布的差异。     要理解交叉熵，有很多小概念需要理解。     信息量，一个事件发生的概率越大，事件发生携带的信息量越小，发生的概率越小，事件发生携带的信息量越大。比如太阳从东边升起，这个事件如果发生了，我们可以从这个事件中获得的信息几乎是没有的。但是，如果哪天太阳从西边升起了，那么我们从这个事件中获得的信息量是极大的，一定发生了什么，或者即将发生什么，才造成了这个事件发生。     假设X是一个离散型随机变量，概率分布函数为：     则定义X = x0事件发生携带的信息量为：     若p(x0)为0，也就是事件x0是不可能发生的事件，但是它却发生了，那么这个事件的信息量是无穷大的，I(x0)的值无穷大，如果p(x0)为1，也就是事件x0是一定会发生的事件，那么这个事件的发生是不带信息量的，I(x0)的值是0。     另外，信息量可以这样理解 [2]： Information quantifies the number of bits required to encode and transmit an event.     信息量可以被理解为，传输或表达这个信息需要的编码的位数。     信息熵，则是信息量的期望值：     参考[3]里给了一个例子：     每次开电脑，都可能会产生三种状况，电脑正常开机的概率是0. 7，电脑无法开机的概率是0. 2，电脑爆炸的概率是0. 1。那么每次开机，得到的信息量的的期望值是：         如果是二项分布，那么信息熵的计算可以简化为：     事件的概率不均，信息熵较小，若各个事件发生的概率一样，信息熵较大。 ​	信息熵在[2]中的描述为： Entropy is the number of bits required to transmit a randomly selected event from a probability distribution.     熵是表达或者传输一个遵循特定概率分布的随机事件需要的位数，个人觉得the number of应该改成the average number of，位数前面应该有个平均，和期望的概念对应上。但也可能是我哪里理解错了。 ​	相对熵(relative entropy)，又称之为KL散度(Kullback-Leibler (KL) divergence)，公式：     相对熵的目标是：计算用P描述目标问题，比Q描述目标问题能获得的信息增量。     如果分布P和分布Q是一样的，那么相对熵是0，如果不一样，相对熵大于0，越大，表示两种分布之间的差距越大。     在机器学习的项目中，通常P表示真实的分布，即需要训练模型达到的分布，Q是现在用的模型预测的分布。     相对熵在参考[2]中的描述是： In other words, the KL divergence is the average number of extra bits needed to encode the data, due to the fact that we used distribution q to encode the data instead of the true distribution p.                                       — Page 58, Machine Learning: A Probabilistic Perspective, 2012.     相对熵是，当我们用分布q来替代事件真实遵循的分布p时，传输和表达事件时，比使用分布p多需要的平均位数。也就是q是我们用的分布，p是事件真实遵循的分布，用q的话，比用p需要更多的位数来表达和传输这个事件，平均多多少呢，相对熵就是这个多出来的多少。     交叉熵(Cross entropy)，将相对熵公式变形：     前半部分是信息熵的负值，后半部分则是交叉熵，交叉熵的公式是：     因为P的信息熵是一定的，那么其实是可以省略这部分计算的，交叉熵和相对熵的意义是一样的。只是最后计算出的值，区间不一样。     交叉熵在参考[2]中的描述是： …, the cross entropy is the average number of bits needed to encode data coming from a source with distribution p when we use model q, … — Page 57, Machine Learning: A Probabilistic Perspective, 2012     交叉熵是当你用模型q来预测分布p时，表达和传输事件需要的平均位数。 以下定义来自参考[2]，俺就不翻了:  Cross-Entropy: Average number of total bits to represent an event from Q instead of P.  Relative Entropy (KL Divergence): Average number of extra bits to represent an event from Q instead of P.     Cross-Entropy Loss 和 Softmax Loss     毫无疑问，交叉熵可以用作损失函数，且比起MSE，MAE，要优秀不少， … using the cross-entropy error function instead of the sum-of-squares for a classification problem leads to faster training as well as improved generalization. — Page 235, Pattern Recognition and Machine Learning, 2006.      结合上面猫狗分类的案例，假如有一张猫图输入，P是[1, 0], Q是[0. 71, 0. 29]，交叉熵的计算为：     H(P, Q) = – (P(cat) * log(Q(cat)) + P(dog) * log(Q(dog)))     值得注意的是，在很多多分类问题中，不论有多少类，P不论有多少个元素，都只有一个为1，其他都为0，所以交叉熵的计算可以化简，也就是说如果P(cat)为1，那么交叉熵的结果和Q(dog)，Q(car)，Q(any other)是无关的：     H(P, Q) = –  log(Q(cat))     因此，如果Q(cat)是用Softmax Function计算出来的，那么H(P, Q)计算得到的就是该样本在该模型下的Softmax Loss。     Softmax Function专门有一篇：Softmax     Softmax Loss的完整公式如下：     N是样本数量，n是class的数量，特征向量的长度为d，Wj是W的第j列，和b一起是获得特征向量的全连接层，W是d*n，bj的长度是n。log后面则是用Softmax Function计算出的‘Q(cat)’。     因此，其实本来没有什么Softmax Loss的概念，这个公式是在多分类任务中使用Softmax Function+Cross Entropy loss产生的。     Cross Entropy Loss 和 Log Loss [4] ​	她俩在解决二分类问题的时候，其实是一回事，不服气的看公式：     这个公式既叫Binary Cross-Entropy，也叫Log Loss，y是label，p(y)是预测结果，符号和之前的公式没有一一对应，看的时候注意一下。     Log Loss的推导基于最大似然估计(Maximum Likelihood)和伯努利分布(Bernoulli distribution)，有机会写一篇。     那么Cross Entropy的值为多少时，表示预测的结果还挺准确的呢？这里参考[2]给了个一些参考，具体问题还要具体分析。  Cross-Entropy = 0. 00: Perfect probabilities.  Cross-Entropy &lt; 0. 02: Great probabilities.  Cross-Entropy &lt; 0. 05: On the right track.  Cross-Entropy &lt; 0. 20: Fine.  Cross-Entropy &gt; 0. 30: Not great.  Cross-Entropy &gt; 1. 00: Terrible.  Cross-Entropy &gt; 2. 00 Something is broken. 参考： [1] Thomas Wood，Softmax Function Definition, DeepAI [2] Jason Brownlee，A Gentle Introduction to Cross-Entropy for Machine Learning，2019 [3] 史丹利复合田，一文搞懂交叉熵在机器学习中的使用，透彻理解交叉熵背后的直觉，CSDN，2018 [4] Daniel Godoy, Understanding binary cross-entropy / log loss: a visual explanation, Towards Data Science, 2018 "
    }, {
    "id": 51,
    "url": "http://localhost:4000/MIT%E5%9B%A0%E6%9E%9C%E8%BF%B7%E4%BD%A0%E8%AF%BE%E7%AC%94%E8%AE%B0-%E5%8F%91%E7%8E%B0%E5%9B%A0%E6%9E%9C%E5%85%B3%E7%B3%BB1/",
    "title": "MIT因果迷你课笔记 —— 发现因果关系1",
    "body": "2020/07/14 - 上上篇：MIT因果迷你课笔记 —— 相关和因果 上篇：MIT因果迷你课笔记 —— 因果语言和因果推理     在这门课中，因果模型的形成主要包含因果关系的学习和发现(causal learning/discovery)，还有因果关系的推理(causal reasoning)。     课程里先说的是因果关系的推理，即有了因果模型，去推导结果或者干预后的状况。接下来说因果关系的发现。     因果关系的发现内容复杂，逻辑绕，可能会分两到三篇来说，这里是第一篇。     因果关系的发现，即从数据中发现因果关系，那么怎么从数据中归纳因果关系？         首先来了解一些基本概念。     RCCP **(Reichenbach’s Common Cause Principle), 是HansReichenbach死后出版的一本未完成的书《The Direction of Time**》中提出的概念。假设我们现在有两个事件，A和B，如果p(A∩B)&gt;p(A)p(B)说明事件A和B不相互独立，即两个事件相关，那么相关和因果是什么关系呢？Reichenbach’s Common Cause Principle (RCCP) 认为如果p(A∩B)&gt;p(A)p(B)，且A不是B的因，B也不是A的因，那么A，B应该有共同的因。     概念定义如下：     A、B是两个事件，如果：       遵循RCCP原理，则A是B的因，或者B是A的因，或者A，B有共同的因，在第三种情况下，如果共同因事件是C，那么A和B则是有条件的独立：     RCCP的概念还有很多可追究的地方，推荐斯坦福的一篇文章，这篇文章甚至还描述了RCCP在量子力学中的地位，有机会可以专门开个坑说一下这些概念，https://plato. stanford. edu/entries/physics-Rpcc/。     接下来说一下图的定义：     图由点和边构成，图可以是有向图和无向图，这个课程里的图基本都是有向图，在图形上的表现就是边是有箭头的，有指向的。v-structure是说，比如X2和X3均指向X1，但是X2和X3之间却没有直接的关联，这种结构就被称之为immorality。     d-separation, 一个和图还有RCCP相关的非常重要的概念，因为相对复杂，所以专门写了一篇**D-separation，d-connection**，一定要先看这篇。     Causal Markov Condition 有很多种定义方式，基于d-separation的定义方式如下：     换种写法如下，P是变量的分布情况（distribution），w. r. t. (with respect to) G表示这组变量之间的因果关系是按G图表示的那样：     我们认为分布P是Markov的，如果Xi和Xj在图G中被S d-separated能推导出Xi⊥Xj |S。在这里我们用⊥代替图中的独立符号，因为图中的独立符号在这里打不出来。     我们回想一下RCCP的概念，则不难得出，马可夫状态是RCCP的一般化的形式。另外，马可夫状态还有其他两种定义方式：     其中Y是一组变量的集合，PA(X)指X的所有的父变量的集合，ND(X)指除了X变量和其后裔变量之外的所有变量的集合。下面这种是基于分解的定义方式：     这几种定义方式是等价的。     假设现在你获得了一批数据，你可以很容易得到各个变量的分布，即P，换言而之，你可以很轻松地知道Xi⊥Xj |S是否成立，现在你想通过分布推导出因果关系，那么可以用前面的Causal Markov Condition吗？明显不行，因为箭头是单向的，可以由d-separation推导出分布，但不能根据上面的定理推导出d-separation。     为了可以反过来推导，我们定义了一个假设：     faithfulness assumption，这个概念和马可夫是一个对应的概念，注意下面推理的箭头，和马可夫概念中推理的箭头是反方向。这个假设使我们可以通过分布来推导因果关系。如果可以反过来推，我们认为P faithful。     现在我们来做几道题，辅助理解一下上面的两个概念，我们假设P是markov且faithful w. r. t. G，现在P有如下几种状况，请分别推导出对应的因果图G：     （i）X⊥Z **(vars: **X, Y,Z)     （ii）X⊥Y|Z **(vars: **X, Y,Z)     （iii）X⊥Y，X⊥W|Z **，Y⊥W|Z ，X⊥W|Z, **Y，Y⊥W|Z, **X **(vars:X,Y,W, Z)     以上三种情况对应的G可以是（一种d-separation的状况可以匹配多个因果图）：     （i）X-&gt;Y&lt;-**Z **       （ii）X-&gt;Z-&gt;Y；X&lt;-Z&lt;-Y；X&lt;-Z-&gt;Y。     （iii）(X-&gt;Z)&amp;(Y-&gt;Z-&gt;W)     要得到因果图，从P中，首先找出所有的独立关系，和相对独立的关系，再根据Markov和Faithfulness的假设得到因果关系。     通过以上的例子，我们发现，一种d-separations的情况可以得到不同的因果图： 我们称这些由同样的d-separations的状况得到的因果图为Markov Equivalence的，反过来说就是，     Def：如果两个因果图能得到同样的d-separations，则称这两个图为Markov Equivalence的，是基于马可夫等价的。     再来看由此得到的一个命题：     Prop：(Verma &amp; Pearl)因果关系图G，H是Markov Equivalence &lt;=&gt; G和H有相同的骨架，和v-structrues。     可以对照上面三个案例来验证一下这个命题。     有P对于某个G不是faithful的案例吗？当然是有的，如果G是这样的：     那么，Y =2X - Z + Ny = 2X - 2X - Nz+ Ny = Ny  - Nz，所以X⊥Y，但是却并不能得到X和Y被一个空集d-separated的结果。简而言之，faithfulness的假设将这个世界的状况简化了，真实世界的数据是十分复杂的。     接下来，我们来回顾我们之前见过的一个案例：     案例：夜间照明和小孩患近视的因果关系     由统计数据可以知道，为孩子提供夜间照明和小孩患近视这两件事情是相关的。根据RCCP原理，要么这两件事存在因果关系，要么它们有共同的因。而通过调查得到一个d-separation，night light ⊥ child myopia | parent myopia，即night light (NL)和child myopia (CM)可以被parent myopia (PM) d-separated。所以，根据faithfulness的假设，这三件事之间的关系可能是：     (i) NL-&gt;PM-&gt;CM     (ii) CM-&gt;PM-&gt;NL     (iii) CM&lt;-PM-&gt;NL     由于(i)和(ii)可以用常识来否定，给孩子的夜间照明不能够成为大人近视的因，孩子的近视不能够成为大人近视的因，因此最后得到的因果关系是：     以上，我们知道，通过数据分布P得到的d-separations的状况，可以让我们得到一组Markov Equivalence的因果图，那么接下来一个显而易见的问题是，如何区分Markov Equivalence的因果图。如上例，我们怎么知道X，Y，Z之间的因果关系是X-&gt;Z-&gt;Y，或是X-&gt;Z-&gt;Y，或是X&lt;-Z-&gt;Y，显然，在同一个现实场景中，这三种因果关系不可能同时成立。那么究竟是哪一个呢？如何才能知道是哪一个呢？除了像上例一样依靠常识来排除，还有其他可行的方法吗？ 上篇：MIT因果迷你课笔记 —— 因果语言和因果推理 上上篇：MIT因果迷你课笔记 —— 相关和因果 声明：所有图片均来自参考，没有原创图片，公式和定理。 参考： [1] Jonas Peters, University of Copenhagen, Mini course on Causality, Laboratory for Information &amp; Decision Systems (LIDS) and Models, Inference &amp; Algorithms of the Broad Institute, MIT, 2017 [2] https://www. andrew. cmu. edu/user/scheines/tutor/d-sep. html [3] https://plato. stanford. edu/entries/physics-Rpcc/ [4] Judea Pearl and Dana Mackenzie, The Book of Why, 2018 "
    }, {
    "id": 52,
    "url": "http://localhost:4000/Softmax/",
    "title": "Softmax",
    "body": "2020/07/09 -     Softmax，这个概念参考[1]解释的非常仔细，这里只做简述，先上公式： 这公式是啥子意思呢？先看一个应用：     假如我们现在有一个分类任务，如果模型足够理想，输入一张猫图，输出[1, 0]，输入一张狗图，输出[0, 1]。通常这种任务，前面会是一个深度卷积神经网络，最后会有一个全连接层，经过这个全连接层会得到图的特征向量(embedding)，我自己喜欢管embedding叫特征向量。上图中最后得到的特征向量是[1. 2, 0. 3]，再经过softmax： 得到了[0. 71, 0. 29]，我们可以这样理解最后这个结果，这张图是猫的概率是0. 71，是狗的概率是0. 29，它们两加起来是1，不管softmax的输入向量为何，输出向量里的值相加一定是1，得到的结果可以理解为图在各个类上的概率分布，向量的长度即类别（class）的数量。再以一个长度为3的一维向量为例： 假设你现在要训练一个模型，它需要有能力分辨猫，狗，鸟，你模型训练好以后，输入了一张鸟图，得到了一个这样的特征向量，现在要做softmax，计算步骤如下：      所以按模型判断，该图是鸟图的概率是0. 0003，明显这个模型不准诶。     softmax还可用于增强学习（reinforcement learning），先上公式：     在机器视觉领域，softmax的输出，是图像在各个类别上的概率分布，在增强学习领域，softmax的输出可以是各个策略的在某个步骤或时间会被采取的可能性。公式中a是我们可以选的行动，t表示的是步骤或者时间，τ是系统温度，这个值越高，模型越冲动，越会去探索新的可能。qt(a)是从现在已知的情况来看，选择行动a会获得成功的概率，Pt(a)则是模型在t这个步骤或时间上会采取a行动的概率。     想象一下，我们正在训练一种强化学习模型。我们必须配置一个系统温度τ，它是系统随机行动的可能性。该系统目前有两个选项：打Ace或打King。根据过往经验，打Ace成功的概率是80％，打King成功的概率是20％。我们将温度τ配置为2。     那么在这一轮中，模型打Ace的概率是：     这意味着尽管该模型目前有80％的把握确定打Ace是正确的策略，但只有57％的可能性使用该策略。这是因为在强化学习中，我们为探索（测试新策略）和开发（使用已知策略）均分配了价值。如果我们提高温度，则该模型将变得“更具冲动性”：更有可能探索新策略，而不是使用最有可能获胜的策略。 声明：图文，案例均来自参考，本篇仅是翻译和摘要。 参考： [1] Thomas Wood，Softmax Function Definition, DeepAI ## "
    }, {
    "id": 53,
    "url": "http://localhost:4000/%E6%8D%A2%E4%B8%AA%E6%80%9D%E8%B7%AF%E5%AE%9E%E7%8E%B0%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD_-%E5%9C%A8%E8%A7%86%E8%A7%89%E7%8E%AF%E5%A2%83%E4%B8%AD%E7%94%A8%E5%9B%A0%E6%9E%9C%E5%BD%92%E7%BA%B3%E5%AE%8C%E6%88%90%E7%9B%AE%E6%A0%87%E5%AF%BC%E5%90%91%E7%9A%84%E4%BB%BB%E5%8A%A1-%E5%9B%A0%E6%9E%9C%E5%BD%92%E7%BA%B3%E6%A8%A1%E5%9E%8B/",
    "title": "换个思路实现人工智能: 在视觉环境中用因果归纳完成目标导向的任务——因果归纳模型",
    "body": "2020/07/03 -     上篇：换个思路实现人工智能: 在视觉环境中用因果归纳完成目标导向的任务——生成数据     上上篇：换个思路实现人工智能: 在视觉环境中用因果归纳完成目标导向的任务(上)     今天我们来说一说《CAUSAL INDUCTION FROM VISUAL OBSERVATIONS FOR GOAL DIRECTED TASKS》实验中训练归纳模型的部分。     官方源代码地址： https://github. com/StanfordVL/causal_induction    1. 归纳模型训练(Train Induction Model) python3 trainF. py --horizon 7 --num 7 --fixed-goal 0 --structure one_to_one --type iter --images 1 --seen 10 --data-dir output/    先说一下各个参数的意思：             horizon是上图中H的值，num是开关或灯的数量，–fixed-goal是指学习是否是goal-conditioned，如果–fixed-goal=0，那表示有多个目标，是goal-conditioned。structure是开关和灯的控制模式，有如下四类： 每一类都可以组合出非常多不同的因果关系，比如说One-to-One这种情况，如果有七组开关和灯，那么开关和灯之间可以有5040种不同的因果关系。在训练的过程中，给的训练数据会包含一些因果关系，最后需要测试训练好的agent在它从未见过的因果关系上的效果。seen这个参数用于设定参与因果归纳模型训练的因果关系的数量，在这篇论文中，seen这个值会被设为10，50，100和500来做实验。images这个参数如果设定为1，表示会存储场景图片，而如果设为0，则不会存储观察到的场景图，而是灯的状态向量。data_dir，数据的存储路径。     上面这些参数，上篇生成数据中都有出现，这篇多出的是type，用来指定模型，有三个模型可指定，分别是Iterative causal induction network (ICIN)，temporal convolutions (TCIN)，iterative model without the attention mechanism (ICIN (No Attn))。对应的参数分别是iter_attn，cnn，和iter。     1. 1 初始化模型 F = IterativeModel(args. horizon, args. num, ms = msv, images=args. images)    对照着模型的定义来看： def __init__(self, horizon,num=5, ms=False, images=False):    ms是指structure是否是masterswitch模式，当structure是masterswitch的时候，ms为True，horizon = 2args. horizon -1，不然，ms为False，horizon是args. horizon。原因上一篇有说过，找master switch的时候需要把所有switch遍历一遍，再将剩下的N-1个switch遍历一遍。     1. 2 训练模型 train_supervised(F, a, a2, args. num, steps=2000, bs=512, images=args. images)    对照着函数定义来看： def train_supervised(F, buf, gtbuf, num, steps = 1, bs=32, images=False):    F是定义的模型，buf是预存的训练样本，gtbuf是预存的训练样本对应的因果关系，steps是训练多少轮，bs是batch size的意思，是每轮有多少个样本参与训练。     前35000个样本做训练，后5000做测试。分别随机排序，提取前bs个样本做训练和测试。 perm = th. randperm(buf. size(0)-5000)testperm = th. randperm(5000) + 35000idx = perm[:bs]samples = buf[idx]gts= gtbuf[idx]testidx = testperm[:bs]testsamples = buf[testidx]testgts= gtbuf[testidx]    从样本中分离出状态(states)和动作(actions)： states = samples[:, :, :split]. contiguous(). view(bs, -1). cuda()actions = samples[:, :, split:]. contiguous(). view(bs, -1). cuda()    获得每组样本对应的因果关系(y)，另外用样本预测出因果关系(y’)： groundtruth = gts. cuda()pred = F(states, actions)    计算损失，调用反向传播函数调整网络： loss = ((pred - groundtruth)**2). sum(1). mean()testloss = ((testpred - testgroundtruth)**2). sum(1). mean()loss. backward()    接下来看实验中三个模型：     1. 3 ImageEncoder     先说三个模型都会用到的ImageEncoder，对应的是图中的Observation Encoder，将32×32×3的图形输入这个ImageEncoder，输出是维度为N的向量，这里N是灯的数量，如果ImageEncoder被训练好了，这个向量理想状况应该要能表示灯的状态。所以ImageEncoder的作用是将图像转换为灯的状态向量。网络结构如下，具体可以参看源代码，三层卷积池化激励，最后一层全连接层。 e1 = self. encoder_conv(x)     # 3*32*32 -&gt; 8*16*16e2 = self. encoder_conv2(e1)   # 8*16*16 -&gt; 16*8*8e3 = self. encoder_conv3(e2)   # 16*8*8 -&gt; 32*4*4e3 = e3. view(e3. size(0), -1)  # 16*8*8 -&gt; 32*4*4   encoding = self. fc(e3)        # 32*4*4 -&gt; N    1. 4 ICIN (No Attn), TCIN 相同部分     三个模型的forward一开始都会经历这一段代码： if self. images:  s_im = s. view(-1, 32, 32, 3). permute(0,3,1,2)  senc = self. ie(s_im)  sp = senc. view(-1, self. horizon, self. num)else:  sp = s. view(-1, self. horizon, self. num)sp[:,1:] = sp[:,1:] - sp[:,:-1]   # TCIN 没有这行a = a. view(-1, self. horizon, self. num+1)e = th. cat([sp, a], 2)    s是states，是灯的状态图，将样本的维度格式转换成适合ImageEncoder处理的格式，转变成灯的状态向量，动作后的状态向量减去动作前的状态向量，得到状态差（对应的是上面结构图中的state residual），将状态差向量与动作向量拼接。之前生成样本的时候有说过，每组样本是agent采取了一系列动作之后，得到的一系列状态，那么后一个状态减去前一个状态，则是动作让环境产生的变化。要留意的是，TCIN中没有求状态差。     1. 3 ICIN     初始化，下面不是所有的初始化代码，有些看了就能明白，不需要提到的，且三个模型都一样的初始化代码就没放上来。 if self. ms:  self. attnsize = num + 1  self. outsize = numelse:  self. attnsize = num     self. outsize = numself. fc1 = nn. Linear(2*num+1, 1024)self. fc3 = nn. Linear(512, self. attnsize + num)self. fc4 = nn. Linear(self. attnsize*self. outsize, self. attnsize + num)self. softmax = nn. Softmax(dim=1)        这里我们有可能会觉得古怪的是fc1的输入维度是2×N+1，照理来说，输入是动作向量和状态差向量，两个向量大小应该一致，都是N才对，拼起来是2×N才对，但却多了一维出来，原因是动作向量多了一维，动作向量是N+1维。这个设计到现在没有看出来有什么用，多出来的那维好像也没有被用到过。 p = th. zeros((sp. size(0), self. attnsize, self. outsize)). cuda()for i in range(self. horizon):  inn = e[:,i,:]    e1 = self. relu(self. dp(self. fc1(inn))) # 2N+1 -&gt; 1024   e2 = self. relu(self. dp(self. fc2(e1))) # 1024 -&gt; 512    e3 = self. fc3(e2)                      # 512  -&gt; attnsize+N  atn = self. softmax(e3[:, :self. attnsize]). unsqueeze(-1)  e3 = self. sigmoid(e3[:, self. attnsize:]. unsqueeze(1). repeat(1, self. attnsize, 1))  r = atn * e3    p = p + r    这一段代码对应图中Transition Encoder和Edge Decoder两个部分，agent每做一个动作，就会对状态差进行一次编码解码。Transition Encoder 的部分是三层全连接层，对应的是代码中inn-&gt;e3的部分，最后输出的大小是attnsize+N的向量。而Edge Decoder的原理图如下： 将Transition Encoder输出的向量分解为上图中Attention的部分和Edge Update的部分，两部分相乘，预测出最后的因果关系。Attention是N维，或者N+1维（当structure是masterswitch的时候），Edge Update是N维向量，所以最后p（Predicted Graph Update）的维度或大小是N×N或者(N+1)×N。     这部分的公式表达为： &lt;div align=center&gt;&lt;/div&gt; &lt;div align=center&gt;&lt;/div&gt;     到这里，我们要回过头去看一个问题，上一小节中sp的维度是horizon×N，但这其实不是很合理，sp存储随动作序列产生的状态向量，加上最原始的状态，进行了horizon个动作的序列，应该会有horizon+1个状态，但是这个程序在最开始生成数据的时候，没有存储最后一个动作执行后的状态向量，因此sp是horizon×N维，而存储状态差的sp[:,1:]只有(horizon-1)×N维，那和最后一个动作拼接的其实是最后一个动作执行前的状态向量，而不是这个动作造成的状态差向量，这样最后一个输入和其他输入在意义上完全不一样，这样的设计感觉是逻辑上的疏失，也可能会对训练出来的模型精确度造成不小的影响。     结构图上也能体现这个疏失，下面产生的因果关系是从1到H，但照图上的逻辑，状态差只能从1到H-1。     最后，再多一个编码解码的过程，得到最后的因果关系。不是很理解，为什么最后加这么一段网络。根据论文上的描述，应该是如果最后还有灯没有和哪个开关产生因果关系，由这一段来补齐，这样有点像在补前两段提到的训练代码设计的不合理的地方。 e3 = self. fc4(p. view(-1, self. attnsize*self. num)) # attnsize*N -&gt; attnsize+Natn = self. softmax(e3[:, :self. attnsize]). unsqueeze(-1)e3 = self. sigmoid(e3[:, self. attnsize:]. unsqueeze(1). repeat(1, self. attnsize, 1))r = atn * e3p = p + rp = p. view(-1, self. attnsize*self. num)    至于Attention机制的设计，可以这样想，因果关系里Edge Update决定哪些列是有效的，Attention决定哪些行是有效的。在因果关系矩阵里，列坐标表示的是开关编号，行坐标表示的是功能，即控制哪个灯，是否是master switch。理论上，如果网络训练的足够好，Attention会指示这一步哪些功能被开关触发，Edge Update会指示触发了哪些开关。     1. 4 ICIN (No Attn)     初始化部分，没有Attention的机制，输出的outsize即因果关系向量的大小： if self. ms:  self. outsize = num**2 + numelse:  self. outsize = num**2       self. fc1 = nn. Linear(2*num+1, 1024)self. fc3 = nn. Linear(512, self. outsize)self. fc4 = nn. Linear(self. outsize, self. outsize)    forward部分，： e = e. permute(0,2,1)c2 = ep = th. zeros((sp. size(0), self. outsize)). cuda()for i in range(self. horizon):  e1 = self. relu(self. dp(self. fc1(c2[:,:,i]))) # 2N+1 -&gt; 1024   e2 = self. relu(self. dp(self. fc2(e1)))  # 1024 -&gt; 512  e3 = self. sigmoid(self. fc3(e2))  # 512 -&gt; attnsize+N  p = p + e3p = self. sigmoid(self. fc4(p))    因为没有了Attention机制，自然也没有了Encoder，Decoder的区分，在时序中，三层全连接网络将灯的状态差和动作向量转换成因果关系，最后再通过一层全连接网络得到最后的因果关系。     1. 5 TCIN ​	初始化部分，和前两个网络不同的地方是，除了有全连接层，还有三个卷积层： if self. ms:  self. outsize = num**2 + numelse:  self. outsize = num**2 self. cnn1 = nn. Conv1d(2*num+1, 256, kernel_size=3, padding=1)self. cnn2 = nn. Conv1d(256, 128, kernel_size=3, padding=1)self. cnn3 = nn. Conv1d(128, 128, kernel_size=3, padding=1)self. fc1 = nn. Linear(self. horizon*128, 1024)self. fc3 = nn. Linear(512, self. outsize)    forward部分： e = e. permute(0,2,1)c1 = self. relu(self. cnn1(e)) # (2N+1)*horizon -&gt; 256*horizon c2 = self. relu(self. cnn2(c1)) # 256*horizon -&gt; 128*horizonc2 = self. relu(self. cnn3(c2)) # 128*horizon -&gt; 128*horizonc2 = c2. view(-1, self. horizon*128) e1 = self. relu(self. dp(self. fc1(c2))) # 128*horizon -&gt; 1024e2 = self. relu(self. dp(self. fc2(e1))) # 1024 -&gt; 512rec = self. sigmoid(self. fc3(e2))   # 512 -&gt; (N+1)Nreturn rec    先将状态向量组合在一起，然后经过三层一维卷积层，再经过三层全连接层，输出最后因果向量。     卷积网络设计的过分简单了。     2. 因果模型评估(Causal Model Evaluation)     这部分内容不多，就和训练写在一块了。 python3 evalF. py --horizon 7 --num 7 --fixed-goal 0 --structure one_to_one --method trajFi --images 1 --seen 10 --data-dir output/    method对应的是模型的种类，指定为trajF，为TCIN，指定为trajFia，为ICIN，其他为ICIN (No Attn)，这里选择ICIN评估。     初始化环境，tj = “gt”时，l. gt是一维化后的因果关系向量，filename用于记录l. step运行过程中产生的log，在这篇中，没有被调用： gc = 1 - args. fixed_goaltj =  gt l = LightEnv(args. horizon, args. num, tj, args. structure, gc, filename= exp/ +str(gc)+ _ +args. method, seen = args. seen)env = DummyVecEnv(1 * [lambda: l])    加载模型： F = th. load(args. data_dir+ iter_attn_Redo_L2_S +str(args. seen)+ _h +str(args. horizon)+ _ +str(args. structure)+addonn). cuda()F = F. eval() # 模型中有用到dropout或者BN,需要设定eval模式进行测试，ta    模型中有用到dropout或者BN，需要设定eval模式进行测试，train模式进行训练。     测试过程，先重置环境，用训练过程中用到的因果关系生成一组数据，用模型生成预测的因果关系，和真正的因果关系对比，求F1 Score，再将环境设置为test模式，重置环境，用训练没有用过的因果关系生成一组数据，用模型预测因果关系，和真正的因果关系对比，求F1 Score。如此循环一百次，分别计算训练中用过的因果关系产生的F1 Scores和没用过的因果关系产生的F1 Scores的均值。     代码如下： for mep in range(100):  l. keep_struct = False    obs = env. reset()         #重置环境，主要作用是切换因果关系  l. keep_struct = True  buf = induction(args. structure,args. num, args. horizon, l, images=args. images)  pred = predict(buf, F,args. structure, args. num)    f = f1score(pred, l. gt)      trloss. append(f)  #### TEST ON UNSEEN CS  l. keep_struct = False    l. train = False      #将train设置为False,则因果关系会从训练样本中没有出现果的集合中选择    obs= env. reset()    buf = induction(args. structure,args. num, args. horizon, l, images=args. images)    pred = predict(buf, F,args. structure, args. num)    f = f1score(pred, l. gt)    tsloss. append(f)    l. keep_struct = True    l. train = Trueprint(np. mean(trloss), np. mean(tsloss))    induction的部分和生成数据中产生样本的部分完全一致，就不贴出来说了，输出是生成测试要用的样本图序列，以及对应的动作。     predict函数，predgt是模型根据分解出的状态图组和动作组得到的因果关系： s = th. FloatTensor(buf[:,:-(num+1)]). float(). cuda()a = th. FloatTensor(buf[:,-(1+num):]). float(). cuda()predgt = F(s, a)    最后F1 Score函数，gt是这个环境现在真实使用的因果关系： p = 1 * (pred &gt; 0. 5) # 结果二值化，0 or 1if np. sum(p) == 0:  prec = 0else:  prec = np. sum(gt * p) / np. sum(p)rec =np. sum(gt*p) / np. sum(gt)if (prec == 0) and (rec==0):   return 0return 2 * (prec * rec) / (prec+rec)    论文里尝试调整了switch的数量，在四种structure模式下测试了三种模型的效果，ICIN的方法要优于其他两种方法很多，且受switch数量影响较小。      这篇论文的代码解析还有一篇，是策略模型的定义，训练和评估。敬请关注和期待。（PS: 看代码看得有点想吐了，缓缓，先更一集因果课再回头把最后一部分代码更完）     记：     这个训练模型的程序也是有比较多问题的：     1. 训练过程中的testloss应该用模型没见过的因果关系更合适，不然出来的数据，几乎和loss一样，并没有很大参考价值。          2. action，动作向量设置成了N+1维，似乎没有必要。          3. 每组样本序列，生成的最后一张图没有保存下来，保存的是最开始的，和最后一个动作执行前的图，程序里和最后一个动作拼接的其实是最后一个动作执行前的状态向量，而不是这个动作造成的状态差向量，这样最后一个输入和其他输入意义上非常不一样，这样的设计感觉是逻辑上的疏失。             4. 因果归纳网络的结构图有容易让人误解的地方。             5. evalF中定义的st没有用到     上篇：换个思路实现人工智能: 在视觉环境中用因果归纳完成目标导向的任务——生成数据     上上篇：换个思路实现人工智能: 在视觉环境中用因果归纳完成目标导向的任务(上) 参考论文：  Suraj Nair, Yuke Zhu, Silvio Savarese, Li Fei-Fei, Stanford University, CAUSAL INDUCTION FROM VISUAL OBSERVATIONS FOR GOAL DIRECTED TASKS, 2019参考代码：  https://github. com/StanfordVL/causal_induction"
    }, {
    "id": 54,
    "url": "http://localhost:4000/%E6%8D%A2%E4%B8%AA%E6%80%9D%E8%B7%AF%E5%AE%9E%E7%8E%B0%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD_-%E5%9C%A8%E8%A7%86%E8%A7%89%E7%8E%AF%E5%A2%83%E4%B8%AD%E7%94%A8%E5%9B%A0%E6%9E%9C%E5%BD%92%E7%BA%B3%E5%AE%8C%E6%88%90%E7%9B%AE%E6%A0%87%E5%AF%BC%E5%90%91%E7%9A%84%E4%BB%BB%E5%8A%A1-%E7%94%9F%E6%88%90%E6%95%B0%E6%8D%AE/",
    "title": "换个思路实现人工智能: 在视觉环境中用因果归纳完成目标导向的任务——生成数据",
    "body": "2020/06/28 -  上篇：换个思路实现人工智能: 在视觉环境中用因果归纳完成目标导向的任务(上)   ​ ​	今天我们来说一说《CAUSAL INDUCTION FROM VISUAL OBSERVATIONS FOR GOAL DIRECTED TASKS》实验中生成数据的方式。     官方源代码地址： https://github. com/StanfordVL/causal_induction    测试这个代码依赖一个软件：mujoco, mujoco-py, 安装可以参考这篇： https://maple52046. github. io/2020/04/25/Mujoco-Installation/#    1. 生成数据(Generate Data) python3 collectdata. py --horizon 7 --num 7 --fixed-goal 0 --structure one_to_one --seen 10 --images 1 --data-dir output/     ​	先说一下各个参数的意思：        horizon是上图中H的值，num是开关或灯的数量，–fixed-goal是指学习是否是goal conditioned，即agent在采取行动的时候是只需要考虑现在的状态(state)还是也需要考虑目标(goal)，在这个研究里，如果–fixed-goal=0，那表示有多个目标，是goal-conditioned。structure是开关和灯的结构关系，也是这个案例中的因果关系，有如下四类： 每一类都可以组合出非常多不同的因果关系，比如说One-to-One这种情况，有七组开关和灯，那么开关和灯之间可以有5040种不同的因果关系。在训练的过程中，给的训练数据会包含一些因果关系，最后需要测试训练好的agent在它从未见过的因果关系上的效果。seen这个参数是会参与训练的因果关系的数量，在这篇论文中，seen这个值会被设为10，50，100和500来做实验。images这个参数如果设定为1，表示会存储场景图片，而如果设为0，则不会存储观察到的场景图，而是灯的状态向量。data_dir，数据的存储路径。     1. 1 初始化环境 l = LightEnv(args. horizon, args. num, st, args. structure, gc, filename=str(gc)+ _traj , seen = args. seen)    对照着环境的初始定义来看： '''Light Switch Environment for Visual Causal Induction'''def __init__(self, horizon=5, num=5, cond= gt , structure= one_to_one , gc=True, filename=None, seen=10):    horizon，num，structure，seen，延续上面的意义和取值，gc是goal conditioned的意思，gc=1-fixed_goal，filename是在程序过程中记录log的地方，st/cond这个参数在生成数据这块似乎是没有用到的。     初始化因果关系的二值矩阵： self. aj = np. zeros((self. num, self. num))    初始化目标： if self. gc:  self. goal = self. _sample_goal()def _sample_goal(self):      Samples a new goal and returns it.            state = np. random. randint(0, 2, size=(self. num))    print(state)    light = np. dot(state. T, self. aj)    print(light)    light = light % 2    self. sim. model. light_active[:] = light    print(dir(self. sim))    self. goalim = self. sim. render(mode='offscreen', width=32, height=32, camera_name= birdview ) / 255. 0     return light    随机生成一组开关的状态，与因果关系相乘，得到灯的状态，再将灯的状态传给环境，渲染出目标场景图，返回灯的状态，这即是要达到的目标状态。     初始化开关的状态： self. state = np. zeros((self. num))    排列组合出所有可能的因果关系： if (self. structure ==  one_to_many ) or (self. structure ==  many_to_one ):  if self. num == 9:    self. all_perms = self. generate_cs_set1(self. num, True)  else:    self. all_perms = self. generate_cs_set1(self. num)else:  self. all_perms = self. generate_cs_set2(self. num)np. random. shuffle(self. all_perms)    如果是one-to-one的结构，那么all_perms会包含所有可能的5040（7!）种因果关系，每种因果关系用一个N维向量表示，如[3, 4, 7, 0, 1, 2, 5, 6]，表示开关0控制着灯3，…。如果是one-to-many或者many-to-one的结构，all_perms包含所有可能的823543（7^7）种因果关系。如果开关和灯的数量都是9，那么one-to-many或者many-to-one的因果关系会有9^9个，上亿了，因此需要随机过滤掉一些，除了最底层，每一层会随机过滤掉40%，最后大约会取到因果总数的0. 6^8。值得注意的是，如果是many_to_one, 那么需要把向量的位置理解成开关，如果是one_to_many，则要把向量的位置理解成灯的编号，而里面的数字是开关的编号。得到all_perms后要将里面的因果关系随机排序。     获得当前的场景向量： obs = self. _get_obs()    obs向量中包含了灯的状态，目标向量，还有轨迹向量，目标向量和轨迹向量在生成数据的过程中没有被使用到。     定义动作域和观察域，这两个在生成数据的过程中也没有被用到。 self. action_space = spaces. Discrete(self. num+1)self. observation_space = spaces. Box(0, 1, shape=obs. shape, dtype='float32')    1. 2 获得训练样本     在这个实验中，会生成num_episodes组样本，样本存在buffer中，样本对应的因果关系，存在gtbuffer中。 buffer = []gtbuffer = []num_episodes = 40000    每组样本又如何生成？重设环境，重新选择因果关系，设定新的目标，获得新的初始状态的观察向量。 l. keep_struct = Falseobs = env. reset()l. keep_struct = Trueif train:  ind = np. random. randint(0, self. seen)else:  ind = np. random. randint(self. seen, self. pmsz)perm = self. all_perms[ind]## Set graph according to causal structureif self. structure ==  one_to_one :  aj = np. zeros((self. num,self. num))  for i in range(self. num):    aj[i, perm[i]] = 1  self. aj = aj  self. gt = self. aj. flatten()elif self. structure ==  one_to_many :  aj = np. zeros((self. num,self. num))  for i in range(self. num):    aj[i, perm[i]] = 1  self. aj = aj. T  self. gt = self. aj. flatten()elif self. structure ==  many_to_one :  aj = np. zeros((self. num,self. num))  for i in range(self. num):    aj[i, perm[i]] = 1  self. aj = aj  self. gt = self. aj. flatten()elif self. structure ==  masterswitch :  aj = np. zeros((self. num,self. num))  for i in range(self. num):    aj[i, perm[i]] = 1  self. aj = aj  self. ms = np. random. randint(self. num)  m = np. zeros((self. num))  m[self. ms] = 1  self. gt = self. aj. flatten()  self. gt = np. concatenate([self. gt, m])    如果是要获取训练样本，则在seen的范围内选择因果关系，否则在seen的范围外选择因果关系。选择到因果关系后，将一维向量转换成二维二值向量。若是masterswitch结构，还需要随机产生一个master switch。     生成样本的时候，当structure是masterswitch的时候，agent会先把所有的开关都试一遍，找出master  switch，再将master之外的开关试一遍： if args. structure ==  masterswitch :  it = None  for i in range(args. num):    p = l. _get_obs()    if args. images:      pi = l. _get_obs(images=True)    p = p. reshape((1, -1))    a = np. zeros((1, args. num+1))    a[:,i] = 1    if args. images:      mem = np. concatenate([np. expand_dims(pi. flatten(), 0), a], 1)    else:      mem = np. concatenate([p[:,:args. num], a], 1)    if i == 0:      epbuf = mem    else:       epbuf = np. concatenate([epbuf, mem], 0)    l. step(i, count = False)    p2 = l. _get_obs()    if (p != p2). any():      it = i      break  for i in range(args. num):    if i != it:      p = l. _get_obs()      if args. images:        pi = l. _get_obs(images=True)      p = p. reshape((1, -1))      a = np. zeros((1, args. num+1))      a[:,i] = 1      if args. images:        mem = np. concatenate([np. expand_dims(pi. flatten(), 0), a], 1)      else:        mem = np. concatenate([p[:,:args. num], a], 1)      epbuf = np. concatenate([epbuf, mem], 0)      l. step(i, count = False)  ln = epbuf. shape[0]  buf = np. zeros((2 * args. horizon - 1, epbuf. shape[1]))  buf[:ln] = epbuf    保存当前的场景图后，执行动作，然后切换到下个场景，如果在找master switch，判断现在这个switch是不是master，如果是的话，结束找master的过程，进入下一阶段。 def step(self, action):  '''Step in env.   Args:      action: which switch to toggle    '''  ## If  Do Nothing  Action  if action == self. num:    pass  else:    if self. structure ==  masterswitch :      ## Only once masterswitch is activated can others be activated      if (action == self. ms) or (self. state[self. ms] == 1):        change = np. zeros(self. num)        change[action] = 1        self. state = np. abs(self. state - change)    else:      change = np. zeros(self. num)      change[action] = 1            self. state = np. abs(self. state - change)    obs = self. _get_obs()  done = False  info = {'is_success': self. _is_success(obs)}  self. correct. append((info[ is_success ]))    reward = self. compute_reward(obs, info)  return obs, reward, done, info    step会判断是否成功触发了开关，触发开关后是否成功达到了目标，以及计算得失，不过是否成功达到目标和计算得失都不会在这个阶段被用到，不论达没达到目标，所有的开关都会被试一遍（masterswitch结构可能不止一遍）。 记：     这个生成训练样本的程序是有比较多问题的：     1. 费内存，40000组样本都存在buffer里，最后才写入文件，内存小的电脑不要轻易尝试。       2. 训练样本会用到的因果关系很少，实验中只会尝试10，50，100，500，而buffer中存入的数据，其实只和场景，因果关系，因果结构(structure)，开关数量，可见的seen有关，步骤又都是顺序的，所以如果seen是10，则buffer中的40000组训练数据中只有10组不同的数据。在生成数据的程序中，每组数据的生成都重新对环境渲染，整个过程费时间，费内存，费计算，存下来废硬盘，重新读入还是费时费内存，可以优化，换一种写法和存训练数据的方法，会更合理。       3. 在初始化环境的时候，似乎不需要定义st/cond的值。 上篇：换个思路实现人工智能: 在视觉环境中用因果归纳完成目标导向的任务(上) 参考论文：  Suraj Nair, Yuke Zhu, Silvio Savarese, Li Fei-Fei, Stanford University, CAUSAL INDUCTION FROM VISUAL OBSERVATIONS FOR GOAL DIRECTED TASKS, 2019参考代码：  https://github. com/StanfordVL/causal_induction"
    }, {
    "id": 55,
    "url": "http://localhost:4000/MIT%E5%9B%A0%E6%9E%9C%E8%BF%B7%E4%BD%A0%E8%AF%BE%E7%AC%94%E8%AE%B0-%E5%9B%A0%E6%9E%9C%E8%AF%AD%E8%A8%80%E5%92%8C%E5%9B%A0%E6%9E%9C%E6%8E%A8%E7%90%86/",
    "title": "MIT因果迷你课笔记 —— 因果语言和因果推理",
    "body": "2020/06/21 - 一、分布和因果图     例1. 海拔和温度     假设，海拔和温度的分布及因果关系是已知的，且如下图所示：     海拔和温度存在因果关系，平均每上升100米，温度下降0. 6摄氏度。上图中左图是海拔和温度的分布情况，右边是因果图。Nx，Ny为独立同分布，假设它们都服从期望值为0，标准差为1的标准正太分布。X，Y服从二维正太分布如上图所示。     如何对这个系统进行干预呢？直接确定X的值即可。                 当确定了X的值后，Y的分布也会产生变化，Y服从均为为-16，标准差为1的正太分布。     也可以对Y进行干预，使Y服从均值为2，标准差为2的正太分布：     那么X和Y之间的因果关系就被破坏了，因为要改变Y的分布，比如你装了一台超级大的空调，使得Y服从了这样的分布，那么海拔的变化便不再有影响力。海拔和温度之间的因果关系就不存在了。海拔和温度就变成了两个独立的变量，X，Y的联合分布也变成了上图所示。     可观察到的所有变量X1，…，Xd的分布式集，我们称之为P，即下图左边的所有公式。     当我们要对系统进行干预时，即改变某些变量的值或者和其他变量的关系，如把X1的值固定在0上，分布式集则变为Pdo(X1:=0)，变量之间的因果关系也会产生相应的变化。 ​	Pdo的下标do表示的是干预的这个动作。下图中将X4这个变量的值通过干预使之固定为13。为什么要加一个do的下标来表示干预呢，因为直接写成P(X4:=13)，很容易被误理解为P(. |X4=13)，即X4=13时系统的状况。Pdo(X4:=13)与P(. |X4=13)完全是两码事。    例2. 肾结石     关于这个例子，上一节MIT因果迷你课笔记 —— 相关和因果中有做一些讨论。     假设我们现在只知道肾结石治疗的因果图，不知道每个变量的分布情况。肾结石治愈率受治疗方案和结石大小影响，而结石大小又对治疗方案的选择有直接的影响。     我们想知道的是，治疗方案A和治疗方案B的治愈率分别是多少，那么它们在这个场景中则不能简单地用P( R =1 | T =A )和P( R =1 | T =B)去表示治疗方案A和B的治愈率。而应该用Pdo(T:=A)(R=1)和Pdo(T:=B)(R=1)来表示，因为我们想知道的是，如果我们不管结石大或小，选择治疗方案A或B能获得的治愈率，如下图：     现有一些样本统计结果：     Pdo(T:=A)(R=1)的值的计算如下：     说几个比较重要的点，Pdo(T:=A)(S = s, T = A) = Pdo(T:=A)(S = s)，是因为有了do(T:=A)这个干预后，S和T相互独立。Pdo(T:=A)(S = s) = P(S = s)，因在因果关系中S不受治疗方案和治愈率的影响。Pdo(T:=A)(R=1|S = s, T = A) = P(R=1|S = s, T = A)的道理也是一样，这样的干预并没有改变R和S还有T之间的因果关系。最后得到的Pdo(T:=A)(R=1)不等于P(R=1|T=A)，因此，当我们想进行主动干预的时候，比如主动选择治疗方案的时候，因果推理就十分有必要了。     将上面的现象一般化，即可得到下面有效调整集(valid adjustment set)的定义，对于上一例，结石大小就属于(T，R)的有效调整集。     为什么需要定义一个有效调整集？在探索X，Y之间的因果关系时，需要计算Pdo(X:=x)(y)，而去场景中干预X做实验，有时候是不现实的。比如，假设一个人去了医院发现了肾结石，但医院检查结石大小的机器恰好坏了，他得在A，B方案中选择一个，因此他得知道A，B方案的治愈率，那个时候再去干预X做实验来获得X和Y之间的因果关系显然是不现实的，他拥有的只有过往的一些统计数据，还有统计变量之间的因果关系。那么是哪些变量使得这些统计数据不能直接用来表示X和Y的因果关系？如何去掉这些变量造成的影响，通过对已有数据的计算来得到X对Y的影响？     符合上述条件的即为有效调整集，那么怎么样快速有效的找到有效调整集，下面给出了一个方法，但注意这个方法不能找出所有的有效调整集。     结合下面这张因果结构图理解一下有效调整集的概念。     X&lt;-A-&gt;K-&gt;Y被定义为“backdoor path”，上面因果图中，人们其实并不关心其他的因果关系，他们只关心X和Y的因果关系，就像肾结石那例，我们并不关心医生依照结石大小会对方案做出的选择，我们只想知道每种治疗方案的治愈率。因此我们想知道的是X-&gt;D-&gt;Y这条关系，X对Y有什么样的影响。对于A对X造成的影响，我们不关心，因此，X&lt;-A-&gt;K-&gt;Y这种同时影响着X和Y的因果关系链被称之为“backdoor path”。     上图因果关系中的有效调整集都有哪些呢？这里举三个有效调整集，{A，C}，{K}，{F，C，K}，这三个并不是全部的有效调整集。{A，C}这组有效调整集即X的父结点的集合，为什么取X的不包含Y的父节点的集合能得到有效调整集呢？因为有效调整集必须能够切断所有的“backdoor path”。在这一例中，能够切断“backdoor path”的结点是A和K，而其他结点都是可有可无的，比如C或F在不在有效调整集中，都不会影响调整集的有效性。     那么为什么要切断所有“backdoor path”呢？当我们对X进行干预的时候，A对X的影响就被切断了，A只对Y产生影响。do(X:=x)这个动作改变了因果关系，舍掉了A对X能产生的影响，所以计算中也应该排除A对X产生的影响。值得注意的是，虽然A才是关键的影响着X的结点，但是统计计算的时候受A影响又影响着Y的K也可以作为有效调整集的必要点，因为它的状况可以反馈A的状况，如果A不易被观察到，观察K也可以得到正确结果。     另外，D，G，H都不应该出现在有效调整集当中。D，G会直接或者间接地切断X和Y的关联，H则会暴露Y。     例3. 烟草和肺癌     再回到烟草和肺癌这个经典案例：     关于这个例子，上一节MIT因果迷你课笔记 —— 相关和因果中有做一些讨论，在1950年的时候有这样一篇文章发表，详细分析了吸烟和肝癌之间的相关性，现在我们知道，相关不一定构成因果，可能是由潜在的原因同时导致了吸烟和肝癌。     而这篇文章的优秀之处在于，结合了各种可能的潜在原因，比如将各种压力，性别，地域等因素列为观察的变量。即增加不同的调整集，发现吸烟和肝癌总是强相关，即，找不到有效调整集可以改变吸烟和肝癌之间的相关性。既然吸烟有害身体健康，那么政府就想让烟草公司纳税，烟草公司为了避税，把这个潜在原因归到了基因上，即某种基因导致了一个人既喜欢吸烟，又容易患肝癌，我们今天来看当然是非常荒诞的，但是当时科技还不怎么发达，人们对基因的认知也很有限，就这样被烟草公司忽悠了。这里Jonas Peters推荐了一本书《Merchants of Doubt》，不能尽信科学家，他们也会被利益驱动去做一些违背道德的事情，商人和政客就更不用说了。     例4. 坏血病和柠檬     在1747年的五月，James Lind在一艘行驶在海上的船上，对患有坏血病的12名船员做了一个随机化实验，将十二名船员随机分成六组，配给不同的食物，得到的结果是，每天获得柠檬的病人，病情明显好转。他可能意识到调整饮食可以治愈这种病，但是又不知道如何调整，才能治愈这种病。因此他设计了一个随机化实验，这种实验方式后来被广泛应用于医疗领域。     随机化实验的价值在于，因果关系是复杂的，但如果你把船员随机分组，然后再配给食物，那么食物配给就不受任何因素影响了，即切断了因果图中所有到food这个结点的父结点，那么观察到的food和recovery之间的因果关系就不需要有效调整集来调整了，得到的相关性就是它们之间的因果关系。     接下来是一些小概念，如果这两个模型有相同的概率{概率和干预}分布，我们称两个模型概率上{干预上}对等。在干预时，随机化干预能得到正确的因果关系。     下面用四种方式定义了认定X是Y的因需要符合的条件。(i)中X和Y中间的符号是dependence的意思，去掉斜杠是independence。     思考：如果X和Y之间存在因果关系，那么X和Y之间因果关系的强度又该怎么定义和计算？     也许是： &lt;div align=center&gt;&lt;/div&gt;     工具变量(Instrumental Variables)     我们依然是想计算X，Y之间的因果关系，以及它们之间的系数，但是由于H不好观察，X不好控制，我们没有办法通过控制X来得到它和Y之间的关系，这个时候如果有个变量I，它独立于X，H，Y这个系统，那么就可以通过控制它得到X和Y之间的关系。这种变量被称之为工具变量。     反事实(Counterfactuals)     《生活大爆炸》第四季里有一段关于反事实的游，感兴趣的搜索着看一下。     关于反事实，上一节有提过，这个概念对照的是人类反思和想象的能力，这个能力非常重要，是让我们有洞察力的关键，不过也有很多副作用。     对应到SCM，可以做如下定义：     通俗地说，反事实就是对SCM中分布公式中的随机部分做定义，从而改变因果关系。举个例子：     如上图，假设一种病，是否治疗服从NT随机分布，如果治疗，治愈率为99%，如果不治疗，治愈率为1%。如今，Tom也得了这种病，而他恰好就是一个特例，如果他治疗，治愈率是0，如果他不治疗，反而会好。但这只有发生后才明确知道，因此，在Tom接受治疗，死亡后，我们说如果Tom不治疗，那么他反而会痊愈，这就是反事实。          接下来总结一下这个部分：     1. 因果关系不是什么时候都有必要研究，很多时候我们只需要知道相关性，比如你想让机器人把猫认出来，相关性即可，你想要机器人把猫画出来，可能会需要因果关系。因果关系通常是，我们想对环境进行干预，并想知道干预后的结果，才有必要去了解的。          2. 当我们有了因果图和变量的分布公式后，我们就能推演出干预后变量分布的变化。反事实也是如此。     下面这个研究很有趣，想知道死神走多快吗？可能是0. 82m/s，要走的比他快哦，被赶上就完蛋了。 此篇上一节为：MIT因果迷你课笔记 —— 相关和因果 此系列未完待续，敬请关注^_^。 声明：所有图片均来自Jonas Peters的课件，没有原创图片和公式。 参考： [1] Jonas Peters, University of Copenhagen, Mini course on Causality, Laboratory for Information &amp; Decision Systems (LIDS) and Models, Inference &amp; Algorithms of the Broad Institute, MIT, 2017 [2] Judea Pearl and Dana Mackenzie, The Book of Why, 2018 [3] 我是啤酒，MIT因果迷你课笔记 —— 相关和因果，2020 "
    }, {
    "id": 56,
    "url": "http://localhost:4000/MIT%E5%9B%A0%E6%9E%9C%E8%BF%B7%E4%BD%A0%E8%AF%BE%E7%AC%94%E8%AE%B0-%E7%9B%B8%E5%85%B3%E5%92%8C%E5%9B%A0%E6%9E%9C/",
    "title": "MIT因果迷你课笔记 —— 相关和因果",
    "body": "2020/06/05 - 一、因果关系的意义     看下面这两张图：         &lt;div align=center&gt;&lt;/div&gt;     两张图分别是基因A, B活跃度变化的时候，phentype的变化，那么现在我们来思考一个问题，如果我们把基因A或B单独删掉，抑制其活跃性，那么phenotype的值会落在哪个区间?     看删掉基因A的结果:     如果删掉基因A，phenotype的值会降到红色线条所示的区间，因为A的活跃度影响着phenotype的值，两个元素之间成因果关系。     再看删掉基因B的结果：     如果删掉基因B，phenotype的值还在原来的区间，没有产生明显的变化，因为基因B和phenotype同时被另一个元素confounder影响，它们之间本身不构成因果关系，只是单纯的相关关系。     通过上面的示例，可以大概了解到，通常我们都在研究相关关系，而因果关系是不同的，检验是因果还是相关，可能需要主动干预，形成对照。     毫无疑问，因果关系的确立有助于我们判断事物的发展。 二、因果模型     因果模型是什么？                     &lt;div align=center&gt;&lt;/div&gt;     以结构化因果模型（Structral Causal Models, SCMs）为例，因果模型应该包含如上图所示的四个部分：     1. Distribution，SCMs要能解释数据的分布状况       2. Causal Graph，生成可以表现因果关系，因果结构的因果图       3. Intervention Distribution，模型能预测干预后系统的反馈         4. Counterfactuals，模型可以做反事实推断     这里比较难解释的是反事实推断，第一个部分是现在的机器学习和深度学习方法可以轻易实现的，即挖掘数据和数据，数据和概念等等之间的相关性，比如人类拥有的感知外界的功能，看见一只鸟，传入眼睛的是光折射出的图案，但人有能力将这个图案和鸟这个概念关联起来，现在计算机也可以。第三个部分是干预，即人类偶尔知道做什么样的行为，可能能得到自己想要的结果，比如他看见一只鸟，他知道如果他想吃到鸟肉，他得把这只鸟打下来，要把这只鸟打下来，他得有弓箭，猎枪或者其他杀伤性武器。第四部分则是反事实推断的能力，即能够做如果…，那么…的假设和推断，“如果那天我没有把那只鸟打下来，吃上肉，我现在还活着吗？”，“如果当时那只鸟飞得再高一点，我还能打到它吗？”，这对应的是人类想象，回顾和理解的能力。关于这部分概念，可以参考《The Book of Why》的第一章和第八章。     接下来看因果推理和因果学习的概念，上图是因果学习和推理对照概率模型的示意图，如概率模型一样，如果有了模型，那么就可以根据观察到的数据进行推理，如果有完整的观察到的数据和数据对应的结果，则可以进行模型训练，学习。但因果学习和概率模型的学习有很不一样的地方，假设你有无限多的数据，即你知道每一种可能和其对应的答案，那么训练概率模型就没有必要了，而因果模型却还是要经过学习，发现的过程才能得到。 三、因果关系相关的案例     1. 巧克力和诺贝尔奖     上面这幅图中，横坐标是人均每年吃的巧克力的量，纵坐标则是每千万人中获得诺贝尔奖的数量。根据上面的图，可以轻易的得出结论，巧克力的消耗量和诺贝尔奖的获奖数量是强相关的。因此就有了下面的一些报道： 吃巧克力，可以增加获诺贝尔奖的人数，以及下面这则新闻，聪明的人似乎更爱吃巧克力。 这是一个典型把相关性当成因果关系的案例。          2. 抽烟和肝癌     上面这篇发表于1950年的文章，证明了吸烟和肝癌之间存在强相关性：     所以政府想说，哎呀，吸烟导致肝癌啊，那推出烟草税吧，烟草公司就不乐意了，站出来说，并不一定是吸烟导致了肝癌，或者提高了得肝癌的几率，也有可能是一个不知名的原因，共同导致了抽烟过多和肝癌呢？     怎么判断抽烟和肝癌之间存在因果关系，还是它们两个有一个共同的原因导致，是关键的问题。     3. 夜间光照与近视     左边这幅图出自一个关于近视的研究，这个研究表明，被家长更多地提供夜间照明的孩子们反而更多的患上了近视。也就是夜间照明和近视强相关，夜间照明极有可能是近视的一个诱因。     因此，有专利就把夜间照明和近视当成因果关系，做了发明。但它们其实不算是因果关系，只是有可能是因果关系。     4. 肾结石的两个治疗方案     肾结石有两个治疗方案，A和B，如果看总的治愈率会发现，B方案明显好于A方案。但是肾结石的石头又可分为大小两种情况，从这两种情况的治愈率来看，A方案又明显好于B方案，造成这种结果的原因是，治愈率不但和治疗方案有关，还和结石大小有关，A方案的治愈率虽然高，但主要用在了治愈率较低的大结石这种情况，而B方案则主要用在了治愈率较高的小结石这种情况，因此，简单地比较总的治愈率，对A方案来说是不公平的。治愈率，结石大小，和治疗方案的因果关系可以以下图表示：     5.   广告推荐     当我们使用搜索引擎搜索一些东西的时候，或者做一些其他和网络互动的时候，经常会被推送一些广告，如下图：     下面这个研究则通过研究推送广告过程中的因果关系来优化模型： 用户的意图会决定用户的操作和一些其他的数据，而通过分析用户的数据，我们来决定给他推送什么样的广告，以及推送多少广告，而这些又会一定程度上决定用户的行为。     6. 基因的相互作用     上面这个研究中，有6170个基因，图中的横坐标表示基因5954的活性，纵坐标表示基因4710的活性。有160笔观察数据，左边这张图就是观察数据画出来的，可以发现，两个基因的活性表现出了一定的相关性。这个实验还有1479笔干预数据，即对一些基因做删除动作，总共有1479个不同的删除动作，中间的图则是被干预后基因5954和4710的活性的变化，更可以发现这两种基因的活性是强相关的。     这个研究的目的其实是要发现这些基因之间的因果关系。但非常有意思的是，这些数据可以支持一个论点，即因果关系通常在系统中是稳定的。在试验中，基因5954是基因4710的因，右图中当对基因5954做了干预后，4710的活性也随着产生了明显的变化。由中间那幅图的实验，1479次的干预来看，不论删掉系统中哪个部分，基因5954和基因4710都是强相关的。可见因果关系在系统中是一种稳定的关系。     举个例子，有如下因果关系：x&lt;-w-&gt;z-&gt;y，z和y是因果关系，x和z之间不存在因果关系，则改变x的状态，对z和y的状态不会造成影响，改变w的值，那么z和y会跟着一起改变。 此系列未完待续，敬请关注^_^。 声明：所有图片均来自Jonas Peters的课件，没有原创图片和公式。 参考： [1] Jonas Peters, University of Copenhagen, Mini course on Causality, Laboratory for Information &amp; Decision Systems (LIDS) and Models, Inference &amp; Algorithms of the Broad Institute, MIT, 2017 [2] Judea Pearl and Dana Mackenzie, The Book of Why, 2018 "
    }, {
    "id": 57,
    "url": "http://localhost:4000/D-separation-d-connection/",
    "title": "D-separation，d-connection",
    "body": "2020/05/05 -     这篇介绍一个颇复杂的概念，d-separation，关于图和因果关系的一个小概念，逻辑有点绕。     关于这个概念的前因后果可以看这个链接：https://www. andrew. cmu. edu/user/scheines/tutor/d-sep. html。这里只描述概念本身。     d-separation和d-connection中的d是dependence的意思，指依赖性。如果两个变量或变量集X, Y可以被有向图中的一组变量Z d-separated，则X，Y在该图可以表达的所有概率分布以Z为独立条件。换句话说，就是一旦你了解了Z，那么你对X的了解就不会再给你任何多的有关Y的信息，那么X, Y则是基于Z独立的。     如果一条路径携带信息，或者存在依赖性，则认为这条路径是活的(active)。一个图中，变量X，Y可能经过多条路径相连，其中可能有路径是活的，也可能没有，如果有，那么X和Y则是d-connected。如果X和Y是d-separated, 那表示连接这两个变量的所有路径都不是活的。     怎么样的路径是活的呢？路径上的所有变量(点, 不包括首尾顶点)和路径(边)是活的。路径和路径上的变量是否是活的是基于一组变量的集合Z的。让我们先考虑Z为空的情况，两个变量A, B经过一个变量C有以下几种方式： 1) A –&gt; C –&gt; B2) A &lt;– C &lt;– B3) A &lt;– C –&gt; B 4) A –&gt; C &lt;– B     用因果逻辑来分析上面四个关系，1）A是B的间接因，2）B是A的间接因，3）C是A和B的共同因，4）C是A和B的共同果。前面三种关系都表示A，B存在关联(association)，或依赖性(dependence)，基于d-separation的理论，前面这三种有向路径都是活的。而第四种路径，A和B之间并无关联，所以第四种路径是死的(inactive)。所以当条件集Z是空时，只有能表达因果关系的路径才算活的。     之前说过，按d-separation理论，路径中所有的顶点和路径都是活的，该路径才是活的，上述示例中，C是A，B之间唯一的顶点，所以它在前三个路径中是活的，在第四个路径里是死的(无效的)。     这里引入一个新的概念，如果在图中有两个或以上的顶点是它的因，我们将一个顶点称之为碰撞体(collider)，反之则为非碰撞体(non-collider)。在上面四个示例中，前三个C都是非碰撞体，最后一个C为碰撞体。当条件集为空时，非碰撞体是活的顶点，传输信息(依赖关系)，碰撞体是死的，不传输信息(依赖关系)。     因此，当条件集为空，X和Y是否被条件集d-separated的可以转换成问题：X和Y之间是否存在不含碰撞体的路径。     现在考虑条件集不为空的情况。现在我们重新考虑上面四个路径，只是这次要解的问题是，A和B是否被C d-separated。C为条件集，我们要考虑的即，当C的值确定时，A和B之间是否会存在相关关系或者依赖关系。在头三条路径中，当条件集为空时，C是活的，那么现在C则是死的。即当C的值确定时，你再知道B的值，对解出A毫无帮助，反之亦然。Reichenbach称其为”Principle of the Common Cause”，Markov则形容它为，已知的当下分开了过去和未来，让未来不再依赖过去。     第四条路径中，C是碰撞体，因此在条件集为空时是无效的，现在则是有效的。我们可以这样来看这个问题，如果C的值被确定了，那么必然是A或者B或者它们两个同时造成C这样的结果，如果此时你知道A的值，发现C的结果不是A造成的，或不单单是A造成的，那么就可以推理出B多多少少造成了C现在这样的结果。所以获得A的信息的同时也获得了B的信息。这里有一个Pearl (1988)举的例子，如果一辆车无法发动，那么可能是引擎坏了或者油耗尽了，因此有下面的因果关系： ​												dead battery –&gt; car won’t start &lt;– no gas     如果你获得的信息是引擎是好的，那么这个信息对于推断有没有汽油是没有价值的。但是，如果告诉你车无法发动，但是引擎是好的，那么你是否可以推断，应该是没油了，这个时候，引擎是好的对于推断有没有汽油则是有价值，有信息量的。所以当确定了果，则与这个果关联的原本独立的因也关联了起来。     D-connection则可定义为：如果G是一个有向图，在图中X, Y和Z是三个没有交集的顶点的集合。当且仅当存在一条无向路径U，连接了集合X和Y中的某些结点，U中所有的碰撞体或者其后代在Z中，没有非碰撞体在Z中，则可称之为X和Y在G图中被Z d-connected。X和Y在G图中被Z d-separated当且仅当X和Y在G图中没有被Z d-connected。     接下来，看一个示例：      分析：有两条路径，2-&gt;1&lt;-3&lt;-4-&gt;5，4，3为非碰撞体，4在Z中，另一条路径2-&gt;1-&gt;5，1为非碰撞体，1在Z中，因此，结论成立。      分析：有两条路径，4-&gt;5&lt;-1，5是碰撞体，5不在Z中，而4-&gt;3-&gt;1中3是非碰撞体，3在Z中，因此结论成立。      分析：有两条路径，4-&gt;5&lt;-1&lt;-2，5是碰撞体，1不是碰撞体，5不在Z中，而4-&gt;3-&gt;1&lt;-2，1是碰撞体，1不在Z中，因此结论成立。      分析：有两条路径，4-&gt;5&lt;-1，5是碰撞体，5在Z中，因此结论成立。 声明：所有图片均来自Jonas Peters的课件，没有原创图片。 参考： [1] Jonas Peters, University of Copenhagen, Mini course on Causality, Laboratory for Information &amp; Decision Systems (LIDS) and Models, Inference &amp; Algorithms of the Broad Institute, MIT, 2017 [2] https://www. andrew. cmu. edu/user/scheines/tutor/d-sep. html 以下是[2]中的references, 做个备份 Blalock, H. (Ed. ) (1971). Causal Models in the Social Sciences. Aldine-Atherton, Chicago. Blalock, H. (1961). Causal Inferences in Nonexperimental Research. University of North Carolina Press, Chapel Hill, NC. Costner, H. (1971). Theory, deduction and rules of correspondence. Causal Models in the Social Sciences, Blalock, H. (ed. ). Aldine, Chicago. Geiger, D. and Pearl, J. (1989b). Axioms and Algorithms for Inferences Involving Conditional Independence. Report CSD 890031, R-119-I, Cognitive Systems Laboratory, University of California, Los Angeles. Geiger, D. , Verma, T. , and Pearl, J. (1990) Identifying independence in Bayesian Networks. Networks 20, 507-533. Glymour, C. , Scheines, R. , Spirtes, P. , and Kelly, K. (1987). Discovering Causal Structure. Academic Press, San Diego, CA. Kiiveri, H. and Speed, T. (1982). Structural analysis of multivariate data: A review. Sociological Methodology, Leinhardt, S. (ed. ). Jossey-Bass, San Francisco. Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems. Morgan and Kaufman, San Mateo. Pearl, J. (1995). Causal diagrams for empirical research. Biometrika, 82, pp. 669-710. Pearl, J. and Dechter, R. (1989). Learning structure from data: A survey.  Proceedings COLT ‘89, 30-244. Pearl, J. , Geiger, D. and Verma, T. (1990). The logic of influence diagrams. Influence Diagrams, Belief Nets and Decision Analysis. R. Oliver and J. Smith, editors. John Wiley &amp; Sons Ltd. Pearl, J. and Verma, T. (1991). A theory of inferred causation. Principles of Knowledge Representation and Reasoning: Proceedings of the Second International Conference, Morgan Kaufmann, San Mateo, CA. Pearl, J. and Verma, T. (1990). A Formal Theory of Inductive Causation. Technical Report R-155, Cognitive Systems Labratory, Computer Science Dept. UCLA. Pearl, J. and Verma, T. (1987). The Logic of Representing Dependencies by Directed Graphs. Report CSD 870004, R-79-II, University of California at Los Angeles Cognitive Systems Laboratory. Richardson, T. (1994). Properties of Cyclic Graphical Models. MS Thesis, Carnegie Mellon  University. Richardson (1995). A Polynomial-Time Algorithm for Deciding Markov Equivalence of Directed Cyclic Graphical Models, Technical Report PHIL-63, Philosophy Department, Carnegie Mellon University. Reichenbach, H. (1956). The Direction of Time. Univ. of California Press, Berkeley, CA. Salmon, W. (1980). Probabilistic causality. Pacific Philosophical Quarterly 61, 50-74. Spirtes, P. (1994a). “Conditional Independence in Directed Cyclic Graphical Models for Feedback. ” Technical Report CMU-PHIL-54, Department of Philosophy, Carnegie Mellon University, Pittsburgh, PA. Spirtes, P. , Glymour, C. , &amp; Scheines, R. (1993). Causation, prediction, and search. Springer-Verlag Lecture Notes in Statistics 81,. Springer-Verlag, NY. Spirtes, P. , Richardson, T. , Meek, C. , Scheines, R. , and Glymour, C. (1996). Using d-separation to calculate zero partial correlations in linear models with correlated errors.  Technical Report CMU-PHIL-72, Dept. of Philosophy, Carnegie Mellon University, Pittsburgh, PA, 15213. Suppes, P. (1970). A Probabilistic Theory of Causality. North-Holland, Amsterdam. "
    }, {
    "id": 58,
    "url": "http://localhost:4000/%E6%8D%A2%E4%B8%AA%E6%80%9D%E8%B7%AF%E5%AE%9E%E7%8E%B0%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD_-%E5%9C%A8%E8%A7%86%E8%A7%89%E7%8E%AF%E5%A2%83%E4%B8%AD%E7%94%A8%E5%9B%A0%E6%9E%9C%E5%BD%92%E7%BA%B3%E5%AE%8C%E6%88%90%E7%9B%AE%E6%A0%87%E5%AF%BC%E5%90%91%E7%9A%84%E4%BB%BB%E5%8A%A1(%E4%B8%8A)/",
    "title": "换个思路实现人工智能: 在视觉环境中用因果归纳完成目标导向的任务(上)",
    "body": "2020/03/29 - 能让机器拥有智慧的，可能既不是机器学习，也不是深度学习。              今天我们来说一说这篇文章：     什么是因果，《The Book of Why》中举了两个例子，公鸡打鸣，太阳升起，那么公鸡打鸣是太阳升起的因吗？显然不是，如果公鸡打鸣，太阳升起，但有一天你按住了公鸡的嘴，让它打不了鸣，那天太阳居然没有升起，则公鸡打鸣才可能是太阳升起的因。另一个例子，现在有一种疾病，服用药物D的人比不服用药物D的人存活时间要长，那么你可以判断服用药物D是患病的人存活时间较长的因吗？显然也不可以，因为很有可能不是因为服用了药物D，而是因为买得起药物D的人能获得更好的医疗资源来延长寿命。因此，如果要确认药物D是不是这个因，则需要对照实验。《The Book of Why》这本书更认为，发现因果的能力即智能，而我们现在构建的深度学习，机器学习，只能提取事物之间的相关性，并没有发现因果的能力。     能改变果的元素才是因，这个元素可以不单一。人类天生就有从与世界的互动中提取因果的能力，这也是一项基本生存技能。那么怎么赋予机器这种能力呢？              今天的这个系统则尝试赋予智能系统因果归纳和推理的能力，以便在视觉环境内完成目标导向的任务。     想象一下，有天，你买了一个机器人回家做家务，它第一次来到陌生的你的家，如果你事先没有和它说家的情况，它就需要自己理清开关和灯之间的关系，这之后你才能给它下达开关灯的命令。它自己触碰开关，理清开关和灯之间的关系的这个过程叫做因果归纳。在它理清之后，你下达命令，它根据之前归纳的因果关系，推理触碰哪个开关，则称之为因果推理。怎么让你的机器人拥有这项基本的因果归纳和推理的能力呢？  一、问题定义     首先，定义问题，用(S, A, p, G, r, γ)定义一个基于目标的马可夫决策过程(Goal-Conditioned Markov Decision Process)[可以先了解一下马可夫决策过程再看这篇]，其中S是状态域，A是动作域，p: S × A → S 是状态域随采取动作的变化(transition dynamics)，G是目标域，r : S × A × G → R 是收益方程，每行动一次，根据结果，有的即时收益。γ是discount factor。这个问题的解是可以使过程获得最大收益的策略πG: S × G → A。收益的计算方式如下：     t是时间或者步骤编号。     在这里我们不止有一个MDP，而是有K个有着不同的p: S × A → S的MDP，用M = {M(1), M(2), . . . , M(K)}表示。M(k) 则是(S, A, p(k), G, r, γ)。p(k) 定义了动作和状态之间的因果关系。遵循不同的p(k)则在同样的状态下做同样的动作，会转换到不同的状态。 &lt;div align=center&gt;&lt;/div&gt;    如上图所示，因果归纳有两个步骤，第一步，我们执行交互策略πI来收集一系列转变τ = {(s1 , a1 ), (s2 , a2 ), . . . }，这一系列转变会被用来训练因果归纳模型(causal induction model) F，生成潜在因果模型(latent causal model)C^ = F (τ )模型。第二步，环境不变，用第一步得到的C^来得到πG，这样系统就可以用新的策略来交互。我们将所有的MDP的集合M分成两个不相交的子集，分别是Mtrain和Mtest，在训练过程中，我们用Mtrain来学习因果归纳模型F和基于目标的策略πG，在测试过程中，我们会评估模型F是否能够通过观察交互数据来为新的MDP解出最佳策略πG。 二、迭代因果归纳网络(ITERATIVE CAUSAL INDUCTION NETWORK)     这个网络的目标是理清因和果的对应关系，即理清行动会对环境造成的影响。     刚开始的时候，在我们的因果结构C^里，是没有因果之间的连接的，也就是所有关系的初始权值是0。如下图所示，在这个场景里，有四种因果结构，一对一，一对多，多对一，总闸模式，因是开关，果是灯，而初始的时候，开关和灯的关联是没有的，也就是下图中的带箭头的黑线是没有的。     接着，我们会有一个时间序列，在这个时间序列里，我们会尝试不同的操作(action)。如下图所示，t=1的时候，会有一个场景图，在执行了a1这个动作后，场景图产生了变化，转变成了t=2下面的场景图。每个场景图会通过一个编码器(Observation Encoder)转换成一个编码s，两个场景编码之间的差则为R1，表示场景因动作而产生的变化。R1和a1随后会经过一个过渡编码器(Transition Encoder)融合场景变化和动作信息，再经过一个边解码器(Edge Decoder)转换成因果之间的对应关系∆C^。 所以新的因果结构更新为：  &lt;div align=center&gt;，&lt;/div&gt; &lt;div align=center&gt;。&lt;/div&gt;  值得注意的是，最后的C^并不是C^H，C^H再经过一层转换才最终得到C^。可以看出边解码器(Edge Decoder)的输入可以是经过转换后的R和a，也可以是经过转换后的C^H。如下图所示，边解码器输出soft attention vector α (1 × N )，其中N是可以有的动作的数量，而∆e是边权重(edge weights)。α是用来权衡应该指向的点或者形成的边。公式中φ是过渡编码器(Transition Encoder)。 三、学习基于目标的策略(LEARNING GOAL-CONDITIONED POLICIES)     首先，策略要完成的事情是，给定初始状态图s0，目标状态图g，和因果结构C^，在一定步骤T内，使状态从s0变为g。     如图所示，目标图片和初始状态图片首先会经过一个编码器(Observation Encoder)，再经过一个全连接网络输出attention vector α，作用在N × N的因果结构C^上，输出一个大小为N的向量，表示选定的边。这个向量和图片编码后的向量再经过全连接层，最后输出需要实施的动作。 φi指全连接网络。 记：  第一次写论文的笔记，发现很多单词翻译起来实在疲惫，翻译不好，不尽义，因此很多词躲懒没翻，以后可能会越来越懒哦。 公众号不能插入公式，所以有些符号会稍微变形。 这篇论文恰好有释出源代码，所以训练，实验部分会结合代码分析做个下篇 最近因为看了几页《The Book of Why》，对因果方向的研究稍感兴趣了些，查了一下相关研究，19年似乎就这么一篇，门庭如此冷落，大家要踩踩啊。参考论文： Suraj Nair, Yuke Zhu, Silvio Savarese, Li Fei-Fei, Stanford University, CAUSAL INDUCTION FROM VISUAL OBSERVATIONS FOR GOAL DIRECTED TASKS, 2019 "
    }, {
    "id": 59,
    "url": "http://localhost:4000/%E9%A9%AC%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B(-Markov-Decision-Processes)_%E4%B8%8D%E7%A1%AE%E5%AE%9A%E4%B8%96%E7%95%8C%E7%9A%84%E5%86%B3%E7%AD%96%E6%96%B9%E6%B3%95/",
    "title": "马可夫决策过程( Markov Decision Processes):不确定世界的决策方法",
    "body": "2020/03/05 -     今天来学习一下马可夫决策过程，以下简称MDP。     现在考虑一个问题，如上图所示，机器人前面有一堆火，而它需要去取火那边的钻石，它如何才能取到那颗钻石。     下面是一张俯视图： 游戏规则：     行动: 画叉的那一格是墙，不能进入的方格，所以在这张图中，机器人可以移动的方格总共是十一个。如果机器人采取向一个方格移动，80%的概率会成功，10%向左边的方向，10%会向右边的方向的方格移动。     奖惩: 如果机器人进入+1的方格，机器人加1分，游戏结束。若机器人进入-1的方格，机器人减1分，游戏结束。机器人每采取一个行动会获得小的奖惩，比如加0. 1分或者减0. 1分。     目标：得最多的分。     可以看出，MDP要解决的不是一个确定的世界的问题，是一个不一定的世界的问题。机器人虽然计划采取某个行动，但它不一定能完成计划，它想往北走，却有可能是向西或者向东走了一步。比如一个人想要到达一个目的地，选择开车，结果堵车了，选择坐地铁，地铁故障了，选择走路，结果摔了一跤骨折折到了医院，选择坐公交，结果一见钟情，坐过了站。      确定的世界，根据现在的状态来采取行动，就一定能到达想要达到的状态，     不确定的世界，虽然根据现在的状态采取了行动，但是不一定能到达想要到达的地方，现实世界就是一个不确定的世界。      MDP算法可以应用在指导机器人行动决策，工厂生产决策，农业生产决策等等不确定世界的问题上。          这篇文章主要分为三个部分：      1.   Markov Decision Process 马可夫决策过程     2.   Policy Evaluation  策略评估     3.   Value Iteration 【找最佳策略】1.   Markov Decision Process 马可夫决策过程       定义一个MDP：     以机器人拿钻石为例，S [States] 是状态集，包含机器人所有可能的状态，以第二张图为例，有十二个格子，除了墙之外的其他十一个格子机器人都可以进去，所以机器人在游戏中总共可以有11种状态，而机器人最初在的那一格为初始状态，用下标start注明，初始状态是一定会有的状态。A(s)[Actions(s)]是机器人在状态为s的时候所有可以做的行动的集合，这个例子中，机器人可以有向东，西，南，北移动这四个动作。T(s, a, s’)表示机器人在状态s的时候做了行为a，最后变为状态s’的概率，如果状态列举得完整，那么应该有：  R(s, a, s’) [Reward(s, a, s’)]表示机器人在状态s采取行动a，进入状态s’后会获得的奖励或惩罚。IsEnd(s)判断游戏是否结束。     由T(s, a, s’)和R(s, a, s’)可以看出，MDP的理念是，未来和过去是无关的，未来的可能只取决于现在的状态和即将采取的行动，和过去的状态和行动是无关的。如果要解决的问题不止依赖于现在的状态和行为，也依赖于过去的状态和行为，那可以重新定义更大的状态集(S)来使问题符合MDP的要求。     MDP得到的行动方案称之为策略π，比如机器人在状态s的时候下一步要采取的行动。     最佳的策略为π*，是MDP要求得的结果。     到这里，MDP的定义就结束了。 2.  Policy Evaluation  策略评估     假设MDP给出了一个策略(policy)，怎么评估它的好坏?     这里用效能(utility)来表示遵循策略衍生出来的路径产生的奖惩(reward)，比如给机器人下了一个向东的指令，它最后不一定成功向东，也可能向南或北移动了，向东之后如果游戏没有结束，它可能接着往南走，在结束游戏之前它会有很多种走法，每种走法全程产生的分数称之为效能(utility)：     策略的价值(value)则是这个策略的效能的期望值，价值(value)最大的策略即最佳策略π*：     下面看几个不同情况下的最佳策略： &lt;div align=center&gt;&lt;/div&gt; &lt;div align=center&gt;&lt;/div&gt; &lt;div align=center&gt;&lt;/div&gt; &lt;div align=center&gt;&lt;/div&gt; 图中R(s)表示的是机器人每做一个动作的得分，这样设置一个负分是为了促使机器人尽快完成游戏，R(s)值越小，则机器人越需要尽早结束游戏，免得拉低总分。图中每个箭头表示在当前格子中的最佳策略。当R(s)=-0. 01的时候，如果机器人在第二排第三格，最佳策略是向西，所以会出现三种结果，80%原地不动，10%向北移一格，10%向南移一格。而当R(s)=-0. 03的时候，同样的状态，最佳策略变成了向北，因为R(s)的减低，游戏对拖延的容忍值降低了，因此最佳策略产生了变化，机器人需要冒更多的风险去尽早结束游戏。当R(s)=-2. 0时，则最佳策略是尽早结束游戏，同样的状态直接向东往-1格去。     此外还有一个概念是discount factor γ，即随着时间的推移，你的得分（收获，奖赏）会被打折扣。拖得越久折扣越大。     这里我们引入一个新的游戏，这是一个掷骰子的游戏：     游戏规则:      1. **状态：**游戏有两种状态，in (在游戏中)和 end（退出游戏）     2. **动作**：有两个动作可以选，quit(退出游戏)和stay(留在游戏)     3. **奖惩**：选择quit时，退出游戏，获得$10奖励；选择stay，则会掷一个六面骰子，如果骰子是1，2向上，则奖励$4，并退出游戏，如果骰子是3，4，5，6朝上，则奖励$4，并继续留在游戏里。    通过游戏规则可以绘制如下状态图，蓝色圆框是状态，线路表示策略所产生的方向，线路上会注明策略和报酬，以及产生这条线路的概率。这里引入了一个中间状态，即红色虚线圆框，它表示在执行策略后，一个判断当前状态的动作。值得留意的是这个游戏只有一个动作会引发不确定的结果，即stay，quit的结果是确定的。     以掷骰子的游戏为例，当discount factor γ 为1的时候，表示收到回报的时间不重要，回报不会因为收到时间上的延后而贬值。当 γ 为0的时候，表示只有当下收到的回报才是重要的，未来是不需要考虑的。而通常γ 的值会介于0-1之间，收到回报的时间是重要的，时间也是一个很重的成本，越早取得成功越好，但也不会只考虑当下的收益。当γ大于1呢？     同样，以机器人取钻石的游戏为例，当 γ在0-1之间，越晚拿到钻石，回报也会越低。     下面这个游戏可以解一下，答案已经标在了上面：     在状态s执行策略π的价值如下定义：     在中间状态s执行动作a的价值如下定义：     这两个定义的区别在哪里？即一个是当前状态的价值，另一个则是中间状态的价值，如果当前状态是end，则价值是0，当前状态是in的时候，有中间状态的价值：      以掷骰子游戏为例，策略为stay的价值如下：     根据公式推导，则可以得出在状态in的时候，使用stay策略，价值是12。推导过程见下图：     会不会有一个游戏可以永远玩下去，获得无穷无尽的收益呢？     避免这种情况产生的方法有两个：     1. 设置游戏的最长时间或者步数。或者随步数和时间的增加，可以执行的策略也随着产生变化。把人生比作一场游戏，那么很明显无论你采取如何正确的策略过每分每秒，这个游戏也必然有停止的时候，而随着年纪增大，能做的策略选择也越来越少，有天你会老得跑不动，只能拄着拐杖慢慢踱，有天你老得掉光了牙，很多东西也吃不了了，希望看到这里的人都有这么一天，O(∩_∩)O。          2. 另一个就是前面提到的discount factor γ ，根据下面的定理，只要 γ小于1，则收益必然收敛。最后的收益必然小于等于游戏中单步最大收益/(1-γ)。     3. Absorbing state: 将游戏设计得无论游戏者设计什么样的游戏策略，最终都有可能到达游戏结束那个状态。     如何计算策略价值，虽然收益通过一些方法限制，最后也会收敛，但是游戏都是有可能无限长时间玩下去的，所以可以通过限制最长步长或者游戏时间的方式来计算策略价值。 如上图所示，当最长步长为0的时候所有状态下的策略价值都是0，因为游戏根本就不会开始，当最长步长为1时，则会产生收益，当步长为t时，策略价值则可以基于s’状态下t-1为步长的策略价值来得到，以为已经走了一步，还剩下t-1步。     将最长步长设定为多少是比较合适的呢？那就是，当策略价值随着最长步长的增加几乎不再增加的时候。     这样做的算法复杂度为：     以掷骰子游戏为例，如果最长步数是1，策略价值为4，如果最长步数是100，策略价值则会很接近最大可能的收益：  3.  Value Iteration     如何找到最佳策略，首先定义最大策略价值：     中间最大行动价值定义：     使得中间行动价值最大的动作则是最佳策略：      以机器人拿钻石的游戏为例，首先会计算每个状态下每个策略的期望收益：     再由此得到最佳策略：     最佳策略获得的期望收益是：     时间复杂度为： 参考： [1] Lecture 8: Markov Decision Processes (MDPs), CS188 Artificial Intelligence UC Berkeley, Spring 2013 Instructor: Prof. Pieter Abbeel [2] Lecture 9: Markov Decision Processes II, CS188 Artificial Intelligence UC Berkeley, Spring 2013 Instructor: Prof. Pieter Abbeel [3] Lecture 7: Markov Decision Processes - Value Iteration | Stanford CS221: AI (Autumn 2019), Percy Liang, Associate Professor &amp; Dorsa Sadigh, Assistant Professor "
    }, {
    "id": 60,
    "url": "http://localhost:4000/HelloWorld/",
    "title": "Hello, World!",
    "body": "2020/02/22 - 现在是2020年2月22日，总觉得该做点什么。 不如敲行python代码吧！ print( Hello, World! )来日方长，请多关照！ "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><span class='body'>"+ body +"</span><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}

function lunr_search(term) {
    $('#lunrsearchresults').show( 400 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-primary btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><small><span class='body'>"+ body +"</span><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
    
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 5 );
        $( "body" ).removeClass( "modal-open" );
    });
});